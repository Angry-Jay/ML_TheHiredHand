{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hired Hand\n",
    "\n",
    "**Apprentissage Automatique pour la Prédiction de Changement d'Emploi**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des Matières\n",
    "\n",
    "1. [Description du Projet et du Jeu de Données](#1-description-du-projet-et-du-jeu-de-données)\n",
    "   - [1.1 Objectif du Projet](#11-objectif-du-projet)\n",
    "   - [1.2 Solutions Existantes](#12-solutions-existantes)\n",
    "   - [1.3 Informations sur le Jeu de Données](#13-informations-sur-le-jeu-de-données)\n",
    "   - [1.4 Hypothèses Initiales](#14-hypothèses-initiales)\n",
    "2. [Importation des Bibliothèques](#2-importation-des-bibliothèques)\n",
    "3. [Accès aux Données](#3-accès-aux-données)\n",
    "4. [Analyse Exploratoire du Jeu de Données](#4-analyse-exploratoire-du-jeu-de-données)\n",
    "   - [4.1 Analyse des Métadonnées](#41-analyse-des-métadonnées)\n",
    "   - [4.2 Analyse des Valeurs Manquantes](#42-analyse-des-valeurs-manquantes)\n",
    "   - [4.3 Distributions et Valeurs Aberrantes](#43-distributions-et-valeurs-aberrantes)\n",
    "   - [4.4 Étude de la Variable Cible](#44-étude-de-la-variable-cible)\n",
    "   - [4.5 Corrélations](#45-corrélations)\n",
    "   - [4.6 Clustering Non Supervisé](#46-clustering-non-supervisé)\n",
    "   - [4.7 Feature Engineering](#47-feature-engineering)\n",
    "   - [4.8 Synthèse EDA et Vérification des Hypothèses](#48-synthèse-eda-et-vérification-des-hypothèses)\n",
    "5. [Modélisation](#5-modélisation)\n",
    "   - [5.1 Préparation des Données](#51-préparation-des-données)\n",
    "   - [5.2 Modèles de Référence](#52-modèles-de-référence)\n",
    "   - [5.3 Optimisation](#53-optimisation)\n",
    "   - [5.4 Évaluation Finale](#54-évaluation-finale)\n",
    "6. [Analyse Critique](#6-analyse-critique)\n",
    "   - [6.1 Analyse des Erreurs](#61-analyse-des-erreurs)\n",
    "   - [6.2 Feature Importance](#62-feature-importance)\n",
    "   - [6.3 Retour sur les Hypothèses](#63-retour-sur-les-hypothèses)\n",
    "7. [Conclusion](#7-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Description du Projet et du Jeu de Données\n",
    "\n",
    "### 1.1 Objectif du Projet\n",
    "\n",
    "Ce projet applique des techniques d'Apprentissage Automatique pour prédire si un candidat cherche activement un nouvel emploi.\n",
    "\n",
    "**Contexte métier :** Une entreprise de formation en Data Science souhaite identifier parmi ses participants ceux qui cherchent un emploi, afin de :\n",
    "- Mieux cibler les candidats pour des offres d'emploi\n",
    "- Comprendre les facteurs qui poussent à chercher un nouvel emploi\n",
    "- Optimiser les ressources de placement\n",
    "\n",
    "**Objectifs techniques :**\n",
    "- Construire un modèle de classification binaire (cherche emploi vs ne cherche pas)\n",
    "- Identifier les facteurs clés influençant la recherche d'emploi\n",
    "- Appliquer une méthodologie ML rigoureuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Solutions Existantes\n",
    "\n",
    "**Approche Traditionnelle :**\n",
    "Les départements RH utilisent des filtres manuels (expérience, diplôme, localisation) pour identifier les candidats. Cette approche est subjective et ne capture pas les interactions complexes entre facteurs.\n",
    "\n",
    "**Solutions ML courantes :**\n",
    "- **Modèles de référence :** Régression Logistique, KNN\n",
    "- **Modèles ensemblistes :** Random Forest, XGBoost, Gradient Boosting\n",
    "\n",
    "**Constats de la littérature :**\n",
    "- Les méthodes ensemblistes surpassent généralement les modèles simples\n",
    "- L'ingénierie des caractéristiques impacte significativement les performances\n",
    "- La gestion du déséquilibre des classes est cruciale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Informations sur le Jeu de Données\n",
    "\n",
    "**Source :** [HR Analytics: Job Change of Data Scientists](https://www.kaggle.com/datasets/arashnic/hr-analytics-job-change-of-data-scientists)\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Taille :** ~19 000 instances, 14 caractéristiques\n",
    "- **Type :** Données tabulaires mixtes (numériques et catégorielles)\n",
    "- **Cible :** `target` (1 = cherche un emploi, 0 = ne cherche pas)\n",
    "\n",
    "**Variables principales :**\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `city_development_index` | Indice de développement de la ville |\n",
    "| `experience` | Années d'expérience |\n",
    "| `company_size` | Taille de l'entreprise actuelle |\n",
    "| `company_type` | Type d'entreprise |\n",
    "| `last_new_job` | Années depuis le dernier changement d'emploi |\n",
    "| `training_hours` | Heures de formation suivies |\n",
    "| `education_level` | Niveau d'éducation |\n",
    "| `major_discipline` | Discipline principale |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Hypothèses Initiales\n",
    "\n",
    "Avant d'explorer les données, voici nos hypothèses sur les facteurs qui influenceront la recherche d'emploi :\n",
    "\n",
    "**H1 - Expérience professionnelle**\n",
    "> Les candidats avec plus d'expérience seront moins enclins à chercher un nouvel emploi (stabilité de carrière).\n",
    "\n",
    "**H2 - Niveau de développement de la ville**\n",
    "> Les candidats dans des villes moins développées chercheront plus activement (moins d'opportunités locales).\n",
    "\n",
    "**H3 - Situation professionnelle actuelle**\n",
    "> Les candidats sans emploi actuel (`company_size` manquant) chercheront logiquement plus un emploi.\n",
    "\n",
    "**H4 - Mobilité passée**\n",
    "> Les candidats ayant changé d'emploi récemment (`last_new_job` faible) seront plus mobiles.\n",
    "\n",
    "**H5 - Formation**\n",
    "> Les candidats avec un niveau d'éducation élevé (Graduate, Masters) seront plus mobiles sur le marché.\n",
    "\n",
    "---\n",
    "\n",
    "➡️ **Ces hypothèses seront confrontées aux résultats en section 6.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Importation des Bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Accès aux Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/refs/heads/main/aug_train.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_URL)\n",
    "\n",
    "print(f\"Dataset chargé : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Analyse Exploratoire du Jeu de Données\n",
    "\n",
    "### 4.1 Analyse des Métadonnées\n",
    "\n",
    "Objectifs de cette section :\n",
    "- Comprendre la structure du dataset (dimensions, types)\n",
    "- Détecter les problèmes de qualité (doublons, colonnes inutiles)\n",
    "- Vérifier l'absence de fuite de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure du dataset\n",
    "print(f\"Dimensions : {df.shape[0]} lignes x {df.shape[1]} colonnes\")\n",
    "print(f\"Doublons : {df.duplicated().sum()}\")\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation par types\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numériques ({len(num_cols)}) : {num_cols}\")\n",
    "print(f\"Catégorielles ({len(cat_cols)}) : {cat_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives (numériques)\n",
    "df[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinalité des variables catégorielles\n",
    "print(\"Cardinalité des variables catégorielles :\")\n",
    "for col in cat_cols:\n",
    "    print(f\"  {col}: {df[col].nunique()} valeurs uniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la fuite de données\n",
    "suspect_keywords = ['salary', 'offer', 'hired', 'compensation']\n",
    "leakage_cols = [col for col in df.columns if any(kw in col.lower() for kw in suspect_keywords)]\n",
    "\n",
    "if leakage_cols:\n",
    "    print(f\"ATTENTION - Colonnes suspectes : {leakage_cols}\")\n",
    "else:\n",
    "    print(\"Pas de fuite de données détectée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 4.1 :**\n",
    "- **19 158 instances**, 14 colonnes (13 prédicteurs + 1 cible)\n",
    "- **Aucun doublon**\n",
    "- **2 variables numériques** : `city_development_index`, `training_hours`\n",
    "- **10 variables catégorielles** dont `city` (123 valeurs) = haute cardinalité\n",
    "- `enrollee_id` = identifiant à exclure de la modélisation\n",
    "- **Pas de fuite de données** détectée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analyse des Valeurs Manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs manquantes par colonne\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Manquants': missing,\n",
    "    'Pourcentage': missing_pct\n",
    "}).query('Manquants > 0').sort_values('Pourcentage', ascending=False)\n",
    "\n",
    "print(f\"Colonnes avec valeurs manquantes : {len(missing_df)} / {len(df.columns)}\")\n",
    "print(f\"Lignes affectées : {df.isnull().any(axis=1).sum()} ({df.isnull().any(axis=1).sum()/len(df)*100:.1f}%)\")\n",
    "print()\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des valeurs manquantes\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.barh(missing_df.index, missing_df['Pourcentage'], color='coral', edgecolor='black')\n",
    "ax.set_xlabel('Pourcentage de valeurs manquantes (%)')\n",
    "ax.set_title('Valeurs Manquantes par Variable', fontweight='bold')\n",
    "ax.bar_label(bars, fmt='%.1f%%', padding=3)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrence des valeurs manquantes (insight clé)\n",
    "both_missing = (df['company_size'].isnull() & df['company_type'].isnull()).sum()\n",
    "print(f\"company_size ET company_type manquantes ensemble : {both_missing} ({both_missing/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 4.2 :**\n",
    "- **8 colonnes** sur 14 ont des valeurs manquantes\n",
    "- **53% des lignes** sont affectées (10 203 / 19 158)\n",
    "- Les plus impactées : `company_type` (32%), `company_size` (31%), `gender` (24%)\n",
    "- **Pattern clé** : `company_size` + `company_type` manquantes ensemble → candidats sans emploi actuel\n",
    "- **Stratégie** : créer une catégorie \"Non employé\" plutôt qu'imputer (le manquant est informatif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Distributions et Valeurs Aberrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numériques à analyser (excluant enrollee_id et target)\n",
    "num_features = ['city_development_index', 'training_hours']\n",
    "\n",
    "# Histogrammes + Boxplots côte à côte\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for idx, col in enumerate(num_features):\n",
    "    # Histogramme\n",
    "    axes[0, idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0, idx].set_title(f'Distribution de {col}', fontweight='bold')\n",
    "    axes[0, idx].set_ylabel('Fréquence')\n",
    "    \n",
    "    # Boxplot\n",
    "    axes[1, idx].boxplot(df[col].dropna(), vert=True)\n",
    "    axes[1, idx].set_title(f'Boxplot de {col}', fontweight='bold')\n",
    "    axes[1, idx].set_ylabel('Valeur')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détection des outliers (méthode IQR)\n",
    "print(\"Outliers détectés (méthode IQR) :\")\n",
    "for col in num_features:\n",
    "    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
    "    print(f\"  {col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des principales variables catégorielles\n",
    "cat_to_plot = ['education_level', 'experience', 'company_size', 'company_type', 'last_new_job', 'relevent_experience']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(cat_to_plot):\n",
    "    counts = df[col].value_counts()\n",
    "    if len(counts) > 8:\n",
    "        counts = counts.head(8)\n",
    "    axes[idx].barh(counts.index.astype(str), counts.values, color='steelblue', edgecolor='black')\n",
    "    axes[idx].set_title(col, fontweight='bold')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 4.3 :**\n",
    "\n",
    "**Variables numériques :**\n",
    "- `city_development_index` : distribution asymétrique gauche (majorité villes développées ~0.9)\n",
    "- `training_hours` : distribution asymétrique droite (mode ~50h, outliers jusqu'à 336h)\n",
    "- **Outliers conservés** : ils représentent des comportements réels (engagement élevé, villes peu développées)\n",
    "\n",
    "**Variables catégorielles - déséquilibres notables :**\n",
    "- `education_level` : dominé par Graduate (60%)\n",
    "- `company_type` : dominé par Pvt Ltd (83%)\n",
    "- `relevent_experience` : 72% ont une expérience pertinente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Étude de la Variable Cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "target_counts = df['target'].value_counts()\n",
    "ratio = target_counts.max() / target_counts.min()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "bars = ax.bar(['Ne cherche pas (0)', 'Cherche emploi (1)'], target_counts.values, \n",
    "              color=['#e74c3c', '#27ae60'], edgecolor='black')\n",
    "ax.bar_label(bars, fmt='%d')\n",
    "ax.set_title(f'Distribution de la Cible (Ratio {ratio:.1f}:1)', fontweight='bold')\n",
    "ax.set_ylabel('Nombre')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Classe 0 (ne cherche pas) : {target_counts[0]} ({target_counts[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Classe 1 (cherche emploi) : {target_counts[1]} ({target_counts[1]/len(df)*100:.1f}%)\")\n",
    "print(f\"Ratio de déséquilibre : {ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des distributions numériques par cible\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for idx, col in enumerate(num_features):\n",
    "    for target_val, color, label in [(0, '#e74c3c', 'Ne cherche pas'), (1, '#27ae60', 'Cherche')]:\n",
    "        data = df[df['target'] == target_val][col]\n",
    "        axes[idx].hist(data, bins=20, alpha=0.6, color=color, label=label, edgecolor='black')\n",
    "    axes[idx].set_title(f'{col} par cible', fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taux de recherche d'emploi par variable catégorielle clé\n",
    "key_cats = ['relevent_experience', 'enrolled_university', 'education_level', 'last_new_job']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(key_cats):\n",
    "    rates = df.groupby(col)['target'].mean().sort_values(ascending=False) * 100\n",
    "    axes[idx].barh(rates.index.astype(str), rates.values, color='steelblue', edgecolor='black')\n",
    "    axes[idx].set_title(f'Taux de recherche par {col}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('%')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 4.4 :**\n",
    "\n",
    "**Déséquilibre des classes :** Ratio **3:1** (75% / 25%) → nécessite `class_weight='balanced'`\n",
    "\n",
    "**Variables numériques :**\n",
    "- `city_development_index` : **discriminant** - CDI faible → plus de chercheurs (confirme H2)\n",
    "- `training_hours` : **peu discriminant** - distributions similaires entre classes\n",
    "\n",
    "**Variables catégorielles - facteurs discriminants :**\n",
    "| Variable | Observation |\n",
    "|----------|-------------|\n",
    "| `relevent_experience` | Sans exp. → 34% vs avec exp. → 22% |\n",
    "| `enrolled_university` | Temps plein → 38%, non inscrit → 21% |\n",
    "| `education_level` | Graduate → 28% (paradoxe : PhD → 14%) |\n",
    "| `last_new_job` | \"never\" → 30%, >4 ans → 18% |\n",
    "\n",
    "**Profil type du chercheur d'emploi :**\n",
    "> Étudiant temps plein, niveau Graduate, sans expérience pertinente, premier emploi ou changement récent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrélation des variables numériques (Pearson)\n",
    "num_for_corr = ['city_development_index', 'training_hours', 'target']\n",
    "corr_matrix = df[num_for_corr].corr()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.3f')\n",
    "plt.title('Corrélation - Variables Numériques', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association des variables catégorielles avec la cible (V de Cramér)\n",
    "\n",
    "Le V de Cramér mesure la force d'association entre variables catégorielles :\n",
    "- **< 0.1** : Négligeable | **0.1-0.3** : Faible | **> 0.3** : Modérée à forte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats.contingency import association\n",
    "\n",
    "# Variables catégorielles à analyser (excluant identifiants)\n",
    "categorical_cols = ['gender', 'relevent_experience', 'enrolled_university', \n",
    "                    'education_level', 'major_discipline', 'experience',\n",
    "                    'company_size', 'company_type', 'last_new_job']\n",
    "\n",
    "# Calcul du V de Cramér pour chaque variable catégorielle vs target\n",
    "cramer_results = []\n",
    "for col in categorical_cols:\n",
    "    df_clean = df[[col, 'target']].dropna()\n",
    "    contingency = pd.crosstab(df_clean[col], df_clean['target'])\n",
    "    cramer_v = association(contingency, method='cramer')\n",
    "    cramer_results.append({'Variable': col, 'V de Cramér': cramer_v})\n",
    "\n",
    "cramer_df = pd.DataFrame(cramer_results).sort_values('V de Cramér', ascending=False)\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#27ae60' if v > 0.1 else '#95a5a6' for v in cramer_df['V de Cramér']]\n",
    "bars = ax.barh(cramer_df['Variable'], cramer_df['V de Cramér'], color=colors, edgecolor='black')\n",
    "ax.axvline(x=0.1, color='red', linestyle='--', label='Seuil faible (0.1)')\n",
    "ax.set_xlabel(\"V de Cramér\")\n",
    "ax.set_title(\"Association des Variables Catégorielles avec la Cible\", fontweight='bold')\n",
    "ax.bar_label(bars, fmt='%.3f', padding=3)\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 4.5 :**\n",
    "\n",
    "#### Analyse de la Matrice de Corrélation\n",
    "\n",
    "- **Lien négatif fort avec le CDI (-0.342)** : L'indice de développement de la ville est la variable numérique la plus prédictive. Le coefficient négatif confirme que plus le CDI est élevé, moins les candidats cherchent à partir.\n",
    "\n",
    "- **Indépendance des heures de formation (-0.022)** : La corrélation est proche de zéro, confirmant que le volume de formation n'aide pas à prédire l'intention de changement d'emploi.\n",
    "\n",
    "- **Absence de multicolinéarité** : Quasi aucune corrélation entre CDI et training_hours (0.002) → ces variables apportent des informations distinctes.\n",
    "\n",
    "#### Hiérarchie des Variables Catégorielles (V de Cramér)\n",
    "\n",
    "**Prédicteurs clés (V > 0.1)** :\n",
    "| Variable | V de Cramér | Observation |\n",
    "|----------|-------------|-------------|\n",
    "| `experience` | **0.192** | Variable catégorielle la plus déterminante |\n",
    "| `enrolled_university` | **0.156** | Statut universitaire = indicateur majeur |\n",
    "| `relevent_experience` | **0.128** | Complète le podium |\n",
    "\n",
    "**Variables secondaires** : `education_level` (0.09) et `last_new_job` (0.08) sont juste sous le seuil mais restent plus informatives que `gender` ou `major_discipline`.\n",
    "\n",
    "#### Synthèse Croisée\n",
    "\n",
    "> **Le CDI est roi** : Avec r = -0.342, c'est le facteur numérique le plus puissant. Les candidats des villes moins développées sont les plus mobiles.\n",
    "> \n",
    "> **Top 3 catégorielles** : `experience`, `enrolled_university`, `relevent_experience` - tous liés à la situation professionnelle actuelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Clustering Non Supervisé\n",
    "\n",
    "**Objectif :** Vérifier si des patterns naturels émergent des données **sans utiliser la variable cible**.\n",
    "\n",
    "Si le clustering identifie des groupes avec des taux de recherche différents, cela confirme que les features contiennent un signal prédictif exploitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Prétraitement pour clustering (sans target ni enrollee_id)\n",
    "X_cluster = df.drop(['enrollee_id', 'target'], axis=1)\n",
    "num_cols_cluster = X_cluster.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols_cluster = X_cluster.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "cluster_preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_cols_cluster),\n",
    "    ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), cat_cols_cluster)\n",
    "])\n",
    "\n",
    "X_processed = cluster_preprocessor.fit_transform(X_cluster)\n",
    "print(f\"Données prétraitées : {X_processed.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode du coude\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_processed)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Nombre de clusters (k)')\n",
    "plt.ylabel('Inertie')\n",
    "plt.title('Méthode du Coude', fontweight='bold')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means avec k=4 (choisi d'après le coude)\n",
    "k_optimal = 4\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_processed)\n",
    "\n",
    "# PCA pour visualisation\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_processed)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title(f'K-Means (k={k_optimal}) - Projection PCA', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variance expliquée par PCA : {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des clusters vs target\n",
    "df_clusters = df.copy()\n",
    "df_clusters['cluster'] = clusters\n",
    "\n",
    "cluster_analysis = df_clusters.groupby('cluster')['target'].agg(['count', 'sum', 'mean'])\n",
    "cluster_analysis.columns = ['Total', 'Chercheurs', 'Taux (%)']\n",
    "cluster_analysis['Taux (%)'] = (cluster_analysis['Taux (%)'] * 100).round(1)\n",
    "print(\"Distribution du target par cluster :\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "colors = plt.cm.RdYlGn_r(cluster_analysis['Taux (%)'] / 100)\n",
    "bars = ax.bar(cluster_analysis.index, cluster_analysis['Taux (%)'], color=colors, edgecolor='black')\n",
    "ax.axhline(y=df['target'].mean()*100, color='red', linestyle='--', label=f'Moyenne globale ({df[\"target\"].mean()*100:.1f}%)')\n",
    "ax.set_xlabel('Cluster')\n",
    "ax.set_ylabel('Taux de recherche (%)')\n",
    "ax.set_title('Taux de Recherche d\\'Emploi par Cluster', fontweight='bold')\n",
    "ax.bar_label(bars, fmt='%.1f%%')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profil caractéristique de chaque cluster\n",
    "print(\"=== Profil des Clusters ===\\n\")\n",
    "\n",
    "# Variables clés pour le profilage\n",
    "profile_vars = {\n",
    "    'city_development_index': 'CDI moyen',\n",
    "    'training_hours': 'Heures formation (moy)',\n",
    "}\n",
    "\n",
    "# Calcul des profils\n",
    "for cluster_id in sorted(df_clusters['cluster'].unique()):\n",
    "    cluster_data = df_clusters[df_clusters['cluster'] == cluster_id]\n",
    "    taux = cluster_data['target'].mean() * 100\n",
    "    \n",
    "    print(f\"Cluster {cluster_id} (n={len(cluster_data)}, taux={taux:.1f}%)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Variables numériques\n",
    "    print(f\"  CDI moyen : {cluster_data['city_development_index'].mean():.3f}\")\n",
    "    print(f\"  % Employés : {cluster_data['company_size'].notna().mean()*100:.1f}%\")\n",
    "    print(f\"  % Exp. pertinente : {(cluster_data['relevent_experience'] == 'Has relevent experience').mean()*100:.1f}%\")\n",
    "    \n",
    "    # Top education level\n",
    "    top_edu = cluster_data['education_level'].mode().iloc[0] if not cluster_data['education_level'].mode().empty else 'N/A'\n",
    "    print(f\"  Éducation dominante : {top_edu}\")\n",
    "    \n",
    "    # Top enrolled_university\n",
    "    top_univ = cluster_data['enrolled_university'].mode().iloc[0] if not cluster_data['enrolled_university'].mode().empty else 'N/A'\n",
    "    print(f\"  Inscription univ. dominante : {top_univ}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 4.6 :**\n",
    "\n",
    "Le clustering K-Means (k=4) révèle **4 segments de candidats** avec des comportements distincts :\n",
    "\n",
    "| Cluster | Taux | CDI | % Employés | % Exp. pert. | Interprétation |\n",
    "|---------|------|-----|------------|--------------|----------------|\n",
    "| 1 | **12.8%** | 0.899 | 82% | 100% | **Stables** : expérimentés, employés, CDI élevé |\n",
    "| 0 | 17.9% | 0.857 | 72% | 77% | Intermédiaires |\n",
    "| 2 | 27.2% | 0.893 | **39%** | **0.4%** | **Débutants** : peu employés, sans expérience |\n",
    "| 3 | **49.0%** | **0.640** | 64% | 69% | **À risque** : CDI très faible |\n",
    "\n",
    "**Lien avec les hypothèses :**\n",
    "- **Cluster 3** (taux max 49%) : CDI = 0.640 → confirme **H2** (CDI faible = mobilité)\n",
    "- **Cluster 2** (27%) : 39% employés, 0.4% exp. → confirme **H3** (sans emploi) et **H1** (sans exp.)\n",
    "- **Cluster 1** (taux min 12.8%) : 100% exp. pertinente + 82% employés → profil stable (inverse de H1/H3)\n",
    "\n",
    "**Conclusion clé :**\n",
    "> Le clustering **sans utiliser le target** identifie des groupes corrélés au taux de recherche → les features contiennent des **patterns prédictifs naturels** qui valident nos hypothèses H1, H2, H3 avant même la modélisation supervisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Feature Engineering\n",
    "\n",
    "Création de nouvelles variables pour améliorer la séparabilité des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature : Statut d'emploi (basé sur le pattern de valeurs manquantes - section 4.2)\n",
    "df['is_employed'] = df['company_size'].notna().astype(int)\n",
    "\n",
    "# Vérification du pouvoir discriminant\n",
    "employed_rate = df.groupby('is_employed')['target'].mean() * 100\n",
    "print(\"Taux de recherche d'emploi :\")\n",
    "print(f\"  Sans emploi (0) : {employed_rate[0]:.1f}%\")\n",
    "print(f\"  Employé (1)     : {employed_rate[1]:.1f}%\")\n",
    "\n",
    "# V de Cramér pour la nouvelle feature\n",
    "contingency = pd.crosstab(df['is_employed'], df['target'])\n",
    "cramer_v = association(contingency, method='cramer')\n",
    "print(f\"\\nV de Cramér (is_employed vs target) : {cramer_v:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 4.7 :**\n",
    "\n",
    "**Feature créée : `is_employed`**\n",
    "- Transforme le pattern de valeurs manquantes (section 4.2) en signal exploitable\n",
    "- Sans emploi → **40.6%** cherchent vs Employé → **17.9%** cherchent\n",
    "- **V de Cramér = 0.242** → Plus discriminante que toutes les autres variables catégorielles\n",
    "\n",
    "**Pourquoi une seule feature ?** (Principe KISS)\n",
    "> Les autres variables existent déjà sous forme exploitable. Seul le statut d'emploi capture une information nouvelle issue des valeurs manquantes.\n",
    "\n",
    "✓ **Confirme H3** : Les candidats sans emploi cherchent logiquement plus un emploi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Synthèse des Découvertes\n",
    "\n",
    "**Données :** 19 158 instances, 15 colonnes (14 originales + `is_employed`)\n",
    "\n",
    "**Variables numériques :**\n",
    "- `city_development_index` : Distribution asymétrique gauche (majorité CDI > 0.9). Outliers conservés (villes peu développées = signal).\n",
    "- `training_hours` : Distribution asymétrique droite (mode ~50h). 984 outliers (5.1%) conservés (engagement extrême = signal potentiel).\n",
    "\n",
    "**Variables catégorielles - Déséquilibres notables :**\n",
    "- `education_level` : Graduate (60%) | `company_type` : Pvt Ltd (83%) | `relevent_experience` : 72% ont une exp. pertinente\n",
    "\n",
    "**Top 5 des variables discriminantes :**\n",
    "| Rang | Variable | Mesure | Valeur |\n",
    "|------|----------|--------|--------|\n",
    "| 1 | `city_development_index` | Pearson | **-0.342** |\n",
    "| 2 | `is_employed` | V Cramér | **0.242** |\n",
    "| 3 | `experience` | V Cramér | 0.192 |\n",
    "| 4 | `enrolled_university` | V Cramér | 0.156 |\n",
    "| 5 | `relevent_experience` | V Cramér | 0.128 |\n",
    "\n",
    "---\n",
    "\n",
    "#### Vérification des Hypothèses\n",
    "\n",
    "| Hypothèse | Statut | Justification EDA | Validation Clustering |\n",
    "|-----------|--------|-------------------|----------------------|\n",
    "| **H1** - Plus d'expérience → moins de recherche | ✅ Confirmée | V=0.192 | Cluster 1 : 100% exp. → 12.8% |\n",
    "| **H2** - CDI faible → plus de recherche | ✅ **Confirmée** | r = -0.342 | Cluster 3 : CDI=0.64 → **49%** |\n",
    "| **H3** - Sans emploi → plus de recherche | ✅ **Confirmée** | V=0.242 | Cluster 2 : 39% empl. → 27% |\n",
    "| **H4** - Changement récent → plus mobile | ⚠️ Partielle | \"never\" → 30% | Non testé |\n",
    "| **H5** - Éducation élevée → plus mobile | ❌ Infirmée | PhD → 14% | Éducation = Graduate partout |\n",
    "\n",
    "**Double validation :** Les hypothèses H1, H2, H3 sont confirmées à la fois par l'analyse statistique (corrélations) ET par le clustering non supervisé — deux méthodes indépendantes.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implications pour la Modélisation\n",
    "\n",
    "1. **Déséquilibre 3:1** → utiliser `class_weight='balanced'`\n",
    "2. **Valeurs manquantes** → stratégie par variable (catégorie \"Unknown\" ou imputation)\n",
    "3. **Haute cardinalité** (`city` : 123 valeurs) → encodage target/fréquence\n",
    "4. **Outliers conservés** → représentent des comportements réels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split 70/15/15 stratifié :**\n",
    "\n",
    "| Ensemble | Proportion | Rôle |\n",
    "|----------|------------|------|\n",
    "| Train | 70% | Apprentissage |\n",
    "| Validation | 15% | Ajustement hyperparamètres |\n",
    "| Test | 15% | Évaluation finale |\n",
    "\n",
    "**Prétraitement (Pipeline) :**\n",
    "- Numériques → Imputation (médiane) + Standardisation\n",
    "- Catégorielles → Imputation (mode) + One-Hot Encoding\n",
    "- `class_weight='balanced'` → gère le déséquilibre 3:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "X = df.drop(['enrollee_id', 'target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split 70/15/15 stratifié\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "\n",
    "# Identification des types de features\n",
    "num_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Préprocesseur\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_features),\n",
    "    ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), cat_features)\n",
    "])\n",
    "\n",
    "print(f\"Features: {len(num_features)} numériques, {len(cat_features)} catégorielles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Modèles de Référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec/sans feature engineering\n",
    "scoring = ['accuracy', 'f1_weighted', 'roc_auc']\n",
    "\n",
    "# Version SANS is_employed\n",
    "X_no_fe = X_train.drop('is_employed', axis=1)\n",
    "num_feat_no_fe = X_no_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_feat_no_fe = X_no_fe.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "prep_no_fe = ColumnTransformer([\n",
    "    ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_feat_no_fe),\n",
    "    ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), cat_feat_no_fe)\n",
    "])\n",
    "\n",
    "# Version AVEC is_employed (preprocessor déjà défini)\n",
    "\n",
    "# Pipelines\n",
    "pipelines = {\n",
    "    'LR (sans FE)': Pipeline([('prep', prep_no_fe), ('model', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))]),\n",
    "    'LR (avec FE)': Pipeline([('prep', preprocessor), ('model', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))]),\n",
    "    'RF (sans FE)': Pipeline([('prep', prep_no_fe), ('model', RandomForestClassifier(class_weight='balanced', random_state=42))]),\n",
    "    'RF (avec FE)': Pipeline([('prep', preprocessor), ('model', RandomForestClassifier(class_weight='balanced', random_state=42))])\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "results = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    X_data = X_no_fe if 'sans FE' in name else X_train\n",
    "    scores = cross_val_score(pipeline, X_data, y_train, cv=5, scoring='roc_auc')\n",
    "    results[name] = scores.mean()\n",
    "    print(f\"{name}: ROC-AUC = {scores.mean():.3f} (+/- {scores.std():.3f})\")\n",
    "\n",
    "# Gain du feature engineering\n",
    "print(f\"\\nGain FE (Logistic Regression): +{(results['LR (avec FE)'] - results['LR (sans FE)'])*100:.1f}%\")\n",
    "print(f\"Gain FE (Random Forest): +{(results['RF (avec FE)'] - results['RF (sans FE)'])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Optimisation\n",
    "\n",
    "**Stratégie :** Optimiser les deux meilleures familles de modèles\n",
    "\n",
    "| Modèle | Justification |\n",
    "|--------|---------------|\n",
    "| **Logistic Regression** | Meilleur modèle de référence (0.788) → on l'optimise |\n",
    "| **HistGradientBoosting** | Famille différente (boosting vs linéaire) → on explore |\n",
    "\n",
    "Comparaison équitable des deux versions optimisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Optimisation Logistic Regression\n",
    "lr_pipeline_opt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_lr = {\n",
    "    'model__C': [0.01, 0.1, 1, 10],\n",
    "    'model__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(lr_pipeline_opt, param_grid_lr, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(f\"Meilleurs paramètres : {grid_lr.best_params_}\")\n",
    "print(f\"ROC-AUC (CV) : {grid_lr.best_score_:.3f}\")\n",
    "\n",
    "# 2. Optimisation HistGradientBoosting\n",
    "hgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', HistGradientBoostingClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'model__max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "grid_hgb = GridSearchCV(hgb_pipeline, param_grid_hgb, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid_hgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== HistGradientBoosting ===\")\n",
    "print(f\"Meilleurs paramètres : {grid_hgb.best_params_}\")\n",
    "print(f\"ROC-AUC (CV) : {grid_hgb.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Évaluation Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison finale : modèles optimisés\n",
    "models_final = {\n",
    "    'LR (optimisé)': grid_lr.best_estimator_,\n",
    "    'HGB (optimisé)': grid_hgb.best_estimator_\n",
    "}\n",
    "\n",
    "for name, model in models_final.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\n{name}\\n{'='*50}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# Matrices de confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for idx, (name, model) in enumerate(models_final.items()):\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(name)\n",
    "    axes[idx].set_xlabel('Prédit')\n",
    "    axes[idx].set_ylabel('Réel')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 5.4 :**\n",
    "\n",
    "| Modèle | ROC-AUC | Rappel (classe 1) | Candidats détectés |\n",
    "|--------|---------|-------------------|-------------------|\n",
    "| LR (optimisé) | **0.797** | 0.77 | 551/716 (77%) |\n",
    "| HGB (optimisé) | **0.797** | 0.78 | 560/716 (78%) |\n",
    "\n",
    "**Observations :**\n",
    "- Les deux modèles optimisés sont **pratiquement égaux** (ROC-AUC identique)\n",
    "- HGB détecte 9 candidats de plus (+1 point de rappel) — différence marginale\n",
    "- L'optimisation a apporté **+0.9%** vs LR de base (0.788 → 0.797)\n",
    "\n",
    "**Choix du modèle final : Logistic Regression optimisée**\n",
    "\n",
    "*Justification (principe KISS) :*\n",
    "> Performances égales → choisir le modèle le plus simple et interprétable.\n",
    "> \n",
    "> Ce résultat confirme que les relations sont **relativement linéaires** dans ce dataset, ce qui explique pourquoi LR surpassait déjà RF en référence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Analyse Critique\n",
    "\n",
    "### 6.1 Analyse des Erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle final : LR optimisé\n",
    "best_model = grid_lr.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Analyse des erreurs\n",
    "X_test_analysis = X_test.copy()\n",
    "X_test_analysis['y_true'] = y_test.values\n",
    "X_test_analysis['y_pred'] = y_pred\n",
    "X_test_analysis['y_proba'] = y_proba\n",
    "\n",
    "# Types d'erreurs\n",
    "fn = X_test_analysis[(X_test_analysis['y_true'] == 1) & (X_test_analysis['y_pred'] == 0)]  # Faux Négatifs\n",
    "fp = X_test_analysis[(X_test_analysis['y_true'] == 0) & (X_test_analysis['y_pred'] == 1)]  # Faux Positifs\n",
    "\n",
    "print(f\"Faux Négatifs (ratés) : {len(fn)} candidats chercheurs non détectés\")\n",
    "print(f\"Faux Positifs : {len(fp)} candidats stables prédits comme chercheurs\")\n",
    "\n",
    "# Caractéristiques des faux négatifs (les plus coûteux pour le métier)\n",
    "print(\"\\n=== Profil des Faux Négatifs (candidats ratés) ===\")\n",
    "print(f\"CDI moyen : {fn['city_development_index'].mean():.3f} (vs {X_test['city_development_index'].mean():.3f} global)\")\n",
    "print(f\"% employés : {fn['is_employed'].mean()*100:.1f}% (vs {X_test['is_employed'].mean()*100:.1f}% global)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 6.1 :**\n",
    "\n",
    "| Type d'erreur | Nombre | Interprétation |\n",
    "|---------------|--------|----------------|\n",
    "| Faux Négatifs | 165 | Candidats chercheurs **ratés** (23%) |\n",
    "| Faux Positifs | 495 | Candidats stables mal classés |\n",
    "\n",
    "**Profil des candidats ratés (FN) :**\n",
    "- CDI moyen **plus élevé** (0.876 vs 0.827) → villes développées\n",
    "- **80% employés** (vs 68.3%) → ont un emploi actuel\n",
    "\n",
    "> Le modèle rate les candidats \"atypiques\" : employés dans des villes développées mais qui cherchent quand même. Ces cas contredisent le pattern principal appris (CDI bas + sans emploi → cherche)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance via coefficients LR\n",
    "lr_model = best_model.named_steps['model']\n",
    "preprocessor_fitted = best_model.named_steps['preprocessor']\n",
    "\n",
    "# Récupérer les noms des features après transformation\n",
    "num_feat_names = num_features\n",
    "cat_feat_names = preprocessor_fitted.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features).tolist()\n",
    "all_feat_names = num_feat_names + cat_feat_names\n",
    "\n",
    "# Coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': all_feat_names,\n",
    "    'Coefficient': lr_model.coef_[0]\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "# Top 10 features les plus importantes\n",
    "top_features = coef_df.head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['#27ae60' if c > 0 else '#e74c3c' for c in top_features['Coefficient']]\n",
    "plt.barh(top_features['Feature'], top_features['Coefficient'], color=colors, edgecolor='black')\n",
    "plt.xlabel('Coefficient (impact sur probabilité de chercher)')\n",
    "plt.title('Top 10 Features - Logistic Regression', fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Vert = augmente la probabilité de chercher | Rouge = diminue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé 6.2 :**\n",
    "\n",
    "**Top facteurs augmentant la probabilité de chercher (vert) :**\n",
    "- `city_city_21`, `city_city_103`, `city_city_160`, `city_city_100` → certaines villes spécifiques\n",
    "\n",
    "**Top facteurs diminuant la probabilité (rouge) :**\n",
    "- `is_employed` → être employé réduit la recherche ✓ (confirme H3)\n",
    "- `education_level_Primary School` → niveau primaire moins mobile\n",
    "- `last_new_job_never` → premier emploi (relation complexe)\n",
    "- `city_development_index` → CDI élevé réduit la recherche ✓ (confirme H2)\n",
    "\n",
    "> Les coefficients du modèle confirment les insights de l'EDA : la **situation professionnelle** (`is_employed`, CDI) domine les facteurs démographiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Retour sur les Hypothèses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confrontation des hypothèses initiales (section 1.4) avec les résultats :**\n",
    "\n",
    "| Hypothèse | Statut | Preuves EDA | Clustering | Modèle |\n",
    "|-----------|--------|-------------|------------|--------|\n",
    "| **H1** - Expérience → stabilité | ✅ Confirmée | V = 0.192 | Cluster 1 : 100% exp. → 12.8% | Coef. négatifs exp. élevée |\n",
    "| **H2** - CDI faible → cherche | ✅ **Confirmée** | r = -0.342 | Cluster 3 : CDI=0.64 → **49%** | Top feature (coef. négatif) |\n",
    "| **H3** - Sans emploi → cherche | ✅ **Confirmée** | V = 0.242 | Cluster 2 : 39% empl. → 27% | `is_employed` top coef. |\n",
    "| **H4** - Changement récent → mobile | ⚠️ Partielle | \"never\" → 30% | — | Relation non linéaire |\n",
    "| **H5** - Éducation élevée → mobile | ❌ Infirmée | PhD → 14% | Graduate partout | Coef. faible |\n",
    "\n",
    "**Bilan : 3/5 hypothèses confirmées, 1 partielle, 1 infirmée**\n",
    "\n",
    "**Triple validation pour H1, H2, H3 :**\n",
    "1. **EDA** : Corrélations et V de Cramér\n",
    "2. **Clustering** : Groupes naturels sans utiliser le target\n",
    "3. **Modèle** : Coefficients de la régression logistique\n",
    "\n",
    "> La situation professionnelle actuelle (`is_employed`, `city_development_index`) est plus prédictive que les caractéristiques démographiques (éducation, genre). Cette conclusion est robuste car confirmée par trois approches indépendantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé du Projet\n",
    "\n",
    "**Objectif :** Prédire si un candidat cherche activement un nouvel emploi.\n",
    "\n",
    "**Méthodologie :**\n",
    "1. EDA rigoureuse avec hypothèses préalables (H1-H5)\n",
    "2. Feature engineering ciblé (`is_employed` : V = 0.242)\n",
    "3. Comparaison avec/sans FE (+1.4% à +1.8% gain)\n",
    "4. Optimisation de deux familles : LR (linéaire) et HGB (boosting)\n",
    "5. Validation du clustering non supervisé (patterns confirmés)\n",
    "\n",
    "**Résultats :**\n",
    "| Métrique | Valeur |\n",
    "|----------|--------|\n",
    "| ROC-AUC | **0.797** |\n",
    "| Rappel (classe 1) | **77%** |\n",
    "| Candidats détectés | **551/716** |\n",
    "\n",
    "**Modèle final :** Logistic Regression optimisée (C=0.1, solver=liblinear)\n",
    "\n",
    "**Facteurs clés de la recherche d'emploi :**\n",
    "1. `city_development_index` (CDI faible → plus de mobilité)\n",
    "2. `is_employed` (sans emploi → cherche activement)\n",
    "3. `enrolled_university` (étudiant temps plein → cherche)\n",
    "\n",
    "**Limites et perspectives :**\n",
    "- Données déséquilibrées (3:1) malgré `class_weight='balanced'`\n",
    "- 23% des candidats chercheurs non détectés (faux négatifs)\n",
    "- Amélioration possible : features temporelles, données externes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
