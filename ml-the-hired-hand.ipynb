{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yZg8yXCtjE3J",
   "metadata": {
    "id": "yZg8yXCtjE3J"
   },
   "source": [
    "# The Hired Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h_wT-bE8f0JQ",
   "metadata": {
    "id": "h_wT-bE8f0JQ"
   },
   "source": [
    "**Apprentissage Automatique pour la Prédiction de Placement Professionnel**\n",
    "\n",
    "[![Ouvrir dans Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Angry-Jay/ML_TheHiredHand/blob/main/ml-the-hired-hand.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Table des Matières\n",
    "\n",
    "1. [Description du Projet et du Jeu de Données](#1-description-du-projet-et-du-jeu-de-données)\n",
    "   - [1.1 Objectif du Projet](#11-objectif-du-projet)\n",
    "   - [1.2 Solutions Existantes](#12-solutions-existantes)\n",
    "   - [1.3 Informations sur le Jeu de Données](#13-informations-sur-le-jeu-de-données)\n",
    "2. [Importation des Bibliothèques](#2-importation-des-bibliothèques)\n",
    "3. [Accès aux Données](#3-accès-aux-données)\n",
    "4. [Analyse Exploratoire du Jeu de Données](#4-analyse-exploratoire-du-jeu-de-données)\n",
    "   - [4.1 Analyse des Métadonnées](#41-analyse-des-métadonnées)\n",
    "   - [4.2 Analyse des Valeurs Manquantes](#42-analyse-des-valeurs-manquantes)\n",
    "   - [4.3 Distributions des Caractéristiques, Mise à l'Échelle et Valeurs Aberrantes](#43-distributions-des-caractéristiques-mise-à-léchelle-et-valeurs-aberrantes)\n",
    "   - [4.4 Étude de la Variable Cible](#44-étude-de-la-variable-cible)\n",
    "   - [4.5 Corrélation et Sélection des Caractéristiques](#45-corrélation-et-sélection-des-caractéristiques)\n",
    "   - [4.6 Clustering Non Supervisé](#46-clustering-non-supervisé)\n",
    "   - [4.7 Interprétations et Conclusions](#47-interprétations-et-conclusions)\n",
    "5. [Modèles de Référence et Ensembles ML](#5-modèles-de-référence-et-ensembles-ml)\n",
    "   - [5.1 Divisions Entraînement/Validation/Test](#51-divisions-entraînementvalidationtest)\n",
    "   - [5.2 Pipelines et Modèles](#52-pipelines-et-modèles)\n",
    "   - [5.3 Entraînement et Validation](#53-entraînement-et-validation)\n",
    "   - [5.4 Test](#54-test)\n",
    "   - [5.5 Interprétation et Discussion des Résultats](#55-interprétation-et-discussion-des-résultats)\n",
    "6. [Modèles Améliorés et Optimisation des Hyperparamètres](#6-modèles-améliorés-et-optimisation-des-hyperparamètres)\n",
    "   - [6.1 Justification des Choix](#61-justification-des-choix)\n",
    "   - [6.2 Optimisation des Hyperparamètres](#62-optimisation-des-hyperparamètres)\n",
    "   - [6.3 Résultats Finaux et Analyse](#63-résultats-finaux-et-analyse)\n",
    "7. [Conclusion et Travaux Futurs](#7-conclusion-et-travaux-futurs)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Description du Projet et du Jeu de Données\n",
    "\n",
    "### 1.1 Objectif du Projet\n",
    "\n",
    "Ce projet applique des techniques d'Apprentissage Automatique pour prédire les résultats d'emploi des étudiants diplômés en utilisant le **Jeu de Données de Placement Professionnel**. \n",
    "\n",
    "**Objectifs Principaux :**\n",
    "- **Prédire les résultats d'emploi** (Placé vs Non Placé) en fonction des attributs démographiques, académiques et professionnels\n",
    "- **Démontrer une méthodologie ML cohérente** de la découverte des données à l'optimisation du modèle\n",
    "- **Appliquer une analyse de données complète** incluant :\n",
    "  - Nettoyage et prétraitement des données\n",
    "  - Analyse Exploratoire des Données (EDA)\n",
    "  - Ingénierie et sélection des caractéristiques\n",
    "  - Analyse de corrélation et de clustering\n",
    "- **Construire et évaluer plusieurs modèles de classification** avec des techniques de validation appropriées\n",
    "- **Identifier les facteurs clés d'employabilité** à travers l'analyse de l'importance des caractéristiques et l'interprétation du modèle\n",
    "- **Appliquer les meilleures pratiques ML** incluant les divisions appropriées entraînement/validation/test, la construction de pipelines et l'optimisation des hyperparamètres\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Solutions Existantes\n",
    "\n",
    "**Approche Traditionnelle :**\n",
    "\n",
    "Historiquement, les départements RH et les institutions éducatives s'appuient sur des processus de sélection manuelle avec des filtres heuristiques (par exemple, seuils de moyenne, spécialisations de diplôme spécifiques, seuils d'expérience professionnelle). Cette approche traditionnelle présente plusieurs limitations :\n",
    "- Chronophage et difficile à mettre à l'échelle\n",
    "- Subjective et sujette aux biais humains\n",
    "- Souvent inexacte pour prédire le succès réel du placement professionnel\n",
    "- Ne parvient pas à capturer les interactions complexes entre plusieurs facteurs\n",
    "\n",
    "**Solutions d'Apprentissage Automatique :**\n",
    "\n",
    "Plusieurs approches basées sur le ML existent sur des plateformes comme Kaggle et GitHub pour la prédiction de placement :\n",
    "\n",
    "**Algorithmes Couramment Utilisés :**\n",
    "- **Modèles de Référence :** Régression Logistique, K-Plus Proches Voisins (KNN)\n",
    "- **Modèles Basés sur les Arbres :** Arbres de Décision, Forêt Aléatoire, ExtraTrees\n",
    "- **Méthodes de Boosting :** XGBoost, AdaBoost, Gradient Boosting\n",
    "- **Machines à Vecteurs de Support :** SVC avec divers noyaux\n",
    "\n",
    "**Principales Découvertes de la Littérature :**\n",
    "- Les méthodes d'ensemble basées sur les arbres (Forêt Aléatoire, XGBoost) surpassent généralement les références plus simples\n",
    "- Les modèles non linéaires capturent mieux les interactions entre caractéristiques (par exemple, effet combiné de la moyenne et de l'expérience professionnelle)\n",
    "- L'ingénierie des caractéristiques impacte significativement la performance du modèle\n",
    "- La gestion appropriée du déséquilibre des classes est cruciale pour des prédictions précises\n",
    "\n",
    "**Méthodologie Typique :**\n",
    "1. Analyse Exploratoire des Données (distributions, corrélations, déséquilibre des classes)\n",
    "2. Pipelines de prétraitement (encodage des variables catégorielles, mise à l'échelle, imputation)\n",
    "3. Comparaison de modèles utilisant plusieurs métriques : Précision, Exactitude, Rappel, Score-F1, ROC-AUC\n",
    "4. Optimisation des hyperparamètres utilisant GridSearchCV ou RandomizedSearchCV\n",
    "5. Analyse de l'importance des caractéristiques pour l'interprétabilité\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Informations sur le Jeu de Données\n",
    "\n",
    "**Nom du Jeu de Données :** Jeu de Données de Placement Professionnel\n",
    "\n",
    "**Source Originale :** [Kaggle - Job Placement Dataset](https://www.kaggle.com/datasets/ahsan81/job-placement-dataset/data)\n",
    "\n",
    "**Caractéristiques du Jeu de Données :**\n",
    "- **Type :** Données tabulaires structurées et denses\n",
    "- **Taille :** Petite à moyenne (215 instances, 13 caractéristiques)\n",
    "- **Caractéristiques :** Mélange de variables numériques et catégorielles\n",
    "- **Variable Cible :** Classification binaire (Placé / Non Placé)\n",
    "- **Qualité :** Propre sans valeurs manquantes ni doublons\n",
    "\n",
    "**Accès au Jeu de Données :**\n",
    "- **Dépôt GitHub :** `https://github.com/Angry-Jay/ML_TheHiredHand`\n",
    "- **URL des Données Brutes :** `https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/main/aug_train.csv` `https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/main/aug_test.csv`\n",
    "\n",
    "**Aperçu des Caractéristiques :**\n",
    "- Démographie des étudiants (genre)\n",
    "- Performance académique (% SSC, % HSC, % Diplôme, % MBA)\n",
    "- Parcours éducatif (conseil SSC, conseil HSC, spécialisation HSC, type de diplôme, spécialisation MBA)\n",
    "- Expérience professionnelle\n",
    "- Scores aux tests d'emploi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nLt-pHGeqVQk",
   "metadata": {
    "id": "nLt-pHGeqVQk"
   },
   "source": [
    "## 2. Importation des Bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yuLL6t7xfjF9",
   "metadata": {
    "id": "yuLL6t7xfjF9"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prétraitement et Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Sélection et Optimisation de Modèles\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "# Modèles\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Métriques\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L4DKOMIjqkxl",
   "metadata": {
    "id": "L4DKOMIjqkxl"
   },
   "source": [
    "## 3. Accès aux Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wYM1JqfJlgRO",
   "metadata": {
    "id": "wYM1JqfJlgRO"
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/refs/heads/main/aug_train.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_URL)\n",
    "    \n",
    "    print(\"Jeu de données chargé avec succès !\")\n",
    "    print(f\"Dimensions : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "    \n",
    "    display(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement des données depuis {DATA_URL}\")\n",
    "    print(f\"Détails de l'erreur : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dlhLvQwVqrEU",
   "metadata": {
    "id": "dlhLvQwVqrEU"
   },
   "source": [
    "## 4. Analyse Exploratoire du Jeu de Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rP1msGkOoMm8",
   "metadata": {
    "collapsed": true,
    "id": "rP1msGkOoMm8"
   },
   "source": [
    "### 4.1 Analyse des Métadonnées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j9zobh3uwem",
   "metadata": {},
   "source": [
    "Dans cette section, nous analysons les métadonnées du jeu de données pour comprendre sa structure, ses types de données, sa qualité et ses caractéristiques. Cette exploration initiale aide à identifier :\n",
    "\n",
    "- **Les dimensions du jeu de données** et son échelle\n",
    "- **Les types de données des caractéristiques** (numériques vs catégorielles)\n",
    "- **Les problèmes de qualité des données** (doublons, valeurs manquantes, colonnes non pertinentes)\n",
    "- **Les propriétés statistiques** des caractéristiques numériques\n",
    "- **Les préoccupations potentielles de fuite de données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9917HzRuxAi",
   "metadata": {
    "id": "e9917HzRuxAi"
   },
   "outputs": [],
   "source": [
    "# Afficher les informations sur le jeu de données\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cn00ubn5n6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ANALYSE DES DOUBLONS\")\n",
    "print(\"=\" * 60)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Nombre de lignes dupliquées : {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"\\nLignes dupliquées :\")\n",
    "    display(df[df.duplicated(keep=False)])\n",
    "else:\n",
    "    print(\"Aucune ligne dupliquée trouvée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mw8a92853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SÉPARATION DES TYPES DE CARACTÉRISTIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCaractéristiques numériques ({len(numerical_cols)}) :\")\n",
    "print(numerical_cols)\n",
    "\n",
    "print(f\"\\nCaractéristiques catégorielles ({len(categorical_cols)}) :\")\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frr1y7kec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CARACTÉRISTIQUES NUMÉRIQUES - RÉSUMÉ STATISTIQUE\")\n",
    "print(\"=\" * 60)\n",
    "display(df[numerical_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wnbtudgpdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CARACTÉRISTIQUES CATÉGORIELLES - VALEURS UNIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} :\")\n",
    "    print(f\"  Valeurs uniques : {df[col].nunique()}\")\n",
    "    print(f\"  Valeurs : {df[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ezwjvc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation de la Fuite de Données et Identification de la Variable Cible\n",
    "print(\"=\" * 60)\n",
    "print(\"VARIABLE CIBLE ET ÉVALUATION DE LA FUITE DE DONNÉES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identifier la variable cible\n",
    "target_col = 'target'\n",
    "print(f\"\\nVariable cible : '{target_col}'\")\n",
    "print(f\"Classes : {df[target_col].unique().tolist()}\")\n",
    "print(f\"\\nDistribution des classes :\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nProportions des classes :\")\n",
    "print(df[target_col].value_counts(normalize=True).round(3))\n",
    "\n",
    "# Vérifier la composition des caractéristiques\n",
    "print(f\"\\n--- Inventaire des Caractéristiques ---\")\n",
    "print(f\"Caractéristiques totales : {len(df.columns)}\")\n",
    "print(f\"  - Prédicteurs : {len(df.columns) - 1}\")\n",
    "print(f\"  - Cible : 1 ('{target_col}')\")\n",
    "\n",
    "# Vérifier les caractéristiques post-placement qui pourraient divulguer des informations\n",
    "print(f\"\\n--- Vérification de la Fuite de Données ---\")\n",
    "suspicious_keywords = ['salary', 'offer', 'package', 'compensation', 'hired', 'salaire', 'offre', 'rémunération', 'embauché']\n",
    "leakage_found = False\n",
    "\n",
    "for keyword in suspicious_keywords:\n",
    "    if any(keyword in col.lower() for col in df.columns):\n",
    "        print(f\"AVERTISSEMENT : Caractéristique potentielle de fuite contenant '{keyword}' détectée\")\n",
    "        leakage_found = True\n",
    "\n",
    "if not leakage_found:\n",
    "    print(\"Aucune caractéristique évidente de fuite de données détectée.\")\n",
    "    print(\"Toutes les caractéristiques représentent des informations disponibles au moment de la prédiction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqd2bq3x9tl",
   "metadata": {},
   "source": [
    "#### Résumé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8xe1dth30d9",
   "metadata": {},
   "source": [
    "L'analyse initiale des métadonnées révèle un **jeu de données substantiellement plus grand** par rapport aux études de placement typiques, avec **19 158 instances** sur **14 caractéristiques** (13 prédicteurs et 1 cible). Le jeu de données ne présente **aucun enregistrement dupliqué**, garantissant l'intégrité des données. Cependant, **des valeurs manquantes sont présentes** dans plusieurs caractéristiques, avec les lacunes les plus importantes dans `company_type` (6 140 manquantes, 32,1%), `company_size` (5 938 manquantes, 31,0%), `major_discipline` (2 813 manquantes, 14,7%) et `gender` (4 508 manquantes, 23,5%). Cela nécessite des stratégies d'imputation prudentes ou une gestion des valeurs manquantes lors du prétraitement.\n",
    "\n",
    "La composition des caractéristiques se compose de **2 prédicteurs numériques** (`city_development_index` et `training_hours`) et **10 prédicteurs catégoriels** représentant la démographie, l'éducation et l'historique d'emploi. De plus, `enrollee_id` sert d'identifiant unique et doit être exclu de la modélisation, tandis que `target` est la variable de résultat binaire. Les caractéristiques catégorielles présentent une **cardinalité variable** : faible cardinalité pour les caractéristiques binaires comme `relevent_experience` (2 valeurs) et `gender` (3 valeurs incluant manquant), cardinalité modérée pour les caractéristiques comme `education_level` (5 niveaux) et `major_discipline` (6 disciplines), et **cardinalité élevée** pour `city` (123 villes uniques) et `experience` (22 niveaux), qui peuvent nécessiter des techniques d'encodage spécialisées telles que l'encodage cible ou l'encodage de fréquence.\n",
    "\n",
    "La variable cible présente un **déséquilibre de classes significatif**, avec **75,1% des candidats ne cherchant pas de changement d'emploi** (classe 0) et seulement **24,9% recherchant activement un changement** (classe 1), donnant un ratio de déséquilibre de **3,01:1**. Ce déséquilibre substantiel doit être traité pendant l'entraînement du modèle grâce à des techniques telles que la pondération des classes, le rééchantillonnage (SMOTE/sous-échantillonnage) ou l'utilisation de métriques d'évaluation robustes au déséquilibre (score F1, ROC-AUC, courbes de précision-rappel). **Aucune préoccupation de fuite de données** n'a été identifiée ; toutes les caractéristiques représentent des informations collectées lors de l'inscription à la formation, garantissant la validité du modèle pour prédire les intentions réelles de changement d'emploi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6wkcli5oth",
   "metadata": {},
   "source": [
    "### 4.2 Analyse des Valeurs Manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ns9tniisnzm",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ANALYSE DES VALEURS MANQUANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Nombre de valeurs manquantes\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"\\nValeurs manquantes par caractéristique :\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Pourcentage de valeurs manquantes\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"POURCENTAGE DE VALEURS MANQUANTES\")\n",
    "print(\"=\" * 60)\n",
    "missing_percentages = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "print(missing_percentages)\n",
    "\n",
    "# Statistiques récapitulatives\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RÉSUMÉ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total de valeurs manquantes : {total_missing}\")\n",
    "print(f\"Total de cellules : {total_cells}\")\n",
    "print(f\"Taux de valeurs manquantes global : {(total_missing / total_cells * 100):.2f}%\")\n",
    "print(f\"Caractéristiques avec valeurs manquantes : {(missing_counts > 0).sum()} sur {len(df.columns)}\")\n",
    "print(f\"Caractéristiques complètes : {(missing_counts == 0).sum()} sur {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urrc4khdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les valeurs manquantes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Diagramme en barres des valeurs manquantes\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    axes[0].bar(range(len(missing_data)), missing_data.values, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[0].set_xticks(range(len(missing_data)))\n",
    "    axes[0].set_xticklabels(missing_data.index, rotation=45, ha='right')\n",
    "    axes[0].set_title('Nombre de Valeurs Manquantes par Caractéristique', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Caractéristiques')\n",
    "    axes[0].set_ylabel('Nombre de Valeurs Manquantes')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Ajouter les étiquettes de comptage\n",
    "    for i, v in enumerate(missing_data.values):\n",
    "        axes[0].text(i, v + 100, str(v), ha='center', va='bottom')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'Aucune Valeur Manquante', ha='center', va='center', fontsize=14)\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Graphique de pourcentage\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_pct) > 0:\n",
    "    axes[1].bar(range(len(missing_pct)), missing_pct.values, edgecolor='black', alpha=0.7, color='red')\n",
    "    axes[1].set_xticks(range(len(missing_pct)))\n",
    "    axes[1].set_xticklabels(missing_pct.index, rotation=45, ha='right')\n",
    "    axes[1].set_title('Pourcentage de Valeurs Manquantes par Caractéristique', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Caractéristiques')\n",
    "    axes[1].set_ylabel('Pourcentage (%)')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Ajouter les étiquettes de pourcentage\n",
    "    for i, v in enumerate(missing_pct.values):\n",
    "        axes[1].text(i, v + 0.5, f'{v}%', ha='center', va='bottom')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Aucune Valeur Manquante', ha='center', va='center', fontsize=14)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whuo5kvskj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des modèles de valeurs manquantes\n",
    "print(\"=\" * 60)\n",
    "print(\"MODÈLES DE VALEURS MANQUANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Caractéristiques avec valeurs manquantes\n",
    "features_with_missing = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"\\nCaractéristiques avec valeurs manquantes ({len(features_with_missing)}) : {features_with_missing}\")\n",
    "    \n",
    "    # Vérifier la co-occurrence des valeurs manquantes\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CO-OCCURRENCE DES VALEURS MANQUANTES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Vérifier les combinaisons courantes\n",
    "    if 'company_size' in features_with_missing and 'company_type' in features_with_missing:\n",
    "        both_missing = df['company_size'].isnull() & df['company_type'].isnull()\n",
    "        print(f\"\\ncompany_size ET company_type toutes deux manquantes : {both_missing.sum()} ({both_missing.sum() / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    if 'company_size' in features_with_missing and 'company_type' in features_with_missing and 'experience' in features_with_missing:\n",
    "        all_three = df['company_size'].isnull() & df['company_type'].isnull() & df['experience'].isnull()\n",
    "        print(f\"company_size ET company_type ET experience toutes manquantes : {all_three.sum()} ({all_three.sum() / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Distribution du nombre de valeurs manquantes par ligne\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DISTRIBUTION DES VALEURS MANQUANTES PAR LIGNE\")\n",
    "    print(\"=\" * 60)\n",
    "    missing_per_row = df.isnull().sum(axis=1)\n",
    "    print(f\"\\nDistribution des valeurs manquantes :\")\n",
    "    print(missing_per_row.value_counts().sort_index())\n",
    "    \n",
    "    # Lignes avec au moins une valeur manquante\n",
    "    rows_with_missing = df.isnull().any(axis=1).sum()\n",
    "    print(f\"\\n--- Résumé ---\")\n",
    "    print(f\"Total de lignes avec au moins une valeur manquante : {rows_with_missing} ({rows_with_missing / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Lignes avec toutes les valeurs complètes\n",
    "    complete_rows = (~df.isnull().any(axis=1)).sum()\n",
    "    print(f\"Lignes complètes (aucune valeur manquante) : {complete_rows} ({complete_rows / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Modèles de valeurs manquantes les plus courants\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOP 5 DES MODÈLES DE VALEURS MANQUANTES\")\n",
    "    print(\"=\" * 60)\n",
    "    missing_patterns = df[features_with_missing].isnull().astype(int)\n",
    "    pattern_counts = missing_patterns.groupby(features_with_missing).size().sort_values(ascending=False).head(5)\n",
    "    \n",
    "    for idx, (pattern, count) in enumerate(pattern_counts.items(), 1):\n",
    "        missing_features = [feat for feat, is_missing in zip(features_with_missing, pattern) if is_missing == 1]\n",
    "        if missing_features:\n",
    "            print(f\"\\n{idx}. Manquants : {missing_features}\")\n",
    "            print(f\"   Nombre : {count} ({count / len(df) * 100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n{idx}. Aucune valeur manquante\")\n",
    "            print(f\"   Nombre : {count} ({count / len(df) * 100:.2f}%)\")\n",
    "            \n",
    "else:\n",
    "    print(\"\\nAucune valeur manquante détectée dans le jeu de données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sk07yfg2sxf",
   "metadata": {},
   "source": [
    "#### Résumé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lg2fvu8k3nn",
   "metadata": {},
   "source": [
    "L'analyse des valeurs manquantes révèle une **incomplétude significative des données** affectant **53,26% de toutes les lignes** (10 203 instances), tandis que seulement **46,74%** (8 955 instances) sont complètes. Sur **14 caractéristiques**, **8 contiennent des valeurs manquantes** avec un **modèle hiérarchique** : **company_type** (6 140 manquantes, 32,05%) et **company_size** (5 938 manquantes, 30,99%) dominent, suivies de **gender** (4 508 manquantes, 23,53%) et **major_discipline** (2 813 manquantes, 14,68%). Des valeurs manquantes de niveau inférieur apparaissent dans **education_level** (460 manquantes, 2,40%), **last_new_job** (423 manquantes, 2,21%), **enrolled_university** (386 manquantes, 2,01%) et **experience** (65 manquantes, 0,34%). Le taux global de valeurs manquantes du jeu de données est de **7,73%** du total des cellules.\n",
    "\n",
    "**L'analyse des modèles** révèle une **co-occurrence systématique** des valeurs manquantes, confirmant un comportement non aléatoire (MNAR). Les principaux modèles de valeurs manquantes sont : **(1) Lignes complètes sans valeurs manquantes : 46,74%** ; **(2) company_size ET company_type toutes deux manquantes : 14,50%** (2 777 lignes) — indiquant fortement des candidats sans emploi ou des étudiants ; **(3) Seulement gender manquant : 11,61%** (2 224 lignes) ; **(4) major_discipline, company_size ET company_type manquants : 4,42%** (847 lignes) ; **(5) gender, company_size ET company_type manquants : 4,36%** (835 lignes). Notamment, **5 360 lignes (27,98%)** ont les deux caractéristiques d'emploi manquantes ensemble, tandis que seulement **20 lignes (0,10%)** ont les trois caractéristiques liées à l'emploi (company_size, company_type, experience) manquantes simultanément. La distribution des valeurs manquantes par ligne montre que la plupart des lignes affectées ont **1-3 caractéristiques manquantes**, avec une fréquence décroissante pour les nombres plus élevés (628 lignes avec 4 manquantes, 176 avec 5, 62 avec 6 et seulement 12 avec 7).\n",
    "\n",
    "**Stratégie de prétraitement :** Pour **company_size et company_type**, nous allons **créer une catégorie explicite « Non Employé »** plutôt que d'imputer des valeurs, car le modèle de co-occurrence de 27,98% représente clairement des candidats sans emploi actuel (étudiants/débutants) où ces champs ne sont véritablement pas applicables — l'imputation introduirait de fausses informations et obscurcirait cet indicateur significatif du statut d'emploi. Pour les **caractéristiques à faible taux de valeurs manquantes** (education_level, enrolled_university, last_new_job, experience toutes <3%), nous appliquerons une **imputation par mode** car leurs valeurs manquantes sporadiques suggèrent des lacunes aléatoires de collecte de données plutôt que des modèles systématiques, et leur faible prévalence minimise l'impact sur la validité du modèle. Pour **gender et major_discipline**, nous allons **créer des catégories « Inconnu »** car leurs valeurs manquantes indépendantes substantielles (11,61% pour gender seul, 14,68% pour major_discipline) indiquent une réticence à fournir des données ou des préoccupations de confidentialité plutôt qu'une non-applicabilité, et préserver ce signal « non fourni » peut lui-même être prédictif du comportement de changement d'emploi. De plus, nous allons **créer des indicateurs binaires de valeurs manquantes** (`has_employment_info`, `gender_provided`, `education_complete`) car les 53,26% de lignes incomplètes peuvent présenter des comportements de recherche d'emploi distincts, et ces indicateurs pourraient capturer des modèles précieux pour la prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5ds0jss7j",
   "metadata": {},
   "source": [
    "### 4.3 Distributions des Caractéristiques, Mise à l'Échelle et Valeurs Aberrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cl39rkn1jwj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les distributions des caractéristiques numériques (excluant enrollee_id et target)\n",
    "numerical_features_for_viz = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, col in enumerate(numerical_features_for_viz):\n",
    "    axes[idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[idx].set_title(f'Distribution de {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Fréquence')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ja2b8vs26t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détecter les valeurs aberrantes avec des diagrammes en boîte (excluant enrollee_id et target)\n",
    "numerical_features_for_viz = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, col in enumerate(numerical_features_for_viz):\n",
    "    axes[idx].boxplot(df[col].dropna(), vert=True)\n",
    "    axes[idx].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Valeur')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcyttqlj3m8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détection quantitative des valeurs aberrantes en utilisant la méthode IQR (excluant enrollee_id et target)\n",
    "print(\"=\" * 60)\n",
    "print(\"DÉTECTION DES VALEURS ABERRANTES (MÉTHODE IQR)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_features_for_analysis = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "for col in numerical_features_for_analysis:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    \n",
    "    print(f\"\\n{col} :\")\n",
    "    print(f\"  Q1 : {Q1:.2f}, Q3 : {Q3:.2f}, IQR : {IQR:.2f}\")\n",
    "    print(f\"  Limites : [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    print(f\"  Valeurs aberrantes détectées : {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  Valeur aberrante minimale : {df[col][outliers.index].min():.2f}\")\n",
    "        print(f\"  Valeur aberrante maximale : {df[col][outliers.index].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kxjsohtf7di",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des caractéristiques catégorielles (visualiser les catégories principales pour les caractéristiques à haute cardinalité)\n",
    "categorical_features = categorical_cols.copy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features[:10]):\n",
    "    # Pour les caractéristiques à haute cardinalité, afficher seulement les 10 premières\n",
    "    value_counts = df[col].value_counts()\n",
    "    \n",
    "    if len(value_counts) > 10:\n",
    "        value_counts = value_counts.head(10)\n",
    "        title_suffix = \" (Top 10)\"\n",
    "    else:\n",
    "        title_suffix = \"\"\n",
    "    \n",
    "    axes[idx].bar(range(len(value_counts)), value_counts.values, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xticks(range(len(value_counts)))\n",
    "    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=8)\n",
    "    axes[idx].set_title(f'{col}{title_suffix}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=9)\n",
    "    axes[idx].set_ylabel('Nombre', fontsize=9)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Ajouter les étiquettes de comptage sur les barres\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v + max(value_counts.values)*0.01, str(v), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Supprimer les sous-graphes vides\n",
    "for i in range(len(categorical_features[:10]), 12):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimer les statistiques récapitulatives pour toutes les caractéristiques catégorielles\n",
    "print(\"=\" * 60)\n",
    "print(\"RÉSUMÉ DE LA CARDINALITÉ DES CARACTÉRISTIQUES CATÉGORIELLES\")\n",
    "print(\"=\" * 60)\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col} : {unique_count} valeurs uniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdh7cwrgcmn",
   "metadata": {},
   "source": [
    "#### Résumé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cctau1voa9",
   "metadata": {},
   "source": [
    "**Distributions des Caractéristiques Numériques :** Le jeu de données ne contient que **deux prédicteurs numériques significatifs**. L'histogramme de `city_development_index` révèle une **distribution fortement asymétrique vers la gauche** avec une concentration massive aux indices de développement les plus élevés (la plage 0,9-0,95 montre ~7 000+ candidats), indiquant que la plupart des participants à la formation proviennent de centres urbains hautement développés. Le diagramme en boîte confirme les valeurs aberrantes minimales autour de 0,448-0,47. En contraste, `training_hours` affiche une **distribution asymétrique vers la droite** avec le mode aux basses barres (0-50 heures montrant ~2 700 candidats), suivi par des fréquences progressivement décroissantes vers les heures d'entraînement plus élevées. Le diagramme en boîte illustre dramatiquement les **valeurs aberrantes supérieures étendues** (984 instances, 5,1%) s'étirant de ~185 à 336 heures, représentant des candidats ayant un engagement d'entraînement exceptionnel bien au-delà de la limite supérieure IQR de 88 heures.\n",
    "\n",
    "**Analyse des Valeurs Aberrantes :** Nous allons **conserver toutes les valeurs aberrantes** plutôt que de les supprimer. Les valeurs aberrantes de `city_development_index` (17 instances, 0,1%) représentent des candidats de villes moins développées — une minorité légitime et potentiellement informative dont le comportement de recherche d'emploi peut différer de la majorité urbaine. Les valeurs aberrantes de `training_hours` (984 instances, 5,1%) sont particulièrement précieuses : les candidats investissant 185-336 heures dans la formation démontrent un engagement extrême qui pourrait fortement prédire les intentions de changement d'emploi, indiquant soit un perfectionnement actif pour la transition de carrière, soit une formation obligatoire par l'employeur pour les rôles actuels. Supprimer ces 5% de candidats supprimerait une cohorte comportementalement distincte dont le statut de valeur aberrante porte lui-même un signal prédictif. Les valeurs aberrantes représentent une variance comportementale authentique, pas des erreurs de mesure.\n",
    "\n",
    "**Distributions des Caractéristiques Catégorielles :** Les visualisations révèlent des **déséquilibres prononcés dans toutes les caractéristiques**. La **Ville** (montrant les 10 premières sur 123) se concentre lourdement à city_103 (4 355 candidats), les autres villes ayant une représentation dramatiquement plus faible. Le **Genre** affiche une **dominance masculine forte** (13 221 hommes vs 1 238 femmes vs 191 autres), créant un déséquilibre de 10,7:1. L'**Expérience pertinente** est fortement asymétrique vers « Has relevant experience » (13 792 vs 5 366). L'**Université inscrite** montre la plupart comme « no_enrollment » (13 817 vs 3 757 à temps plein vs 1 198 à temps partiel). Le **Niveau d'éducation** est dominé par les **Diplômés** (11 598) suivis par les Masters (4 361), avec une représentation minimale pour l'école secondaire, le doctorat et l'école primaire. La **Discipline majeure** favorise massivement les **STEM** (14 492), éclipsant les diplômes commerciaux, autres, sciences humaines, arts et pas de spécialité. L'**Expérience** (montrant les 10 premières sur 22 niveaux) montre une distribution large avec « >20 » ans en tête (3 286), mais relativement équilibrée sur les plages 5-20 ans. La **Taille de l'entreprise** culmine à « 50-99 » (3 884) avec des fréquences décroissantes pour les tailles plus importantes. Le **Type d'entreprise** est dominé par « **Pvt Ltd** » (10 817), dépassant largement les startups financées, le secteur public, les jeunes startups, les ONG et autres. Le **Dernier nouvel emploi** affiche « 1 » an comme le plus courant (8 040), déclinant pour les écarts plus longs. Ces déséquilibres nécessiteront des stratégies d'encodage prudentes : encodage cible/fréquence pour les caractéristiques à haute cardinalité (city, experience), encodage one-hot pour les caractéristiques à faible cardinalité et attention au déséquilibre des classes lors de l'entraînement du modèle pour éviter les biais vers les classes majoritaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpec963ybwf",
   "metadata": {},
   "source": [
    "### 4.4 Étude de la Variable Cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zpteycwzv7o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Diagramme de comptage\n",
    "target_counts = df['target'].value_counts()\n",
    "axes[0].bar(target_counts.index, target_counts.values, edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "axes[0].set_title('Distribution de la Cible (Nombre)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Cible (0=Pas Recherche, 1=Recherche Changement Emploi)')\n",
    "axes[0].set_ylabel('Nombre')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Diagramme circulaire\n",
    "axes[1].pie(target_counts.values, labels=['Pas Recherche (0)', 'Recherche (1)'], autopct='%1.1f%%', \n",
    "            startangle=90, colors=['red', 'green'], explode=(0.05, 0))\n",
    "axes[1].set_title('Distribution de la Cible (Proportion)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANALYSE DE LA VARIABLE CIBLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDistribution des classes :\")\n",
    "print(target_counts)\n",
    "print(f\"\\nProportions des classes :\")\n",
    "print(df['target'].value_counts(normalize=True).round(3))\n",
    "print(f\"\\nRatio de déséquilibre des classes : {target_counts.max() / target_counts.min():.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zoyq2hjy27i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des caractéristiques numériques par classe cible\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Exclure enrollee_id de la comparaison (c'est juste un identifiant)\n",
    "numerical_features_for_comparison = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "for idx, col in enumerate(numerical_features_for_comparison):\n",
    "    looking = df[df['target'] == 1.0][col]\n",
    "    not_looking = df[df['target'] == 0.0][col]\n",
    "    \n",
    "    axes[idx].hist([not_looking, looking], bins=15, label=['Pas Recherche (0)', 'Recherche (1)'], \n",
    "                   edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "    axes[idx].set_title(f'{col} par Cible', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Fréquence')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ixvk1alzj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison statistique des caractéristiques numériques par classe cible\n",
    "print(\"=\" * 60)\n",
    "print(\"CARACTÉRISTIQUES NUMÉRIQUES - COMPARAISON DES MOYENNES PAR CIBLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_features_for_comparison = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "comparison = df.groupby('target')[numerical_features_for_comparison].mean()\n",
    "comparison.index = ['Pas Recherche (0)', 'Recherche (1)']\n",
    "print(\"\\nValeurs moyennes par classe cible :\")\n",
    "print(comparison.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIFFÉRENCE (Recherche - Pas Recherche)\")\n",
    "print(\"=\" * 60)\n",
    "difference = comparison.loc['Recherche (1)'] - comparison.loc['Pas Recherche (0)']\n",
    "print(difference.round(2))\n",
    "\n",
    "# Visualiser la comparaison des moyennes\n",
    "comparison.T.plot(kind='bar', figsize=(10, 5), edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "plt.title('Comparaison des Moyennes des Caractéristiques Numériques par Cible', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Caractéristiques')\n",
    "plt.ylabel('Valeur Moyenne')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Cible')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w229mmkye5l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caractéristiques catégorielles vs cible - taux de changement d'emploi\n",
    "categorical_features_for_analysis = [col for col in categorical_cols]\n",
    "\n",
    "# Limiter aux catégories principales pour les caractéristiques à haute cardinalité comme 'city'\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features_for_analysis[:10]):\n",
    "    if col == 'city':\n",
    "        # Pour city, afficher seulement les 10 premières villes\n",
    "        top_cities = df[col].value_counts().head(10).index\n",
    "        df_subset = df[df[col].isin(top_cities)]\n",
    "        ct = pd.crosstab(df_subset[col], df_subset['target'], normalize='index') * 100\n",
    "    else:\n",
    "        ct = pd.crosstab(df[col], df['target'], normalize='index') * 100\n",
    "    \n",
    "    ct.plot(kind='bar', ax=axes[idx], edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "    axes[idx].set_title(f'Taux de Changement d\\'Emploi par {col}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Pourcentage (%)')\n",
    "    axes[idx].legend(['Pas Recherche (0)', 'Recherche (1)'], fontsize=7)\n",
    "    axes[idx].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Supprimer les sous-graphes vides\n",
    "for i in range(len(categorical_features_for_analysis[:10]), 12):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0rabbgh7t4ip",
   "metadata": {},
   "source": [
    "#### Résumé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mywig6bawtj",
   "metadata": {},
   "source": [
    "**Distribution de la Classe Cible :** La variable cible présente un **déséquilibre de classe sévère** avec **14 381 candidats (75,1%) ne cherchant pas de changement d'emploi** par rapport à **4 777 candidats (24,9%) recherchant activement un changement**, donnant un **ratio de déséquilibre de 3,01:1**. Ce déséquilibre significatif nécessitera un traitement prudent pendant l'entraînement du modèle grâce à la pondération des classes, au rééchantillonnage SMOTE ou à l'échantillonnage stratifié pour éviter que le modèle ne se contente de prédictions de classe majoritaire et ne réalise une précision trompeusement élevée tout en échouant à identifier les candidats cherchant un emploi.\n",
    "\n",
    "**Caractéristiques Numériques vs Cible :** Les histogrammes comparatifs révèlent un **pouvoir discriminatoire minimal** pour les deux caractéristiques numériques. `city_development_index` montre des distributions presque identiques pour les deux classes, toutes deux concentrées aux indices de développement élevés (plage 0,9-0,95). La comparaison des moyennes confirme cela : **Pas Recherche : 0,83** vs **Recherche : 0,81** — une différence triviale de seulement **-0,02**. De même, `training_hours` affiche des distributions qui se chevauchent avec les deux classes culminant à 0-50 heures, bien que les candidats cherchant un emploi montrent légèrement une diffusion plus large vers les heures d'entraînement plus élevées. La différence moyenne est également minimale : **Pas Recherche : 65,70** vs **Recherche : 64,35** — une différence de **-1,35 heures**. La visualisation du diagramme en barres illustre dramatiquement ces différences négligeables, les deux caractéristiques montrant des valeurs moyennes presque identiques entre les classes. Cela suggère que **ni l'une ni l'autre des caractéristiques numériques ne fournit un signal prédictif fort** seule, indiquant que le modèle devra s'appuyer fortement sur les caractéristiques catégorielles et les interactions entre caractéristiques.\n",
    "\n",
    "**Caractéristiques Catégorielles vs Cible :** L'analyse des caractéristiques catégorielles révèle une **variation substantielle de la propension à la recherche d'emploi** entre différents segments. **La Ville** montre une variance dramatique, city_21 présentant le **taux de changement d'emploi le plus élevé (~60%)** tandis que la plupart des autres villes principales se situent autour de 10-20%. **Le Genre** révèle que les **femmes (26%) ont des taux de recherche d'emploi plus élevés que les hommes (22%)**, contredisant les hypothèses courantes. **L'Expérience Pertinente** montre un modèle frappant : les candidats **sans expérience pertinente ont un taux de changement d'emploi de 35%** par rapport à seulement **20% pour les candidats expérimentés** — suggérant que les professionnels moins établis sont plus mobiles. **L'Université Inscrite** démontre que les **étudiants à temps plein ont la mobilité la plus élevée (38%)**, suivis par les étudiants à temps partiel (25%), tandis que les non-inscrits affichent les taux les plus bas (20%). Le **Niveau d'Éducation** révèle que les **Diplômés et Diplômés de l'École Secondaire** ont une mobilité plus élevée (~28%) comparés aux Masters, Doctorats et Diplômés de l'École Primaire. La **Discipline Majeure** affiche des taux relativement **équilibrés (20-28%) dans tous les domaines**. Les **Niveaux d'Expérience** affichent une intéressante non-linéarité : les **candidats débutants (<1 an) et très expérimentés (>20 ans) affichent des taux de recherche d'emploi plus élevés (~25-30%)** comparés aux professionnels en milieu de carrière (15-20%). La **Taille de l'Entreprise** révèle que les **petites entreprises (<10, 10/49) ont une attrition plus élevée (~23%)** par rapport aux plus grandes corporations (~15-18%). Le **Type d'Entreprise** affiche que les **Jeunes Startups ont la mobilité la plus élevée (~24%)** tandis que le **Secteur Public** affiche la mobilité la plus faible (~18%). Le **Dernier Nouvel Emploi** démontre que les candidats **ayant « jamais » changé d'emploi ont le taux le plus élevé (~32%)**, suggérant les candidats pour leur premier emploi, tandis que les changeurs récents (1-2 ans) montrent ~22-25%. Ces modèles catégoriels seront **cruciaux pour les prédictions du modèle**, car ils révèlent des segments comportementaux distincts avec une propension variable à la recherche d'emploi que les caractéristiques numériques ne parviennent pas à capturer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ido0ujpu1m",
   "metadata": {},
   "source": [
    "### 4.5 Corrélation et Sélection des Caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cq5lfcgssn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de corrélation des caractéristiques numériques\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRÉLATION DES CARACTÉRISTIQUES NUMÉRIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Corréler seulement les caractéristiques numériques significatives (exclure enrollee_id)\n",
    "numerical_features_for_correlation = [col for col in numerical_cols if col not in ['enrollee_id']]\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "corr_matrix = df[numerical_features_for_correlation].corr()\n",
    "\n",
    "print(\"\\nMatrice de Corrélation :\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Visualiser la carte thermique de corrélation\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, fmt='.3f')\n",
    "plt.title('Carte Thermique de Corrélation - Caractéristiques Numériques', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cxjzqv8hg3",
   "metadata": {},
   "source": [
    "#### Corrélation des Caractéristiques Catégorielles (V de Cramér)\n",
    "\n",
    "Le V de Cramér mesure l'association entre variables catégorielles (0 = aucune, 1 = parfaite).\n",
    "- **V < 0,1** : Négligeable | **0,1-0,3** : Faible | **0,3-0,5** : Modérée | **V ≥ 0,5** : Forte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jz36j1hlnq",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.contingency import association\n",
    "\n",
    "# Calculer la matrice V de Cramér pour les caractéristiques catégorielles\n",
    "categorical_features_for_corr = [col for col in categorical_cols]\n",
    "n_features = len(categorical_features_for_corr)\n",
    "cramers_matrix = np.zeros((n_features, n_features))\n",
    "\n",
    "for i, col1 in enumerate(categorical_features_for_corr):\n",
    "    for j, col2 in enumerate(categorical_features_for_corr):\n",
    "        if i == j:\n",
    "            cramers_matrix[i, j] = 1.0\n",
    "        elif i < j:\n",
    "            mask = df[col1].notna() & df[col2].notna()\n",
    "            contingency = pd.crosstab(df.loc[mask, col1], df.loc[mask, col2])\n",
    "            v = association(contingency, method='cramer')\n",
    "            cramers_matrix[i, j] = v\n",
    "            cramers_matrix[j, i] = v\n",
    "\n",
    "# Visualiser\n",
    "cramers_df = pd.DataFrame(cramers_matrix, index=categorical_features_for_corr, columns=categorical_features_for_corr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramers_df, annot=True, cmap='YlOrRd', square=True, fmt='.2f', vmin=0, vmax=1)\n",
    "plt.title(\"V de Cramér - Corrélation des Caractéristiques Catégorielles\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccabs6yjhv",
   "metadata": {},
   "source": [
    "#### Tests du Chi-Carré : Caractéristiques Catégorielles vs Cible\n",
    "\n",
    "Test d'indépendance pour évaluer l'association entre chaque caractéristique catégorielle et la variable cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zdvvamjs4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Tests du chi-carré : caractéristiques catégorielles vs cible\n",
    "chi2_results = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    mask = df[col].notna()\n",
    "    contingency = pd.crosstab(df.loc[mask, col], df.loc[mask, 'target'])\n",
    "    chi2, p_value, dof, _ = chi2_contingency(contingency)\n",
    "    v = association(contingency, method='cramer')\n",
    "    \n",
    "    chi2_results.append({\n",
    "        'Caractéristique': col,\n",
    "        'Chi-carré': round(chi2, 2),\n",
    "        'p-valeur': p_value,\n",
    "        \"V de Cramér\": round(v, 3)\n",
    "    })\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_results).sort_values(\"V de Cramér\", ascending=False)\n",
    "print(\"Tests du Chi-Carré : Caractéristiques Catégorielles vs Cible\")\n",
    "print(chi2_df.to_string(index=False))\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(chi2_df['Caractéristique'], chi2_df[\"V de Cramér\"], color='steelblue', edgecolor='black')\n",
    "plt.xlabel(\"V de Cramér\")\n",
    "plt.title(\"Force d'Association avec la Cible\", fontsize=12, fontweight='bold')\n",
    "plt.axvline(x=0.1, color='orange', linestyle='--', label='Faible (0.1)')\n",
    "plt.axvline(x=0.3, color='red', linestyle='--', label='Modérée (0.3)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ilbo2lx1xpj",
   "metadata": {},
   "source": [
    "#### Résumé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qqjspyjuzqo",
   "metadata": {},
   "source": [
    "**Corrélation numérique :** Les deux caractéristiques numériques (`city_development_index` et `training_hours`) sont indépendantes (r = 0,002). Seul `city_development_index` montre une corrélation modérée avec la cible (r = -0,34), indiquant que les candidats des villes moins développées cherchent davantage à changer d'emploi.\n",
    "\n",
    "**Corrélation catégorielle (V de Cramér) :** La matrice révèle des associations modérées entre certaines caractéristiques liées à l'expérience professionnelle :\n",
    "- `relevent_experience` ↔ `experience` (V = 0,40)\n",
    "- `relevent_experience` ↔ `enrolled_university` / `last_new_job` (V = 0,39)\n",
    "- `relevent_experience` ↔ `education_level` (V = 0,32)\n",
    "\n",
    "Ces corrélations sont logiques (progression de carrière) mais restent modérées, sans multicolinéarité sévère.\n",
    "\n",
    "**Association avec la cible (Chi-carré) :** Toutes les caractéristiques catégorielles sont statistiquement significatives (p < 0,05). Classement par force d'association :\n",
    "- **Modérée (V ≥ 0,3) :** `city` (0,396) — meilleur prédicteur catégoriel\n",
    "- **Faible (0,1-0,3) :** `experience` (0,192), `enrolled_university` (0,156), `relevent_experience` (0,128)\n",
    "- **Négligeable (V < 0,1) :** `education_level`, `last_new_job`, `company_size`, `company_type`, `major_discipline`, `gender`\n",
    "\n",
    "**Conclusion :** `city` et `city_development_index` sont les prédicteurs les plus forts. Les caractéristiques liées à l'expérience (`experience`, `enrolled_university`, `relevent_experience`) forment un second groupe de prédicteurs utiles. Les caractéristiques démographiques (`gender`, `major_discipline`) ont un pouvoir prédictif faible mais restent significatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1uutp5z6qr",
   "metadata": {},
   "source": [
    "### 4.6 Clustering Non Supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8emgkzabdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données pour le clustering\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sélection des features (sans enrollee_id et target)\n",
    "X_cluster = df.drop(['enrollee_id', 'target'], axis=1)\n",
    "\n",
    "# Identifier les colonnes\n",
    "num_cols = X_cluster.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols = X_cluster.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Pipeline de prétraitement\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), num_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# Transformer les données\n",
    "X_processed = preprocessor.fit_transform(X_cluster)\n",
    "print(f\"Données prétraitées: {X_processed.shape[0]} samples, {X_processed.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "poy4lmwci0p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode du coude (Elbow Method)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_processed)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Nombre de clusters (k)')\n",
    "plt.ylabel('Inertie')\n",
    "plt.title('Méthode du Coude', fontsize=13, fontweight='bold')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jx93sby6isl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application K-Means (choisir k après avoir vu le coude)\n",
    "k_optimal = 4  # À ajuster selon le graphique du coude\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_processed)\n",
    "\n",
    "# Réduction PCA pour visualisation\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_processed)\n",
    "\n",
    "# Visualisation des clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title(f'K-Means Clustering (k={k_optimal}) - Projection PCA', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variance expliquée par PCA: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zyofvj1ttk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des clusters vs target\n",
    "df_clusters = df.copy()\n",
    "df_clusters['cluster'] = clusters\n",
    "\n",
    "# Distribution du target par cluster\n",
    "cluster_analysis = df_clusters.groupby('cluster')['target'].agg(['count', 'sum', 'mean'])\n",
    "cluster_analysis.columns = ['Total', 'Looking for Change', 'Rate (%)']\n",
    "cluster_analysis['Rate (%)'] = (cluster_analysis['Rate (%)'] * 100).round(1)\n",
    "print(\"Distribution du target par cluster:\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Taille des clusters\n",
    "axes[0].bar(cluster_analysis.index, cluster_analysis['Total'], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Cluster')\n",
    "axes[0].set_ylabel('Nombre de candidats')\n",
    "axes[0].set_title('Taille des clusters', fontweight='bold')\n",
    "\n",
    "# Taux de recherche d'emploi par cluster\n",
    "colors = plt.cm.RdYlGn_r(cluster_analysis['Rate (%)'] / 100)\n",
    "axes[1].bar(cluster_analysis.index, cluster_analysis['Rate (%)'], color=colors, edgecolor='black')\n",
    "axes[1].axhline(y=df['target'].mean()*100, color='red', linestyle='--', label=f'Moyenne globale ({df[\"target\"].mean()*100:.1f}%)')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Taux de recherche (%)')\n",
    "axes[1].set_title('Taux de recherche d\\'emploi par cluster', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcr5cnxcxhj",
   "metadata": {},
   "source": [
    "#### Résumé\n",
    "\n",
    "Le clustering K-Means (k=4) révèle **4 segments de candidats** avec des comportements distincts vis-à-vis de la recherche d'emploi:\n",
    "\n",
    "- **Cluster 1** (~8,800 candidats, 13%): Profil le plus stable, taux de recherche bien inférieur à la moyenne\n",
    "- **Cluster 3** (~4,800 candidats, 47%): Profil à **haut risque d'attrition**, taux presque 2x supérieur à la moyenne globale (24.9%)\n",
    "- **Clusters 0 et 2**: Comportements intermédiaires (18% et 27%)\n",
    "\n",
    "La variance expliquée par PCA (29.6%) est limitée en raison du grand nombre de features catégorielles encodées, mais les clusters restent visuellement distincts. Le fait que le clustering non-supervisé identifie des groupes corrélés au target **sans l'avoir utilisé** confirme que les features contiennent des patterns prédictifs exploitables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1xqgtjif38",
   "metadata": {},
   "source": [
    "### 4.7 Interprétations et Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6jchjowi29",
   "metadata": {},
   "source": [
    "L'analyse exploratoire a révélé les points clés suivants :\n",
    "\n",
    "**Données :**\n",
    "- 19 158 candidats, 12 caractéristiques prédictives (2 numériques, 10 catégorielles)\n",
    "- Déséquilibre de classes : 75% / 25% (ratio 3:1)\n",
    "- Valeurs manquantes significatives (53% des lignes affectées), notamment `company_size` et `company_type` (candidats sans emploi)\n",
    "\n",
    "**Caractéristiques les plus prédictives :**\n",
    "| Caractéristique | Type | Association avec cible |\n",
    "|---------|------|------------------------|\n",
    "| city | Catégorielle | V = 0,396 (modérée) |\n",
    "| city_development_index | Numérique | r = -0,342 (modérée) |\n",
    "| experience | Catégorielle | V = 0,192 (faible) |\n",
    "| enrolled_university | Catégorielle | V = 0,156 (faible) |\n",
    "| relevent_experience | Catégorielle | V = 0,128 (faible) |\n",
    "\n",
    "**Perspicacités Métier :**\n",
    "- Les candidats des **villes moins développées** cherchent davantage à changer d'emploi\n",
    "- Les **étudiants à temps plein** et candidats **sans expérience pertinente** sont plus mobiles\n",
    "- Le clustering identifie un segment à **haut risque (47%)** vs un segment **stable (13%)**\n",
    "\n",
    "**Implications pour la Modélisation :**\n",
    "1. Gérer le déséquilibre de classes (class_weight, SMOTE)\n",
    "2. Traiter les valeurs manquantes (imputation + catégorie « Inconnu »)\n",
    "3. Encoder les caractéristiques à haute cardinalité (`city` : 123 valeurs)\n",
    "4. Privilégier les modèles non-linéaires (Random Forest, XGBoost) pour capturer les interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqc8z5vqw4l",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Modèles de Référence et Ensembles ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gt7bbs25u3k",
   "metadata": {},
   "source": [
    "### 5.1 Divisions Entraînement/Validation/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xamw7fik04a",
   "metadata": {},
   "source": [
    "Division des données en **3 ensembles distincts** pour une évaluation rigoureuse :\n",
    "\n",
    "| Ensemble | Proportion | Rôle |\n",
    "|----------|------------|------|\n",
    "| **Entraînement** | 70% | Apprentissage des modèles |\n",
    "| **Validation** | 15% | Ajustement des hyperparamètres (section 6) |\n",
    "| **Test** | 15% | Évaluation finale (jamais vus pendant l'entraînement) |\n",
    "\n",
    "**Points clés :**\n",
    "- *Stratification* → préserve le ratio de classes (75/25) dans chaque ensemble\n",
    "- Suppression de `enrollee_id` → identifiant sans valeur prédictive\n",
    "- `random_state=42` → reproductibilité des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1y89f4ne4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "X = df.drop(['enrollee_id', 'target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split 70/15/15 stratifié\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Vérification\n",
    "print(f\"Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w0em90lymgs",
   "metadata": {},
   "source": [
    "### 5.2 Pipelines et Modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nl95wzkjc9p",
   "metadata": {},
   "source": [
    "Construction de **pipelines** qui enchaînent prétraitement et modèle en une seule étape :\n",
    "\n",
    "**Prétraitement automatique :**\n",
    "- *Caractéristiques numériques* → Imputation (médiane) + Standardisation\n",
    "- *Caractéristiques catégorielles* → Imputation (mode) + Encodage One-Hot\n",
    "\n",
    "**Modèles de référence :**\n",
    "| Modèle | Type | Pourquoi |\n",
    "|--------|------|----------|\n",
    "| **Régression Logistique** | Linéaire | Simple, interprétable, référence rapide |\n",
    "| **Forêt Aléatoire** | Ensemble | Capture les interactions non-linéaires |\n",
    "\n",
    "**Remarque :** `class_weight='balanced'` → gère automatiquement le déséquilibre 75/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n3uayft3v9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des types de features\n",
    "num_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Préprocesseur\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_features),\n",
    "    ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), cat_features)\n",
    "])\n",
    "\n",
    "# Pipelines avec modèles\n",
    "lr_pipeline = Pipeline([('preprocessor', preprocessor), ('model', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))])\n",
    "rf_pipeline = Pipeline([('preprocessor', preprocessor), ('model', RandomForestClassifier(class_weight='balanced', random_state=42))])\n",
    "\n",
    "print(f\"Features: {len(num_features)} numériques, {len(cat_features)} catégorielles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7p38rbqmjzj",
   "metadata": {},
   "source": [
    "### 5.3 Entraînement et Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tvesz5yfps",
   "metadata": {},
   "source": [
    "**Validation croisée 5-fold** sur l'ensemble d'entraînement pour évaluer la performance des modèles de manière robuste.\n",
    "\n",
    "*Principe :* L'ensemble d'entraînement est divisé en 5 parties → on entraîne sur 4, on valide sur 1, et on répète 5 fois.\n",
    "\n",
    "**Métriques évaluées :**\n",
    "- `accuracy` → % de prédictions correctes\n",
    "- `f1_weighted` → équilibre précision/rappel (adapté aux classes déséquilibrées)\n",
    "- `roc_auc` → capacité à discriminer les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8y4eycfir0j",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scoring = ['accuracy', 'f1_weighted', 'roc_auc']\n",
    "\n",
    "# Cross-validation pour les deux modèles\n",
    "results = {}\n",
    "for name, pipeline in [('Logistic Regression', lr_pipeline), ('Random Forest', rf_pipeline)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    results[name] = {}\n",
    "    for metric in scoring:\n",
    "        scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=metric)\n",
    "        results[name][metric] = scores.mean()\n",
    "        print(f\"  {metric}: {scores.mean():.3f} (+/- {scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rcsmc5lyfa",
   "metadata": {},
   "source": [
    "### 5.4 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69367e8",
   "metadata": {},
   "source": [
    "### 5.4 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n3mxevhq1k",
   "metadata": {},
   "source": [
    "**Évaluation finale** sur l'ensemble de test (données jamais vues pendant l'entraînement).\n",
    "\n",
    "- Entraînement sur `X_train` complet\n",
    "- Prédiction et évaluation sur `X_test`\n",
    "- **Rapport de classification** → précision, rappel, F1 par classe\n",
    "- **Matrice de confusion** → visualisation des erreurs\n",
    "- **Importance des caractéristiques** → comparaison RF (*Importance de Gini*) vs LR (*coefficients*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l7y521g4zp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement et évaluation sur test set\n",
    "models = {'Logistic Regression': lr_pipeline, 'Random Forest': rf_pipeline}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\n{name}\\n{'='*50}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# Matrice de confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{name}')\n",
    "    axes[idx].set_xlabel('Prédit')\n",
    "    axes[idx].set_ylabel('Réel')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qw6kpu3stal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance - Comparaison des deux modèles\n",
    "feature_names = num_features + list(rf_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Random Forest - Feature Importances\n",
    "rf_importances = rf_pipeline.named_steps['model'].feature_importances_\n",
    "top_rf = np.argsort(rf_importances)[-15:]\n",
    "axes[0].barh(range(15), rf_importances[top_rf], color='steelblue')\n",
    "axes[0].set_yticks(range(15))\n",
    "axes[0].set_yticklabels([feature_names[i] for i in top_rf])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest (Gini Importance)', fontweight='bold')\n",
    "\n",
    "# Logistic Regression - Coefficients (valeur absolue)\n",
    "lr_coefs = np.abs(lr_pipeline.named_steps['model'].coef_[0])\n",
    "top_lr = np.argsort(lr_coefs)[-15:]\n",
    "axes[1].barh(range(15), lr_coefs[top_lr], color='coral')\n",
    "axes[1].set_yticks(range(15))\n",
    "axes[1].set_yticklabels([feature_names[i] for i in top_lr])\n",
    "axes[1].set_xlabel('|Coefficient|')\n",
    "axes[1].set_title('Logistic Regression (Coefficient Magnitude)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4wn484c6q",
   "metadata": {},
   "source": [
    "### 5.5 Interprétation et Discussion des Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sgg1wo5i2bn",
   "metadata": {},
   "source": [
    "**Comparaison des modèles de référence :**\n",
    "\n",
    "| Métrique | Régression Logistique | Forêt Aléatoire | Meilleur |\n",
    "|----------|---------------------|---------------|----------|\n",
    "| Accuracy | 0,73 | **0,77** | RF |\n",
    "| ROC-AUC | **0,777** | 0,765 | LR |\n",
    "| Rappel (classe 1) | **0,70** | 0,39 | LR |\n",
    "| F1 (classe 1) | **0,57** | 0,45 | LR |\n",
    "\n",
    "**Conclusions :**\n",
    "\n",
    "1. **La Régression Logistique surpasse la Forêt Aléatoire** pour l'objectif métier (détecter les candidats cherchant un emploi)\n",
    "   - Détecte 70% des candidats en recherche d'emploi vs seulement 39% pour RF\n",
    "   - Meilleur ROC-AUC (0,777 vs 0,765)\n",
    "\n",
    "2. **L'accuracy est trompeuse** avec des classes déséquilibrées\n",
    "   - RF a 77% d'accuracy mais rate 61% des candidats à risque\n",
    "\n",
    "3. **L'importance des caractéristiques révèle des stratégies différentes :**\n",
    "   - *LR* → exploite fortement la variable `city` (meilleur prédicteur identifié en EDA)\n",
    "   - *RF* → disperse sur `training_hours` malgré sa faible corrélation linéaire\n",
    "\n",
    "**Prochaine étape (Section 6) :** Optimiser les hyperparamètres pour améliorer les performances, notamment le rappel de la classe 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cvggwxipyvh",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Modèles Améliorés et Optimisation des Hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kg16rtxrd5",
   "metadata": {},
   "source": [
    "### 6.1 Justification des Choix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "begk7qkxzl",
   "metadata": {},
   "source": [
    "**Choix du modèle : HistGradientBoostingClassifier**\n",
    "\n",
    "*Justification basée sur notre analyse :*\n",
    "- **EDA (Section 4) :** Déséquilibre de classes (75/25), `city` est le meilleur prédicteur (V=0,396), caractéristiques catégorielles à haute cardinalité\n",
    "- **Références (Section 5) :** Forêt Aléatoire sous-performe sur le rappel classe 1 (0,39), LR meilleure mais limitée aux relations linéaires\n",
    "- **Cours :** « RandomForest, AdaBoost, GBRT, et HGB sont parmi les premiers modèles à tester »\n",
    "\n",
    "*Pourquoi HistGradientBoosting :*\n",
    "- **Gradient Boosting** → souvent meilleur que Forêt Aléatoire sur données tabulaires\n",
    "- Implémentation **optimisée** dans scikit-learn (rapide)\n",
    "- Gère bien les **interactions non-linéaires** entre caractéristiques\n",
    "\n",
    "*Prétraitement :* Même pipeline que les références (imputation + encodage one-hot) pour comparaison équitable.\n",
    "\n",
    "**Métriques d'optimisation :** `roc_auc` et `f1_weighted` (adaptées au déséquilibre de classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m27bj9o9qf",
   "metadata": {},
   "source": [
    "### 6.2 Optimisation des Hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5mm80lfd4l",
   "metadata": {},
   "source": [
    "**GridSearchCV** pour trouver les meilleurs hyperparamètres :\n",
    "- `max_depth` → profondeur des arbres (contrôle l'overfitting)\n",
    "- `learning_rate` → vitesse d'apprentissage\n",
    "- `max_iter` → nombre d'arbres dans l'ensemble\n",
    "\n",
    "Validation sur `X_val` (ensemble de validation créé en 5.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tl2fadk15o",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Pipeline avec preprocessor (réutilise celui de la section 5.2)\n",
    "hgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', HistGradientBoostingClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Grille d'hyperparamètres (préfixe 'model__' pour le pipeline)\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'model__max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "# GridSearchCV avec les deux métriques\n",
    "results_grid = {}\n",
    "for scoring in ['roc_auc', 'f1_weighted']:\n",
    "    print(f\"\\n{'='*50}\\nOptimisation pour: {scoring}\\n{'='*50}\")\n",
    "    \n",
    "    grid_search = GridSearchCV(hgb_pipeline, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    results_grid[scoring] = grid_search\n",
    "    print(f\"Meilleurs paramètres: {grid_search.best_params_}\")\n",
    "    print(f\"Meilleur score CV: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ftpfpzn8gl8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des deux modèles optimisés (ROC-AUC vs F1)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for idx, (metric, label) in enumerate([('roc_auc', 'ROC-AUC'), ('f1_weighted', 'F1-weighted')]):\n",
    "    best_model = results_grid[metric].best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\nHGB optimisé pour {label}\\n{'='*50}\")\n",
    "    print(f\"Params: {results_grid[metric].best_params_}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(f'HGB optimisé {label}')\n",
    "    axes[idx].set_xlabel('Prédit')\n",
    "    axes[idx].set_ylabel('Réel')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qxdtuqjiu4b",
   "metadata": {},
   "source": [
    "### 6.3 Résultats Finaux et Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9kggnmk5w1v",
   "metadata": {},
   "source": [
    "**Comparaison finale de tous les modèles (Ensemble de Test) :**\n",
    "\n",
    "| Modèle | ROC-AUC | Rappel (classe 1) | F1 (classe 1) | Candidats détectés |\n",
    "|--------|---------|-------------------|---------------|-------------------|\n",
    "| Régression Logistique | 0,777 | 0,70 | 0,57 | 504/716 (70%) |\n",
    "| Forêt Aléatoire | 0,765 | 0,39 | 0,45 | 276/716 (39%) |\n",
    "| **HGB (ROC-AUC)** | **0,784** | **0,76** | **0,59** | **542/716 (76%)** |\n",
    "| HGB (F1) | 0,785 | 0,74 | 0,59 | 530/716 (74%) |\n",
    "\n",
    "**Meilleur modèle : HistGradientBoosting optimisé pour ROC-AUC**\n",
    "\n",
    "*Hyperparamètres :* `max_depth=5`, `learning_rate=0,05`, `max_iter=100`\n",
    "\n",
    "**Gains par rapport aux références :**\n",
    "- **+0,7%** ROC-AUC vs Régression Logistique\n",
    "- **+6 points** de rappel vs LR → détecte 38 candidats supplémentaires\n",
    "- **+37 points** de rappel vs Forêt Aléatoire → détecte 266 candidats supplémentaires\n",
    "\n",
    "**Interprétation métier :** Sur 716 candidats cherchant un emploi, le modèle optimisé en identifie correctement **542 (76%)**, contre 504 pour LR et seulement 276 pour RF. Cette amélioration permet un ciblage plus efficace des candidats à risque d'attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ur24e8fjq9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f01f20",
   "metadata": {},
   "source": [
    "**Objectif :** Prédire les candidats cherchant à changer d'emploi à partir de leurs caractéristiques démographiques, éducatives et professionnelles.\n",
    "\n",
    "**Principales découvertes (EDA) :**\n",
    "- Jeu de données de 19 158 candidats avec déséquilibre de classes (75/25)\n",
    "- `city` et `city_development_index` sont les meilleurs prédicteurs\n",
    "- Le clustering révèle un segment à haut risque (47% de mobilité)\n",
    "\n",
    "**Comparaison des modèles :**\n",
    "\n",
    "| Modèle | ROC-AUC | Rappel (classe 1) |\n",
    "|--------|---------|-------------------|\n",
    "| Régression Logistique | 0,777 | 70% |\n",
    "| Forêt Aléatoire | 0,765 | 39% |\n",
    "| **HGB (Optimisé)** | **0,784** | **76%** |\n",
    "\n",
    "**Meilleur modèle :** HistGradientBoostingClassifier\n",
    "- Hyperparamètres : `max_depth=5`, `learning_rate=0,05`, `max_iter=100`\n",
    "- Détecte **76% des candidats** cherchant un emploi (542/716)\n",
    "\n",
    "**Limites :**\n",
    "- Déséquilibre de classes impacte la précision (49% de faux positifs)\n",
    "- Caractéristique `city` à haute cardinalité (123 valeurs) → encodage one-hot crée 123 caractéristiques\n",
    "\n",
    "**Pistes d'amélioration :**\n",
    "- Encodage cible pour `city` (réduire la dimensionnalité)\n",
    "- Tester d'autres modèles (XGBoost, LightGBM)\n",
    "- Ajuster le seuil de décision pour équilibrer précision/rappel"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
