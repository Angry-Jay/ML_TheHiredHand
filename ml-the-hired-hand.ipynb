{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yZg8yXCtjE3J",
   "metadata": {
    "id": "yZg8yXCtjE3J"
   },
   "source": [
    "# The Hired Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h_wT-bE8f0JQ",
   "metadata": {
    "id": "h_wT-bE8f0JQ"
   },
   "source": [
    "**Machine Learning for Job Placement Prediction**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Angry-Jay/ML_TheHiredHand/blob/main/ml-the-hired-hand.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Project & Dataset Description](#1-project--dataset-description)\n",
    "   - [1.1 Project Aim](#11-project-aim)\n",
    "   - [1.2 Existing Solutions](#12-existing-solutions)\n",
    "   - [1.3 Dataset Information](#13-dataset-information)\n",
    "2. [Library Imports](#2-library-imports)\n",
    "3. [Data Access](#3-data-access)\n",
    "4. [Dataset Exploratory Analysis](#4-dataset-exploratory-analysis)\n",
    "   - [4.1 Metadata Analysis](#41-metadata-analysis)\n",
    "   - [4.2 Missing Values Analysis](#42-missing-values-analysis)\n",
    "   - [4.3 Feature Distributions, Scaling & Outliers](#43-feature-distributions-scaling--outliers)\n",
    "   - [4.4 Target Feature Study](#44-target-feature-study)\n",
    "   - [4.5 Feature Correlation & Selection](#45-feature-correlation--selection)\n",
    "   - [4.6 Unsupervised Clustering](#46-unsupervised-clustering)\n",
    "   - [4.7 Interpretations & Conclusions](#47-interpretations--conclusions)\n",
    "5. [ML Baseline & Ensemble Models](#5-ml-baseline--ensemble-models)\n",
    "   - [5.1 Train/Validation/Test Splits](#51-trainvalidationtest-splits)\n",
    "   - [5.2 Pipelines & Models](#52-pipelines--models)\n",
    "   - [5.3 Training & Validation](#53-training--validation)\n",
    "   - [5.4 Testing](#54-testing)\n",
    "   - [5.5 Results Interpretation & Discussion](#55-results-interpretation--discussion)\n",
    "6. [Enhanced Models & Hyperparameter Tuning](#6-enhanced-models--hyperparameter-tuning)\n",
    "   - [6.1 Justification of Choices](#61-justification-of-choices)\n",
    "   - [6.2 Hyperparameter Optimization](#62-hyperparameter-optimization)\n",
    "   - [6.3 Final Results & Analysis](#63-final-results--analysis)\n",
    "7. [Conclusion & Future Work](#7-conclusion--future-work)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project & Dataset Description\n",
    "\n",
    "### 1.1 Project Aim\n",
    "\n",
    "This project applies Machine Learning techniques to predict employment outcomes for graduating students using the **Job Placement Dataset**. \n",
    "\n",
    "**Primary Objectives:**\n",
    "- **Predict employment outcomes** (Placed vs. Not Placed) based on demographic, academic, and professional attributes\n",
    "- **Demonstrate a coherent ML methodology** from data discovery through model optimization\n",
    "- **Apply comprehensive data analysis** including:\n",
    "  - Data cleaning and preprocessing\n",
    "  - Exploratory Data Analysis (EDA)\n",
    "  - Feature engineering and selection\n",
    "  - Correlation and clustering analysis\n",
    "- **Build and evaluate multiple classification models** with proper validation techniques\n",
    "- **Identify key employability factors** through feature importance analysis and model interpretation\n",
    "- **Apply ML best practices** including proper train/validation/test splits, pipeline construction, and hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Existing Solutions\n",
    "\n",
    "**Traditional Approach:**\n",
    "\n",
    "Historically, HR departments and educational institutions rely on manual screening processes with heuristic filters (e.g., GPA cutoffs, specific degree specializations, work experience thresholds). This traditional approach has several limitations:\n",
    "- Time-consuming and difficult to scale\n",
    "- Subjective and prone to human bias\n",
    "- Often inaccurate in predicting actual job placement success\n",
    "- Fails to capture complex interactions between multiple factors\n",
    "\n",
    "**Machine Learning Solutions:**\n",
    "\n",
    "Several ML-based approaches exist on platforms like Kaggle and GitHub for placement prediction:\n",
    "\n",
    "**Common Algorithms Used:**\n",
    "- **Baseline Models:** Logistic Regression, K-Nearest Neighbors (KNN)\n",
    "- **Tree-based Models:** Decision Trees, Random Forest, ExtraTrees\n",
    "- **Boosting Methods:** XGBoost, AdaBoost, Gradient Boosting\n",
    "- **Support Vector Machines:** SVC with various kernels\n",
    "\n",
    "**Key Findings from Literature:**\n",
    "- Tree-based ensemble methods (Random Forest, XGBoost) typically outperform simpler baselines\n",
    "- Non-linear models better capture feature interactions (e.g., combined effect of GPA and work experience)\n",
    "- Feature engineering significantly impacts model performance\n",
    "- Proper handling of class imbalance is crucial for accurate predictions\n",
    "\n",
    "**Typical Methodology:**\n",
    "1. Exploratory Data Analysis (distributions, correlations, class imbalance)\n",
    "2. Preprocessing pipelines (encoding categorical variables, scaling, imputation)\n",
    "3. Model comparison using multiple metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "4. Hyperparameter tuning using GridSearchCV or RandomizedSearchCV\n",
    "5. Feature importance analysis for interpretability\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Dataset Information\n",
    "\n",
    "**Dataset Name:** Job Placement Dataset\n",
    "\n",
    "**Original Source:** [Kaggle - Job Placement Dataset](https://www.kaggle.com/datasets/ahsan81/job-placement-dataset/data)\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- **Type:** Dense, structured tabular data\n",
    "- **Size:** Small-to-medium (215 instances, 13 features)\n",
    "- **Features:** Mix of numeric and categorical variables\n",
    "- **Target Variable:** Binary classification (Placed / Not Placed)\n",
    "- **Quality:** Clean with no missing values or duplicates\n",
    "\n",
    "**Dataset Access:**\n",
    "- **GitHub Repository:** `https://github.com/Angry-Jay/ML_TheHiredHand`\n",
    "- **Raw Data URL:** `https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/main/Job_Placement_Data.csv`\n",
    "\n",
    "**Features Overview:**\n",
    "- Student demographics (gender)\n",
    "- Academic performance (SSC %, HSC %, Degree %, MBA %)\n",
    "- Educational background (SSC board, HSC board, HSC specialization, Degree type, MBA specialization)\n",
    "- Work experience\n",
    "- Employment test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nLt-pHGeqVQk",
   "metadata": {
    "id": "nLt-pHGeqVQk"
   },
   "source": [
    "## 2. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "yuLL6t7xfjF9",
   "metadata": {
    "id": "yuLL6t7xfjF9"
   },
   "outputs": [],
   "source": [
    "# Setting up\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing & Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Model Selection & Tuning\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L4DKOMIjqkxl",
   "metadata": {
    "id": "L4DKOMIjqkxl"
   },
   "source": [
    "## 3. Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wYM1JqfJlgRO",
   "metadata": {
    "id": "wYM1JqfJlgRO"
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/refs/heads/main/aug_train.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_URL)\n",
    "    \n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    display(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from {DATA_URL}\")\n",
    "    print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dlhLvQwVqrEU",
   "metadata": {
    "id": "dlhLvQwVqrEU"
   },
   "source": [
    "## 4. Dataset Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rP1msGkOoMm8",
   "metadata": {
    "collapsed": true,
    "id": "rP1msGkOoMm8"
   },
   "source": [
    "### 4.1 Metadata Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j9zobh3uwem",
   "metadata": {},
   "source": [
    "In this section, we analyze the dataset's metadata to understand its structure, data types, quality, and characteristics. This initial exploration helps identify:\n",
    "\n",
    "- **Dataset dimensions** and scale\n",
    "- **Feature data types** (numerical vs. categorical)\n",
    "- **Data quality issues** (duplicates, missing values, irrelevant columns)\n",
    "- **Statistical properties** of numerical features\n",
    "- **Potential data leakage** concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9917HzRuxAi",
   "metadata": {
    "id": "e9917HzRuxAi"
   },
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cn00ubn5n6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    display(df[df.duplicated(keep=False)])\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mw8a92853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEATURE TYPE SEPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frr1y7kec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES - STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "display(df[numerical_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wnbtudgpdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CATEGORICAL FEATURES - UNIQUE VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {df[col].nunique()}\")\n",
    "    print(f\"  Values: {df[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ezwjvc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Leakage Assessment and Target Variable Identification\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE & DATA LEAKAGE ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify the target variable\n",
    "target_col = 'target'\n",
    "print(f\"\\nTarget variable: '{target_col}'\")\n",
    "print(f\"Classes: {df[target_col].unique().tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df[target_col].value_counts(normalize=True).round(3))\n",
    "\n",
    "# Verify feature composition\n",
    "print(f\"\\n--- Feature Inventory ---\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"  - Predictors: {len(df.columns) - 1}\")\n",
    "print(f\"  - Target: 1 ('{target_col}')\")\n",
    "\n",
    "# Check for post-placement features that could leak information\n",
    "print(f\"\\n--- Data Leakage Check ---\")\n",
    "suspicious_keywords = ['salary', 'offer', 'package', 'compensation', 'hired']\n",
    "leakage_found = False\n",
    "\n",
    "for keyword in suspicious_keywords:\n",
    "    if any(keyword in col.lower() for col in df.columns):\n",
    "        print(f\"WARNING: Potential leakage feature containing '{keyword}' detected\")\n",
    "        leakage_found = True\n",
    "\n",
    "if not leakage_found:\n",
    "    print(\"No obvious data leakage features detected.\")\n",
    "    print(\"All features represent information available at prediction time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqd2bq3x9tl",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8xe1dth30d9",
   "metadata": {},
   "source": [
    "The initial metadata analysis reveals a **substantially larger dataset** compared to typical placement studies, with **19,158 instances** across **14 features** (13 predictors and 1 target). The dataset exhibits **no duplicate records**, ensuring data integrity. However, **missing values are present** in several features, with the most significant gaps in `company_type` (6,140 missing, 32.1%), `company_size` (5,938 missing, 31.0%), `major_discipline` (2,813 missing, 14.7%), and `gender` (4,508 missing, 23.5%). This necessitates careful imputation strategies or missing value handling during preprocessing.\n",
    "\n",
    "The feature composition consists of **2 numerical predictors** (`city_development_index` and `training_hours`) and **10 categorical predictors** representing demographics, education, and employment history. Additionally, `enrollee_id` serves as a unique identifier and must be excluded from modeling, while `target` is the binary outcome variable. Categorical features exhibit **varying cardinality**: low cardinality for binary features like `relevent_experience` (2 values) and `gender` (3 values including missing), moderate cardinality for features like `education_level` (5 levels) and `major_discipline` (6 disciplines), and **high cardinality** for `city` (123 unique cities) and `experience` (22 levels), which may require specialized encoding techniques such as target encoding or frequency encoding.\n",
    "\n",
    "The target variable exhibits **significant class imbalance**, with **75.1% of candidates not looking for job change** (class 0) and only **24.9% actively seeking change** (class 1), yielding an imbalance ratio of **3.01:1**. This substantial imbalance must be addressed during model training through techniques such as class weighting, resampling (SMOTE/undersampling), or using evaluation metrics robust to imbalance (F1-score, ROC-AUC, precision-recall curves). **No data leakage concerns** were identified; all features represent information collected during training enrollment, ensuring model validity for predicting actual job change intentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6wkcli5oth",
   "metadata": {},
   "source": [
    "### 4.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe81107",
   "metadata": {},
   "source": [
    "Missing value analysis is critical for assessing **data quality** and **model performance**. Incomplete data can introduce bias, reduce statistical power, and lead to invalid inferences if not properly handled. Understanding the **missing data mechanism**—whether values are Missing Completely At Random (MCAR), Missing At Random (MAR), or Missing Not At Random (MNAR)—is essential for selecting appropriate imputation strategies. MNAR patterns, where missingness is systematically related to unobserved values, require special attention as they can reveal meaningful information about the underlying population (e.g., unemployed candidates not providing company details). This analysis will quantify missingness, identify patterns, and inform preprocessing decisions to ensure robust model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ns9tniisnzm",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Missing values count\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"\\nMissing values per feature:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Missing values percentage\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MISSING VALUES PERCENTAGE\")\n",
    "print(\"=\" * 60)\n",
    "missing_percentages = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "print(missing_percentages)\n",
    "\n",
    "# Summary statistics\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total missing values: {total_missing}\")\n",
    "print(f\"Total cells: {total_cells}\")\n",
    "print(f\"Overall missingness: {(total_missing / total_cells * 100):.2f}%\")\n",
    "print(f\"Features with missing values: {(missing_counts > 0).sum()} out of {len(df.columns)}\")\n",
    "print(f\"Complete features: {(missing_counts == 0).sum()} out of {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urrc4khdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    axes[0].bar(range(len(missing_data)), missing_data.values, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[0].set_xticks(range(len(missing_data)))\n",
    "    axes[0].set_xticklabels(missing_data.index, rotation=45, ha='right')\n",
    "    axes[0].set_title('Missing Values Count by Feature', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Features')\n",
    "    axes[0].set_ylabel('Number of Missing Values')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, v in enumerate(missing_data.values):\n",
    "        axes[0].text(i, v + 100, str(v), ha='center', va='bottom')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14)\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Percentage plot\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_pct) > 0:\n",
    "    axes[1].bar(range(len(missing_pct)), missing_pct.values, edgecolor='black', alpha=0.7, color='red')\n",
    "    axes[1].set_xticks(range(len(missing_pct)))\n",
    "    axes[1].set_xticklabels(missing_pct.index, rotation=45, ha='right')\n",
    "    axes[1].set_title('Missing Values Percentage by Feature', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Features')\n",
    "    axes[1].set_ylabel('Percentage (%)')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(missing_pct.values):\n",
    "        axes[1].text(i, v + 0.5, f'{v}%', ha='center', va='bottom')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whuo5kvskj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value pattern analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUE PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Features with missing values\n",
    "features_with_missing = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"\\nFeatures with missing values ({len(features_with_missing)}): {features_with_missing}\")\n",
    "    \n",
    "    # Check co-occurrence of missing values\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CO-OCCURRENCE OF MISSING VALUES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check common combinations\n",
    "    if 'company_size' in features_with_missing and 'company_type' in features_with_missing:\n",
    "        both_missing = df['company_size'].isnull() & df['company_type'].isnull()\n",
    "        print(f\"\\ncompany_size AND company_type both missing: {both_missing.sum()} ({both_missing.sum() / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    if 'company_size' in features_with_missing and 'company_type' in features_with_missing and 'experience' in features_with_missing:\n",
    "        all_three = df['company_size'].isnull() & df['company_type'].isnull() & df['experience'].isnull()\n",
    "        print(f\"company_size AND company_type AND experience all missing: {all_three.sum()} ({all_three.sum() / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Distribution of missing counts per row\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MISSING VALUES PER ROW DISTRIBUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    missing_per_row = df.isnull().sum(axis=1)\n",
    "    print(f\"\\nMissing values distribution:\")\n",
    "    print(missing_per_row.value_counts().sort_index())\n",
    "    \n",
    "    # Rows with any missing value\n",
    "    rows_with_missing = df.isnull().any(axis=1).sum()\n",
    "    print(f\"\\n--- Summary ---\")\n",
    "    print(f\"Total rows with at least one missing value: {rows_with_missing} ({rows_with_missing / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Rows with all values complete\n",
    "    complete_rows = (~df.isnull().any(axis=1)).sum()\n",
    "    print(f\"Complete rows (no missing values): {complete_rows} ({complete_rows / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Most common missing value patterns\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOP 5 MISSING VALUE PATTERNS\")\n",
    "    print(\"=\" * 60)\n",
    "    missing_patterns = df[features_with_missing].isnull().astype(int)\n",
    "    pattern_counts = missing_patterns.groupby(features_with_missing).size().sort_values(ascending=False).head(5)\n",
    "    \n",
    "    for idx, (pattern, count) in enumerate(pattern_counts.items(), 1):\n",
    "        missing_features = [feat for feat, is_missing in zip(features_with_missing, pattern) if is_missing == 1]\n",
    "        if missing_features:\n",
    "            print(f\"\\n{idx}. Missing: {missing_features}\")\n",
    "            print(f\"   Count: {count} ({count / len(df) * 100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n{idx}. No missing values\")\n",
    "            print(f\"   Count: {count} ({count / len(df) * 100:.2f}%)\")\n",
    "            \n",
    "else:\n",
    "    print(\"\\nNo missing values detected in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sk07yfg2sxf",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lg2fvu8k3nn",
   "metadata": {},
   "source": [
    "The missing values analysis reveals **significant data incompleteness** affecting **53.26% of all rows** (10,203 instances), while only **46.74%** (8,955 instances) are complete. Out of **14 features**, **8 contain missing values** with a **hierarchical pattern**: **company_type** (6,140 missing, 32.05%) and **company_size** (5,938 missing, 30.99%) dominate, followed by **gender** (4,508 missing, 23.53%) and **major_discipline** (2,813 missing, 14.68%). Lower-level missingness appears in **education_level** (460 missing, 2.40%), **last_new_job** (423 missing, 2.21%), **enrolled_university** (386 missing, 2.01%), and **experience** (65 missing, 0.34%). The overall dataset missingness is **7.73%** of total cells.\n",
    "\n",
    "**Pattern analysis** reveals **systematic co-occurrence** of missing values, confirming non-random (MNAR) behavior. The top missing value patterns are: **(1) Complete rows with no missing values: 46.74%**; **(2) Both company_size AND company_type missing: 14.50%** (2,777 rows) — strongly indicating unemployed candidates or students; **(3) Only gender missing: 11.61%** (2,224 rows); **(4) major_discipline, company_size, AND company_type missing: 4.42%** (847 rows); **(5) gender, company_size, AND company_type missing: 4.36%** (835 rows). Notably, **5,360 rows (27.98%)** have both employment features missing together, while only **20 rows (0.10%)** have all three employment-related features (company_size, company_type, experience) missing simultaneously. The missing values per row distribution shows most affected rows have **1-3 missing features**, with decreasing frequency for higher counts (628 rows with 4 missing, 176 with 5, 62 with 6, and only 12 with 7).\n",
    "\n",
    "**Preprocessing strategy:** For **company_size and company_type**, we will **create an explicit \"Not Employed\" category** rather than impute values, because the 27.98% co-occurrence pattern clearly represents candidates without current employment (students/freshers) where these fields are genuinely not applicable — imputation would introduce false information and obscure this meaningful employment status indicator. For **low-missingness features** (education_level, enrolled_university, last_new_job, experience all <3%), we will apply **mode imputation** since their sporadic missingness suggests random data collection gaps rather than systematic patterns, and their low prevalence minimizes impact on model validity. For **gender and major_discipline**, we will **create \"Unknown\" categories** because their substantial independent missingness (11.61% for gender alone, 14.68% for major_discipline) indicates data provision reluctance or privacy concerns rather than inapplicability, and preserving this \"not provided\" signal may itself be predictive of job change behavior. Additionally, we will **engineer binary missingness indicators** (`has_employment_info`, `gender_provided`, `education_complete`) as the 53.26% of incomplete rows may exhibit distinct job-seeking behaviors, and these indicators could capture valuable patterns for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5ds0jss7j",
   "metadata": {},
   "source": [
    "### 4.3 Feature Distributions, Scaling & Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cl39rkn1jwj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].hist(df[col], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ja2b8vs26t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using boxplots\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].boxplot(df[col], vert=True)\n",
    "    axes[idx].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcyttqlj3m8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative outlier detection using IQR method\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER DETECTION (IQR METHOD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"  Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    print(f\"  Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kxjsohtf7di",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features distribution\n",
    "categorical_features = [col for col in categorical_cols if col != 'status']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    value_counts = df[col].value_counts()\n",
    "    axes[idx].bar(value_counts.index, value_counts.values, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v + 1, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdh7cwrgcmn",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cctau1voa9",
   "metadata": {},
   "source": [
    "**Numerical Feature Distributions:** The histogram analysis reveals that most numerical features exhibit approximately **normal distributions** with slight variations. Academic performance metrics (SSC, HSC, degree, and MBA percentages) are centered around their respective means (62-72%), with the majority of students scoring between 50% and 85%. The `emp_test_percentage` shows a more **uniform distribution** across its range, suggesting diverse performance levels on employment assessments. All features are naturally bounded within the percentage scale, maintaining consistency in measurement units.\n",
    "\n",
    "**Outlier Analysis:** The IQR-based outlier detection identified **minimal outliers** across the dataset. Only **8 outliers (3.7%)** were detected in `hsc_percentage` and **1 outlier (0.5%)** in `degree_percentage`, while other features showed **no outliers**. The boxplots confirm this finding, with `hsc_percentage` displaying several lower-bound outliers (students with unusually low HSC scores around 37-42%). These outliers represent legitimate data points rather than errors and may provide valuable information about placement outcomes for lower-performing students. Given their small proportion, **no removal is recommended** at this stage.\n",
    "\n",
    "**Categorical Feature Distributions:** The categorical features exhibit notable **class imbalances**. Gender distribution shows **139 males (64.7%)** versus **76 females (35.3%)**. Academic backgrounds reveal strong preferences: **Commerce dominates HSC subjects** (113 students), **Sci&Tech leads undergraduate degrees** (145 students), and most students lack **work experience (141 vs. 74)**. Board affiliations are relatively balanced between Central and Others. The **Mkt&Fin specialization** slightly outnumbers Mkt&HR (120 vs. 95). These imbalances should be considered during feature encoding and model interpretation, as minority classes may have reduced predictive power due to limited representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpec963ybwf",
   "metadata": {},
   "source": [
    "### 4.4 Target Feature Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zpteycwzv7o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Count plot\n",
    "target_counts = df['target'].value_counts()\n",
    "axes[0].bar(target_counts.index, target_counts.values, edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "axes[0].set_title('Target Distribution (Count)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Target (0=Not Looking, 1=Looking for Job Change)')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts.values, labels=['Not Looking (0)', 'Looking (1)'], autopct='%1.1f%%', \n",
    "            startangle=90, colors=['red', 'green'], explode=(0.05, 0))\n",
    "axes[1].set_title('Target Distribution (Proportion)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(target_counts)\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df['target'].value_counts(normalize=True).round(3))\n",
    "print(f\"\\nClass imbalance ratio: {target_counts.max() / target_counts.min():.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zoyq2hjy27i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features comparison by target class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Exclude enrollee_id from comparison (it's just an identifier)\n",
    "numerical_features_for_comparison = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "for idx, col in enumerate(numerical_features_for_comparison):\n",
    "    looking = df[df['target'] == 1.0][col]\n",
    "    not_looking = df[df['target'] == 0.0][col]\n",
    "    \n",
    "    axes[idx].hist([not_looking, looking], bins=15, label=['Not Looking (0)', 'Looking (1)'], \n",
    "                   edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "    axes[idx].set_title(f'{col} by Target', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ixvk1alzj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of numerical features by target class\n",
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES - MEAN COMPARISON BY TARGET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_features_for_comparison = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "comparison = df.groupby('target')[numerical_features_for_comparison].mean()\n",
    "comparison.index = ['Not Looking (0)', 'Looking (1)']\n",
    "print(\"\\nMean values by target class:\")\n",
    "print(comparison.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIFFERENCE (Looking - Not Looking)\")\n",
    "print(\"=\" * 60)\n",
    "difference = comparison.loc['Looking (1)'] - comparison.loc['Not Looking (0)']\n",
    "print(difference.round(2))\n",
    "\n",
    "# Visualize mean comparison\n",
    "comparison.T.plot(kind='bar', figsize=(10, 5), edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "plt.title('Mean Comparison of Numerical Features by Target', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Target')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w229mmkye5l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs target - job change rates\n",
    "categorical_features_for_analysis = [col for col in categorical_cols]\n",
    "\n",
    "# Limit to top categories for high-cardinality features like 'city'\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features_for_analysis[:10]):\n",
    "    if col == 'city':\n",
    "        # For city, show only top 10 cities\n",
    "        top_cities = df[col].value_counts().head(10).index\n",
    "        df_subset = df[df[col].isin(top_cities)]\n",
    "        ct = pd.crosstab(df_subset[col], df_subset['target'], normalize='index') * 100\n",
    "    else:\n",
    "        ct = pd.crosstab(df[col], df['target'], normalize='index') * 100\n",
    "    \n",
    "    ct.plot(kind='bar', ax=axes[idx], edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "    axes[idx].set_title(f'Job Change Rate by {col}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Percentage (%)')\n",
    "    axes[idx].legend(['Not Looking (0)', 'Looking (1)'], fontsize=7)\n",
    "    axes[idx].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(categorical_features_for_analysis[:10]), 12):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0rabbgh7t4ip",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mywig6bawtj",
   "metadata": {},
   "source": [
    "The target variable analysis will be completed after running the visualization cells above. This section examines the relationship between predictor features and placement outcomes, revealing which characteristics are most strongly associated with successful job placement. Key areas of investigation include class imbalance quantification, numerical feature differences between placed and not-placed students, and placement rate variations across categorical features. These insights will inform feature selection and model training strategies in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ido0ujpu1m",
   "metadata": {},
   "source": [
    "### 4.5 Feature Correlation & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cq5lfcgssn",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "n1uutp5z6qr",
   "metadata": {},
   "source": [
    "### 4.6 Unsupervised Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8emgkzabdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1xqgtjif38",
   "metadata": {},
   "source": [
    "### 4.7 Interpretations & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqc8z5vqw4l",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ML Baseline & Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gt7bbs25u3k",
   "metadata": {},
   "source": [
    "### 5.1 Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1y89f4ne4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "w0em90lymgs",
   "metadata": {},
   "source": [
    "### 5.2 Pipelines & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n3uayft3v9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7p38rbqmjzj",
   "metadata": {},
   "source": [
    "### 5.3 Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8y4eycfir0j",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rcsmc5lyfa",
   "metadata": {},
   "source": [
    "### 5.4 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l7y521g4zp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "t4wn484c6q",
   "metadata": {},
   "source": [
    "### 5.5 Results Interpretation & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cvggwxipyvh",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Enhanced Models & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kg16rtxrd5",
   "metadata": {},
   "source": [
    "### 6.1 Justification of Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m27bj9o9qf",
   "metadata": {},
   "source": [
    "### 6.2 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tl2fadk15o",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "qxdtuqjiu4b",
   "metadata": {},
   "source": [
    "### 6.3 Final Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9kggnmk5w1v",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ur24e8fjq9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f01f20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
