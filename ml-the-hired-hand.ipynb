{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yZg8yXCtjE3J",
   "metadata": {
    "id": "yZg8yXCtjE3J"
   },
   "source": [
    "# The Hired Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h_wT-bE8f0JQ",
   "metadata": {
    "id": "h_wT-bE8f0JQ"
   },
   "source": [
    "**Machine Learning for Job Placement Prediction**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Angry-Jay/ML_TheHiredHand/blob/main/ml-the-hired-hand.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Project & Dataset Description](#1-project--dataset-description)\n",
    "   - [1.1 Project Aim](#11-project-aim)\n",
    "   - [1.2 Existing Solutions](#12-existing-solutions)\n",
    "   - [1.3 Dataset Information](#13-dataset-information)\n",
    "2. [Library Imports](#2-library-imports)\n",
    "3. [Data Access](#3-data-access)\n",
    "4. [Dataset Exploratory Analysis](#4-dataset-exploratory-analysis)\n",
    "   - [4.1 Metadata Analysis](#41-metadata-analysis)\n",
    "   - [4.2 Missing Values Analysis](#42-missing-values-analysis)\n",
    "   - [4.3 Feature Distributions, Scaling & Outliers](#43-feature-distributions-scaling--outliers)\n",
    "   - [4.4 Target Feature Study](#44-target-feature-study)\n",
    "   - [4.5 Feature Correlation & Selection](#45-feature-correlation--selection)\n",
    "   - [4.6 Unsupervised Clustering](#46-unsupervised-clustering)\n",
    "   - [4.7 Interpretations & Conclusions](#47-interpretations--conclusions)\n",
    "5. [ML Baseline & Ensemble Models](#5-ml-baseline--ensemble-models)\n",
    "   - [5.1 Train/Validation/Test Splits](#51-trainvalidationtest-splits)\n",
    "   - [5.2 Pipelines & Models](#52-pipelines--models)\n",
    "   - [5.3 Training & Validation](#53-training--validation)\n",
    "   - [5.4 Testing](#54-testing)\n",
    "   - [5.5 Results Interpretation & Discussion](#55-results-interpretation--discussion)\n",
    "6. [Enhanced Models & Hyperparameter Tuning](#6-enhanced-models--hyperparameter-tuning)\n",
    "   - [6.1 Justification of Choices](#61-justification-of-choices)\n",
    "   - [6.2 Hyperparameter Optimization](#62-hyperparameter-optimization)\n",
    "   - [6.3 Final Results & Analysis](#63-final-results--analysis)\n",
    "7. [Conclusion & Future Work](#7-conclusion--future-work)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project & Dataset Description\n",
    "\n",
    "### 1.1 Project Aim\n",
    "\n",
    "This project applies Machine Learning techniques to predict employment outcomes for graduating students using the **Job Placement Dataset**. \n",
    "\n",
    "**Primary Objectives:**\n",
    "- **Predict employment outcomes** (Placed vs. Not Placed) based on demographic, academic, and professional attributes\n",
    "- **Demonstrate a coherent ML methodology** from data discovery through model optimization\n",
    "- **Apply comprehensive data analysis** including:\n",
    "  - Data cleaning and preprocessing\n",
    "  - Exploratory Data Analysis (EDA)\n",
    "  - Feature engineering and selection\n",
    "  - Correlation and clustering analysis\n",
    "- **Build and evaluate multiple classification models** with proper validation techniques\n",
    "- **Identify key employability factors** through feature importance analysis and model interpretation\n",
    "- **Apply ML best practices** including proper train/validation/test splits, pipeline construction, and hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Existing Solutions\n",
    "\n",
    "**Traditional Approach:**\n",
    "\n",
    "Historically, HR departments and educational institutions rely on manual screening processes with heuristic filters (e.g., GPA cutoffs, specific degree specializations, work experience thresholds). This traditional approach has several limitations:\n",
    "- Time-consuming and difficult to scale\n",
    "- Subjective and prone to human bias\n",
    "- Often inaccurate in predicting actual job placement success\n",
    "- Fails to capture complex interactions between multiple factors\n",
    "\n",
    "**Machine Learning Solutions:**\n",
    "\n",
    "Several ML-based approaches exist on platforms like Kaggle and GitHub for placement prediction:\n",
    "\n",
    "**Common Algorithms Used:**\n",
    "- **Baseline Models:** Logistic Regression, K-Nearest Neighbors (KNN)\n",
    "- **Tree-based Models:** Decision Trees, Random Forest, ExtraTrees\n",
    "- **Boosting Methods:** XGBoost, AdaBoost, Gradient Boosting\n",
    "- **Support Vector Machines:** SVC with various kernels\n",
    "\n",
    "**Key Findings from Literature:**\n",
    "- Tree-based ensemble methods (Random Forest, XGBoost) typically outperform simpler baselines\n",
    "- Non-linear models better capture feature interactions (e.g., combined effect of GPA and work experience)\n",
    "- Feature engineering significantly impacts model performance\n",
    "- Proper handling of class imbalance is crucial for accurate predictions\n",
    "\n",
    "**Typical Methodology:**\n",
    "1. Exploratory Data Analysis (distributions, correlations, class imbalance)\n",
    "2. Preprocessing pipelines (encoding categorical variables, scaling, imputation)\n",
    "3. Model comparison using multiple metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "4. Hyperparameter tuning using GridSearchCV or RandomizedSearchCV\n",
    "5. Feature importance analysis for interpretability\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Dataset Information\n",
    "\n",
    "**Dataset Name:** Job Placement Dataset\n",
    "\n",
    "**Original Source:** [Kaggle - Job Placement Dataset](https://www.kaggle.com/datasets/ahsan81/job-placement-dataset/data)\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- **Type:** Dense, structured tabular data\n",
    "- **Size:** Small-to-medium (215 instances, 13 features)\n",
    "- **Features:** Mix of numeric and categorical variables\n",
    "- **Target Variable:** Binary classification (Placed / Not Placed)\n",
    "- **Quality:** Clean with no missing values or duplicates\n",
    "\n",
    "**Dataset Access:**\n",
    "- **GitHub Repository:** `https://github.com/Angry-Jay/ML_TheHiredHand`\n",
    "- **Raw Data URL:** `https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/main/Job_Placement_Data.csv`\n",
    "\n",
    "**Features Overview:**\n",
    "- Student demographics (gender)\n",
    "- Academic performance (SSC %, HSC %, Degree %, MBA %)\n",
    "- Educational background (SSC board, HSC board, HSC specialization, Degree type, MBA specialization)\n",
    "- Work experience\n",
    "- Employment test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nLt-pHGeqVQk",
   "metadata": {
    "id": "nLt-pHGeqVQk"
   },
   "source": [
    "## 2. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "yuLL6t7xfjF9",
   "metadata": {
    "id": "yuLL6t7xfjF9"
   },
   "outputs": [],
   "source": [
    "# Setting up\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing & Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Model Selection & Tuning\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L4DKOMIjqkxl",
   "metadata": {
    "id": "L4DKOMIjqkxl"
   },
   "source": [
    "## 3. Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wYM1JqfJlgRO",
   "metadata": {
    "id": "wYM1JqfJlgRO"
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/refs/heads/main/Job_Placement_Data.csv\"\n",
    "\n",
    "try:\n",
    "  # Load the dataset directly into a Pandas DataFrame\n",
    "  df = pd.read_csv(DATA_URL)\n",
    "\n",
    "  print(\" Dataset loaded successfully!\")\n",
    "  print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "  # Display the first 5 rows to verify\n",
    "  display(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"Error loading data. Check your URL.\")\n",
    "  print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dlhLvQwVqrEU",
   "metadata": {
    "id": "dlhLvQwVqrEU"
   },
   "source": [
    "## 4. Dataset Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rP1msGkOoMm8",
   "metadata": {
    "collapsed": true,
    "id": "rP1msGkOoMm8"
   },
   "source": [
    "### 4.1 Metadata Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j9zobh3uwem",
   "metadata": {},
   "source": [
    "In this section, we analyze the dataset's metadata to understand its structure, data types, quality, and characteristics. This initial exploration helps identify:\n",
    "\n",
    "- **Dataset dimensions** and scale\n",
    "- **Feature data types** (numerical vs. categorical)\n",
    "- **Data quality issues** (duplicates, missing values, irrelevant columns)\n",
    "- **Statistical properties** of numerical features\n",
    "- **Potential data leakage** concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9917HzRuxAi",
   "metadata": {
    "id": "e9917HzRuxAi"
   },
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cn00ubn5n6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    display(df[df.duplicated(keep=False)])\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mw8a92853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEATURE TYPE SEPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frr1y7kec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES - STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "display(df[numerical_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wnbtudgpdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CATEGORICAL FEATURES - UNIQUE VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {df[col].nunique()}\")\n",
    "    print(f\"  Values: {df[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b3ezwjvc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Leakage Assessment and Target Variable Identification\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE & DATA LEAKAGE ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify the target variable\n",
    "target_col = 'status'\n",
    "print(f\"\\nTarget variable: '{target_col}'\")\n",
    "print(f\"Classes: {df[target_col].unique().tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df[target_col].value_counts(normalize=True).round(3))\n",
    "\n",
    "# Verify feature composition\n",
    "print(f\"\\n--- Feature Inventory ---\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"  - Predictors: {len(df.columns) - 1}\")\n",
    "print(f\"  - Target: 1 ('{target_col}')\")\n",
    "\n",
    "# Check for post-placement features that could leak information\n",
    "print(f\"\\n--- Data Leakage Check ---\")\n",
    "suspicious_keywords = ['salary', 'offer', 'package', 'compensation', 'hired']\n",
    "leakage_found = False\n",
    "\n",
    "for keyword in suspicious_keywords:\n",
    "    if any(keyword in col.lower() for col in df.columns):\n",
    "        print(f\"WARNING: Potential leakage feature containing '{keyword}' detected\")\n",
    "        leakage_found = True\n",
    "\n",
    "if not leakage_found:\n",
    "    print(\"No obvious data leakage features detected.\")\n",
    "    print(\"All features represent information available at prediction time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqd2bq3x9tl",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8xe1dth30d9",
   "metadata": {},
   "source": [
    "The initial metadata analysis reveals a **clean, well-structured dataset** suitable for classification modeling. With **215 instances** across **13 features** (12 predictors and 1 target), the dataset contains **no missing values or duplicate records**, eliminating the need for imputation strategies at this stage.\n",
    "\n",
    "The feature composition consists of **5 numerical variables** (all academic performance percentages) and **8 categorical variables** (including demographics, educational background, and the target). All categorical features exhibit low cardinality (2-3 unique values), which simplifies encoding requirements for subsequent modeling phases. Numerical features demonstrate similar scaling (percentage ranges from ~40-98%), with means centered around 62-72% and standard deviations ranging from 5-13%, indicating relatively consistent distributions across academic assessment levels.\n",
    "\n",
    "The target variable exhibits **moderate class imbalance**, with *68.8% of students placed* and *31.2% not placed*. This imbalance must be considered during model training and evaluation to prevent bias toward the majority class. **No data leakage concerns** were identified; all features represent information available at prediction time, ensuring the model's validity for real-world deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6wkcli5oth",
   "metadata": {},
   "source": [
    "### 4.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ns9tniisnzm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify missing values\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nMissing values per feature:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5ds0jss7j",
   "metadata": {},
   "source": [
    "### 4.3 Feature Distributions, Scaling & Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cl39rkn1jwj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].hist(df[col], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ja2b8vs26t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using boxplots\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].boxplot(df[col], vert=True)\n",
    "    axes[idx].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcyttqlj3m8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative outlier detection using IQR method\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER DETECTION (IQR METHOD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"  Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    print(f\"  Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "kxjsohtf7di",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features distribution\n",
    "categorical_features = [col for col in categorical_cols if col != 'status']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    value_counts = df[col].value_counts()\n",
    "    axes[idx].bar(value_counts.index, value_counts.values, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v + 1, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdh7cwrgcmn",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cctau1voa9",
   "metadata": {},
   "source": [
    "**Numerical Feature Distributions:** The histogram analysis reveals that most numerical features exhibit approximately **normal distributions** with slight variations. Academic performance metrics (SSC, HSC, degree, and MBA percentages) are centered around their respective means (62-72%), with the majority of students scoring between 50% and 85%. The `emp_test_percentage` shows a more **uniform distribution** across its range, suggesting diverse performance levels on employment assessments. All features are naturally bounded within the percentage scale, maintaining consistency in measurement units.\n",
    "\n",
    "**Outlier Analysis:** The IQR-based outlier detection identified **minimal outliers** across the dataset. Only **8 outliers (3.7%)** were detected in `hsc_percentage` and **1 outlier (0.5%)** in `degree_percentage`, while other features showed **no outliers**. The boxplots confirm this finding, with `hsc_percentage` displaying several lower-bound outliers (students with unusually low HSC scores around 37-42%). These outliers represent legitimate data points rather than errors and may provide valuable information about placement outcomes for lower-performing students. Given their small proportion, **no removal is recommended** at this stage.\n",
    "\n",
    "**Categorical Feature Distributions:** The categorical features exhibit notable **class imbalances**. Gender distribution shows **139 males (64.7%)** versus **76 females (35.3%)**. Academic backgrounds reveal strong preferences: **Commerce dominates HSC subjects** (113 students), **Sci&Tech leads undergraduate degrees** (145 students), and most students lack **work experience (141 vs. 74)**. Board affiliations are relatively balanced between Central and Others. The **Mkt&Fin specialization** slightly outnumbers Mkt&HR (120 vs. 95). These imbalances should be considered during feature encoding and model interpretation, as minority classes may have reduced predictive power due to limited representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpec963ybwf",
   "metadata": {},
   "source": [
    "### 4.4 Target Feature Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "zpteycwzv7o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Count plot\n",
    "status_counts = df['status'].value_counts()\n",
    "axes[0].bar(status_counts.index, status_counts.values, edgecolor='black', alpha=0.7, color=['green', 'red'])\n",
    "axes[0].set_title('Target Distribution (Count)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Status')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(status_counts.values):\n",
    "    axes[0].text(i, v + 2, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', \n",
    "            startangle=90, colors=['green', 'red'], explode=(0.05, 0))\n",
    "axes[1].set_title('Target Distribution (Proportion)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(status_counts)\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df['status'].value_counts(normalize=True).round(3))\n",
    "print(f\"\\nClass imbalance ratio: {status_counts.max() / status_counts.min():.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "zoyq2hjy27i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features comparison by target class\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    placed = df[df['status'] == 'Placed'][col]\n",
    "    not_placed = df[df['status'] == 'Not Placed'][col]\n",
    "    \n",
    "    axes[idx].hist([placed, not_placed], bins=15, label=['Placed', 'Not Placed'], \n",
    "                   edgecolor='black', alpha=0.7, color=['green', 'red'])\n",
    "    axes[idx].set_title(f'{col} by Status', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87ixvk1alzj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of numerical features by target class\n",
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES - MEAN COMPARISON BY STATUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = df.groupby('status')[numerical_cols].mean()\n",
    "print(\"\\nMean values by placement status:\")\n",
    "print(comparison.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIFFERENCE (Placed - Not Placed)\")\n",
    "print(\"=\" * 60)\n",
    "difference = comparison.loc['Placed'] - comparison.loc['Not Placed']\n",
    "print(difference.round(2))\n",
    "\n",
    "# Visualize mean comparison\n",
    "comparison.T.plot(kind='bar', figsize=(12, 5), edgecolor='black', alpha=0.7, color=['green', 'red'])\n",
    "plt.title('Mean Comparison of Numerical Features by Status', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Value (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Status')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "w229mmkye5l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs target - placement rates\n",
    "categorical_features = [col for col in categorical_cols if col != 'status']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    # Create crosstab for placement rates\n",
    "    ct = pd.crosstab(df[col], df['status'], normalize='index') * 100\n",
    "    ct.plot(kind='bar', ax=axes[idx], edgecolor='black', alpha=0.7, color=['green', 'red'])\n",
    "    axes[idx].set_title(f'Placement Rate by {col}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Percentage (%)')\n",
    "    axes[idx].legend(title='Status', fontsize=8)\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0rabbgh7t4ip",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mywig6bawtj",
   "metadata": {},
   "source": [
    "The target variable analysis will be completed after running the visualization cells above. This section examines the relationship between predictor features and placement outcomes, revealing which characteristics are most strongly associated with successful job placement. Key areas of investigation include class imbalance quantification, numerical feature differences between placed and not-placed students, and placement rate variations across categorical features. These insights will inform feature selection and model training strategies in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ido0ujpu1m",
   "metadata": {},
   "source": [
    "### 4.5 Feature Correlation & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cq5lfcgssn",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "n1uutp5z6qr",
   "metadata": {},
   "source": [
    "### 4.6 Unsupervised Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8emgkzabdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1xqgtjif38",
   "metadata": {},
   "source": [
    "### 4.7 Interpretations & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqc8z5vqw4l",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ML Baseline & Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gt7bbs25u3k",
   "metadata": {},
   "source": [
    "### 5.1 Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1y89f4ne4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "w0em90lymgs",
   "metadata": {},
   "source": [
    "### 5.2 Pipelines & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n3uayft3v9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7p38rbqmjzj",
   "metadata": {},
   "source": [
    "### 5.3 Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8y4eycfir0j",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rcsmc5lyfa",
   "metadata": {},
   "source": [
    "### 5.4 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l7y521g4zp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "t4wn484c6q",
   "metadata": {},
   "source": [
    "### 5.5 Results Interpretation & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cvggwxipyvh",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Enhanced Models & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kg16rtxrd5",
   "metadata": {},
   "source": [
    "### 6.1 Justification of Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m27bj9o9qf",
   "metadata": {},
   "source": [
    "### 6.2 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tl2fadk15o",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "qxdtuqjiu4b",
   "metadata": {},
   "source": [
    "### 6.3 Final Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9kggnmk5w1v",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ur24e8fjq9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f01f20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
