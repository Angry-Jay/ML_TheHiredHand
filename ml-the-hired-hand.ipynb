{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yZg8yXCtjE3J",
   "metadata": {
    "id": "yZg8yXCtjE3J"
   },
   "source": [
    "# The Hired Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h_wT-bE8f0JQ",
   "metadata": {
    "id": "h_wT-bE8f0JQ"
   },
   "source": [
    "**Machine Learning for Job Placement Prediction**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Angry-Jay/ML_TheHiredHand/blob/main/ml-the-hired-hand.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Project & Dataset Description](#1-project--dataset-description)\n",
    "   - [1.1 Project Aim](#11-project-aim)\n",
    "   - [1.2 Existing Solutions](#12-existing-solutions)\n",
    "   - [1.3 Dataset Information](#13-dataset-information)\n",
    "2. [Library Imports](#2-library-imports)\n",
    "3. [Data Access](#3-data-access)\n",
    "4. [Dataset Exploratory Analysis](#4-dataset-exploratory-analysis)\n",
    "   - [4.1 Metadata Analysis](#41-metadata-analysis)\n",
    "   - [4.2 Missing Values Analysis](#42-missing-values-analysis)\n",
    "   - [4.3 Feature Distributions, Scaling & Outliers](#43-feature-distributions-scaling--outliers)\n",
    "   - [4.4 Target Feature Study](#44-target-feature-study)\n",
    "   - [4.5 Feature Correlation & Selection](#45-feature-correlation--selection)\n",
    "   - [4.6 Unsupervised Clustering](#46-unsupervised-clustering)\n",
    "   - [4.7 Interpretations & Conclusions](#47-interpretations--conclusions)\n",
    "5. [ML Baseline & Ensemble Models](#5-ml-baseline--ensemble-models)\n",
    "   - [5.1 Train/Validation/Test Splits](#51-trainvalidationtest-splits)\n",
    "   - [5.2 Pipelines & Models](#52-pipelines--models)\n",
    "   - [5.3 Training & Validation](#53-training--validation)\n",
    "   - [5.4 Testing](#54-testing)\n",
    "   - [5.5 Results Interpretation & Discussion](#55-results-interpretation--discussion)\n",
    "6. [Enhanced Models & Hyperparameter Tuning](#6-enhanced-models--hyperparameter-tuning)\n",
    "   - [6.1 Justification of Choices](#61-justification-of-choices)\n",
    "   - [6.2 Hyperparameter Optimization](#62-hyperparameter-optimization)\n",
    "   - [6.3 Final Results & Analysis](#63-final-results--analysis)\n",
    "7. [Conclusion & Future Work](#7-conclusion--future-work)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project & Dataset Description\n",
    "\n",
    "### 1.1 Project Aim\n",
    "\n",
    "This project applies Machine Learning techniques to predict employment outcomes for graduating students using the **Job Placement Dataset**. \n",
    "\n",
    "**Primary Objectives:**\n",
    "- **Predict employment outcomes** (Placed vs. Not Placed) based on demographic, academic, and professional attributes\n",
    "- **Demonstrate a coherent ML methodology** from data discovery through model optimization\n",
    "- **Apply comprehensive data analysis** including:\n",
    "  - Data cleaning and preprocessing\n",
    "  - Exploratory Data Analysis (EDA)\n",
    "  - Feature engineering and selection\n",
    "  - Correlation and clustering analysis\n",
    "- **Build and evaluate multiple classification models** with proper validation techniques\n",
    "- **Identify key employability factors** through feature importance analysis and model interpretation\n",
    "- **Apply ML best practices** including proper train/validation/test splits, pipeline construction, and hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Existing Solutions\n",
    "\n",
    "**Traditional Approach:**\n",
    "\n",
    "Historically, HR departments and educational institutions rely on manual screening processes with heuristic filters (e.g., GPA cutoffs, specific degree specializations, work experience thresholds). This traditional approach has several limitations:\n",
    "- Time-consuming and difficult to scale\n",
    "- Subjective and prone to human bias\n",
    "- Often inaccurate in predicting actual job placement success\n",
    "- Fails to capture complex interactions between multiple factors\n",
    "\n",
    "**Machine Learning Solutions:**\n",
    "\n",
    "Several ML-based approaches exist on platforms like Kaggle and GitHub for placement prediction:\n",
    "\n",
    "**Common Algorithms Used:**\n",
    "- **Baseline Models:** Logistic Regression, K-Nearest Neighbors (KNN)\n",
    "- **Tree-based Models:** Decision Trees, Random Forest, ExtraTrees\n",
    "- **Boosting Methods:** XGBoost, AdaBoost, Gradient Boosting\n",
    "- **Support Vector Machines:** SVC with various kernels\n",
    "\n",
    "**Key Findings from Literature:**\n",
    "- Tree-based ensemble methods (Random Forest, XGBoost) typically outperform simpler baselines\n",
    "- Non-linear models better capture feature interactions (e.g., combined effect of GPA and work experience)\n",
    "- Feature engineering significantly impacts model performance\n",
    "- Proper handling of class imbalance is crucial for accurate predictions\n",
    "\n",
    "**Typical Methodology:**\n",
    "1. Exploratory Data Analysis (distributions, correlations, class imbalance)\n",
    "2. Preprocessing pipelines (encoding categorical variables, scaling, imputation)\n",
    "3. Model comparison using multiple metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "4. Hyperparameter tuning using GridSearchCV or RandomizedSearchCV\n",
    "5. Feature importance analysis for interpretability\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Dataset Information\n",
    "\n",
    "**Dataset Name:** Job Placement Dataset\n",
    "\n",
    "**Original Source:** [Kaggle - Job Placement Dataset](https://www.kaggle.com/datasets/ahsan81/job-placement-dataset/data)\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- **Type:** Dense, structured tabular data\n",
    "- **Size:** Small-to-medium (215 instances, 13 features)\n",
    "- **Features:** Mix of numeric and categorical variables\n",
    "- **Target Variable:** Binary classification (Placed / Not Placed)\n",
    "- **Quality:** Clean with no missing values or duplicates\n",
    "\n",
    "**Dataset Access:**\n",
    "- **GitHub Repository:** `https://github.com/Angry-Jay/ML_TheHiredHand`\n",
    "- **Raw Data URL:** `https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/main/Job_Placement_Data.csv`\n",
    "\n",
    "**Features Overview:**\n",
    "- Student demographics (gender)\n",
    "- Academic performance (SSC %, HSC %, Degree %, MBA %)\n",
    "- Educational background (SSC board, HSC board, HSC specialization, Degree type, MBA specialization)\n",
    "- Work experience\n",
    "- Employment test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nLt-pHGeqVQk",
   "metadata": {
    "id": "nLt-pHGeqVQk"
   },
   "source": [
    "## 2. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yuLL6t7xfjF9",
   "metadata": {
    "id": "yuLL6t7xfjF9"
   },
   "outputs": [],
   "source": [
    "# Setting up\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing & Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Model Selection & Tuning\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L4DKOMIjqkxl",
   "metadata": {
    "id": "L4DKOMIjqkxl"
   },
   "source": [
    "## 3. Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wYM1JqfJlgRO",
   "metadata": {
    "id": "wYM1JqfJlgRO"
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/Angry-Jay/ML_TheHiredHand/refs/heads/main/aug_train.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_URL)\n",
    "    \n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    display(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from {DATA_URL}\")\n",
    "    print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dlhLvQwVqrEU",
   "metadata": {
    "id": "dlhLvQwVqrEU"
   },
   "source": [
    "## 4. Dataset Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rP1msGkOoMm8",
   "metadata": {
    "collapsed": true,
    "id": "rP1msGkOoMm8"
   },
   "source": [
    "### 4.1 Metadata Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j9zobh3uwem",
   "metadata": {},
   "source": [
    "In this section, we analyze the dataset's metadata to understand its structure, data types, quality, and characteristics. This initial exploration helps identify:\n",
    "\n",
    "- **Dataset dimensions** and scale\n",
    "- **Feature data types** (numerical vs. categorical)\n",
    "- **Data quality issues** (duplicates, missing values, irrelevant columns)\n",
    "- **Statistical properties** of numerical features\n",
    "- **Potential data leakage** concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9917HzRuxAi",
   "metadata": {
    "id": "e9917HzRuxAi"
   },
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cn00ubn5n6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    display(df[df.duplicated(keep=False)])\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mw8a92853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEATURE TYPE SEPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frr1y7kec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES - STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "display(df[numerical_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wnbtudgpdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CATEGORICAL FEATURES - UNIQUE VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {df[col].nunique()}\")\n",
    "    print(f\"  Values: {df[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ezwjvc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Leakage Assessment and Target Variable Identification\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE & DATA LEAKAGE ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify the target variable\n",
    "target_col = 'target'\n",
    "print(f\"\\nTarget variable: '{target_col}'\")\n",
    "print(f\"Classes: {df[target_col].unique().tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df[target_col].value_counts(normalize=True).round(3))\n",
    "\n",
    "# Verify feature composition\n",
    "print(f\"\\n--- Feature Inventory ---\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"  - Predictors: {len(df.columns) - 1}\")\n",
    "print(f\"  - Target: 1 ('{target_col}')\")\n",
    "\n",
    "# Check for post-placement features that could leak information\n",
    "print(f\"\\n--- Data Leakage Check ---\")\n",
    "suspicious_keywords = ['salary', 'offer', 'package', 'compensation', 'hired']\n",
    "leakage_found = False\n",
    "\n",
    "for keyword in suspicious_keywords:\n",
    "    if any(keyword in col.lower() for col in df.columns):\n",
    "        print(f\"WARNING: Potential leakage feature containing '{keyword}' detected\")\n",
    "        leakage_found = True\n",
    "\n",
    "if not leakage_found:\n",
    "    print(\"No obvious data leakage features detected.\")\n",
    "    print(\"All features represent information available at prediction time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqd2bq3x9tl",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8xe1dth30d9",
   "metadata": {},
   "source": [
    "The initial metadata analysis reveals a **substantially larger dataset** compared to typical placement studies, with **19,158 instances** across **14 features** (13 predictors and 1 target). The dataset exhibits **no duplicate records**, ensuring data integrity. However, **missing values are present** in several features, with the most significant gaps in `company_type` (6,140 missing, 32.1%), `company_size` (5,938 missing, 31.0%), `major_discipline` (2,813 missing, 14.7%), and `gender` (4,508 missing, 23.5%). This necessitates careful imputation strategies or missing value handling during preprocessing.\n",
    "\n",
    "The feature composition consists of **2 numerical predictors** (`city_development_index` and `training_hours`) and **10 categorical predictors** representing demographics, education, and employment history. Additionally, `enrollee_id` serves as a unique identifier and must be excluded from modeling, while `target` is the binary outcome variable. Categorical features exhibit **varying cardinality**: low cardinality for binary features like `relevent_experience` (2 values) and `gender` (3 values including missing), moderate cardinality for features like `education_level` (5 levels) and `major_discipline` (6 disciplines), and **high cardinality** for `city` (123 unique cities) and `experience` (22 levels), which may require specialized encoding techniques such as target encoding or frequency encoding.\n",
    "\n",
    "The target variable exhibits **significant class imbalance**, with **75.1% of candidates not looking for job change** (class 0) and only **24.9% actively seeking change** (class 1), yielding an imbalance ratio of **3.01:1**. This substantial imbalance must be addressed during model training through techniques such as class weighting, resampling (SMOTE/undersampling), or using evaluation metrics robust to imbalance (F1-score, ROC-AUC, precision-recall curves). **No data leakage concerns** were identified; all features represent information collected during training enrollment, ensuring model validity for predicting actual job change intentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6wkcli5oth",
   "metadata": {},
   "source": [
    "### 4.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ns9tniisnzm",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Missing values count\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"\\nMissing values per feature:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Missing values percentage\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MISSING VALUES PERCENTAGE\")\n",
    "print(\"=\" * 60)\n",
    "missing_percentages = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "print(missing_percentages)\n",
    "\n",
    "# Summary statistics\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total missing values: {total_missing}\")\n",
    "print(f\"Total cells: {total_cells}\")\n",
    "print(f\"Overall missingness: {(total_missing / total_cells * 100):.2f}%\")\n",
    "print(f\"Features with missing values: {(missing_counts > 0).sum()} out of {len(df.columns)}\")\n",
    "print(f\"Complete features: {(missing_counts == 0).sum()} out of {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urrc4khdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    axes[0].bar(range(len(missing_data)), missing_data.values, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[0].set_xticks(range(len(missing_data)))\n",
    "    axes[0].set_xticklabels(missing_data.index, rotation=45, ha='right')\n",
    "    axes[0].set_title('Missing Values Count by Feature', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Features')\n",
    "    axes[0].set_ylabel('Number of Missing Values')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, v in enumerate(missing_data.values):\n",
    "        axes[0].text(i, v + 100, str(v), ha='center', va='bottom')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14)\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Percentage plot\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_pct) > 0:\n",
    "    axes[1].bar(range(len(missing_pct)), missing_pct.values, edgecolor='black', alpha=0.7, color='red')\n",
    "    axes[1].set_xticks(range(len(missing_pct)))\n",
    "    axes[1].set_xticklabels(missing_pct.index, rotation=45, ha='right')\n",
    "    axes[1].set_title('Missing Values Percentage by Feature', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Features')\n",
    "    axes[1].set_ylabel('Percentage (%)')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(missing_pct.values):\n",
    "        axes[1].text(i, v + 0.5, f'{v}%', ha='center', va='bottom')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whuo5kvskj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value pattern analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUE PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Features with missing values\n",
    "features_with_missing = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"\\nFeatures with missing values ({len(features_with_missing)}): {features_with_missing}\")\n",
    "    \n",
    "    # Check co-occurrence of missing values\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CO-OCCURRENCE OF MISSING VALUES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check common combinations\n",
    "    if 'company_size' in features_with_missing and 'company_type' in features_with_missing:\n",
    "        both_missing = df['company_size'].isnull() & df['company_type'].isnull()\n",
    "        print(f\"\\ncompany_size AND company_type both missing: {both_missing.sum()} ({both_missing.sum() / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    if 'company_size' in features_with_missing and 'company_type' in features_with_missing and 'experience' in features_with_missing:\n",
    "        all_three = df['company_size'].isnull() & df['company_type'].isnull() & df['experience'].isnull()\n",
    "        print(f\"company_size AND company_type AND experience all missing: {all_three.sum()} ({all_three.sum() / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Distribution of missing counts per row\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MISSING VALUES PER ROW DISTRIBUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    missing_per_row = df.isnull().sum(axis=1)\n",
    "    print(f\"\\nMissing values distribution:\")\n",
    "    print(missing_per_row.value_counts().sort_index())\n",
    "    \n",
    "    # Rows with any missing value\n",
    "    rows_with_missing = df.isnull().any(axis=1).sum()\n",
    "    print(f\"\\n--- Summary ---\")\n",
    "    print(f\"Total rows with at least one missing value: {rows_with_missing} ({rows_with_missing / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Rows with all values complete\n",
    "    complete_rows = (~df.isnull().any(axis=1)).sum()\n",
    "    print(f\"Complete rows (no missing values): {complete_rows} ({complete_rows / len(df) * 100:.2f}%)\")\n",
    "    \n",
    "    # Most common missing value patterns\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOP 5 MISSING VALUE PATTERNS\")\n",
    "    print(\"=\" * 60)\n",
    "    missing_patterns = df[features_with_missing].isnull().astype(int)\n",
    "    pattern_counts = missing_patterns.groupby(features_with_missing).size().sort_values(ascending=False).head(5)\n",
    "    \n",
    "    for idx, (pattern, count) in enumerate(pattern_counts.items(), 1):\n",
    "        missing_features = [feat for feat, is_missing in zip(features_with_missing, pattern) if is_missing == 1]\n",
    "        if missing_features:\n",
    "            print(f\"\\n{idx}. Missing: {missing_features}\")\n",
    "            print(f\"   Count: {count} ({count / len(df) * 100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n{idx}. No missing values\")\n",
    "            print(f\"   Count: {count} ({count / len(df) * 100:.2f}%)\")\n",
    "            \n",
    "else:\n",
    "    print(\"\\nNo missing values detected in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sk07yfg2sxf",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lg2fvu8k3nn",
   "metadata": {},
   "source": [
    "The missing values analysis reveals **significant data incompleteness** affecting **53.26% of all rows** (10,203 instances), while only **46.74%** (8,955 instances) are complete. Out of **14 features**, **8 contain missing values** with a **hierarchical pattern**: **company_type** (6,140 missing, 32.05%) and **company_size** (5,938 missing, 30.99%) dominate, followed by **gender** (4,508 missing, 23.53%) and **major_discipline** (2,813 missing, 14.68%). Lower-level missingness appears in **education_level** (460 missing, 2.40%), **last_new_job** (423 missing, 2.21%), **enrolled_university** (386 missing, 2.01%), and **experience** (65 missing, 0.34%). The overall dataset missingness is **7.73%** of total cells.\n",
    "\n",
    "**Pattern analysis** reveals **systematic co-occurrence** of missing values, confirming non-random (MNAR) behavior. The top missing value patterns are: **(1) Complete rows with no missing values: 46.74%**; **(2) Both company_size AND company_type missing: 14.50%** (2,777 rows) — strongly indicating unemployed candidates or students; **(3) Only gender missing: 11.61%** (2,224 rows); **(4) major_discipline, company_size, AND company_type missing: 4.42%** (847 rows); **(5) gender, company_size, AND company_type missing: 4.36%** (835 rows). Notably, **5,360 rows (27.98%)** have both employment features missing together, while only **20 rows (0.10%)** have all three employment-related features (company_size, company_type, experience) missing simultaneously. The missing values per row distribution shows most affected rows have **1-3 missing features**, with decreasing frequency for higher counts (628 rows with 4 missing, 176 with 5, 62 with 6, and only 12 with 7).\n",
    "\n",
    "**Preprocessing strategy:** For **company_size and company_type**, we will **create an explicit \"Not Employed\" category** rather than impute values, because the 27.98% co-occurrence pattern clearly represents candidates without current employment (students/freshers) where these fields are genuinely not applicable — imputation would introduce false information and obscure this meaningful employment status indicator. For **low-missingness features** (education_level, enrolled_university, last_new_job, experience all <3%), we will apply **mode imputation** since their sporadic missingness suggests random data collection gaps rather than systematic patterns, and their low prevalence minimizes impact on model validity. For **gender and major_discipline**, we will **create \"Unknown\" categories** because their substantial independent missingness (11.61% for gender alone, 14.68% for major_discipline) indicates data provision reluctance or privacy concerns rather than inapplicability, and preserving this \"not provided\" signal may itself be predictive of job change behavior. Additionally, we will **engineer binary missingness indicators** (`has_employment_info`, `gender_provided`, `education_complete`) as the 53.26% of incomplete rows may exhibit distinct job-seeking behaviors, and these indicators could capture valuable patterns for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5ds0jss7j",
   "metadata": {},
   "source": [
    "### 4.3 Feature Distributions, Scaling & Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cl39rkn1jwj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of numerical features (excluding enrollee_id and target)\n",
    "numerical_features_for_viz = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, col in enumerate(numerical_features_for_viz):\n",
    "    axes[idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ja2b8vs26t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using boxplots (excluding enrollee_id and target)\n",
    "numerical_features_for_viz = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, col in enumerate(numerical_features_for_viz):\n",
    "    axes[idx].boxplot(df[col].dropna(), vert=True)\n",
    "    axes[idx].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcyttqlj3m8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative outlier detection using IQR method (excluding enrollee_id and target)\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER DETECTION (IQR METHOD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_features_for_analysis = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "for col in numerical_features_for_analysis:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"  Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    print(f\"  Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  Min outlier value: {df[col][outliers.index].min():.2f}\")\n",
    "        print(f\"  Max outlier value: {df[col][outliers.index].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kxjsohtf7di",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features distribution (visualize top categories for high-cardinality features)\n",
    "categorical_features = categorical_cols.copy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features[:10]):\n",
    "    # For high-cardinality features, show only top 10\n",
    "    value_counts = df[col].value_counts()\n",
    "    \n",
    "    if len(value_counts) > 10:\n",
    "        value_counts = value_counts.head(10)\n",
    "        title_suffix = \" (Top 10)\"\n",
    "    else:\n",
    "        title_suffix = \"\"\n",
    "    \n",
    "    axes[idx].bar(range(len(value_counts)), value_counts.values, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xticks(range(len(value_counts)))\n",
    "    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=8)\n",
    "    axes[idx].set_title(f'{col}{title_suffix}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=9)\n",
    "    axes[idx].set_ylabel('Count', fontsize=9)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v + max(value_counts.values)*0.01, str(v), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(categorical_features[:10]), 12):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for all categorical features\n",
    "print(\"=\" * 60)\n",
    "print(\"CATEGORICAL FEATURES - CARDINALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col}: {unique_count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdh7cwrgcmn",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cctau1voa9",
   "metadata": {},
   "source": [
    "**Numerical Feature Distributions:** The dataset contains only **two meaningful numerical predictors**. The `city_development_index` histogram reveals a **strongly left-skewed distribution** with a massive concentration at the highest development indices (0.9-0.95 range shows ~7,000+ candidates), indicating most training participants come from highly developed urban centers. The boxplot confirms minimal lower outliers around 0.448-0.47. In contrast, `training_hours` displays a **right-skewed distribution** with the mode at the lowest bins (0-50 hours showing ~2,700 candidates), followed by progressively decreasing frequencies toward higher training hours. The boxplot dramatically illustrates **extensive upper outliers** (984 instances, 5.1%) stretching from ~185 to 336 hours, representing candidates with exceptional training engagement far beyond the IQR upper bound of 88 hours.\n",
    "\n",
    "**Outlier Analysis:** We will **retain all outliers** rather than remove them. The `city_development_index` outliers (17 instances, 0.1%) represent candidates from less-developed cities — a legitimate and potentially informative minority segment whose job-seeking behavior may differ from the urban majority. The `training_hours` outliers (984 instances, 5.1%) are particularly valuable: candidates investing 185-336 hours in training demonstrate extreme commitment that could strongly predict job change intentions, either indicating active upskilling for career transition or employer-mandated training for current roles. Removing these 5% of candidates would discard a behaviorally distinct cohort whose outlier status itself carries predictive signal. The outliers represent genuine behavioral variance, not measurement errors.\n",
    "\n",
    "**Categorical Feature Distributions:** The visualizations reveal **pronounced imbalances across all features**. **City** (showing top 10 of 123) concentrates heavily in city_103 (4,355 candidates), with other cities having dramatically lower representation. **Gender** shows **strong male dominance** (13,221 males vs 1,238 females vs 191 other), creating a 10.7:1 imbalance. **Relevant experience** is heavily skewed toward \"Has relevant experience\" (13,792 vs 5,366). **Enrolled university** shows most are \"no_enrollment\" (13,817 vs 3,757 full-time vs 1,198 part-time). **Education level** is dominated by **Graduates** (11,598) followed by Masters (4,361), with minimal representation for High School, PhD, and Primary School. **Major discipline** overwhelmingly favors **STEM** (14,492), dwarfing Business Degree, Other, Humanities, Arts, and No Major. **Experience** (showing top 10 of 22 levels) shows broad distribution with \">20\" years leading (3,286), but relatively balanced across 5-20 year ranges. **Company size** peaks at \"50-99\" (3,884) with decreasing frequencies for larger sizes. **Company type** is dominated by **\"Pvt Ltd\"** (10,817), far exceeding Funded Startup, Public Sector, Early Stage Startup, NGO, and Other. **Last new job** shows \"1\" year as most common (8,040), declining for longer gaps. These imbalances will require careful encoding strategies: target/frequency encoding for high-cardinality features (city, experience), one-hot encoding for low-cardinality features, and attention to class imbalance during model training to prevent bias toward majority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpec963ybwf",
   "metadata": {},
   "source": [
    "### 4.4 Target Feature Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zpteycwzv7o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Count plot\n",
    "target_counts = df['target'].value_counts()\n",
    "axes[0].bar(target_counts.index, target_counts.values, edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "axes[0].set_title('Target Distribution (Count)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Target (0=Not Looking, 1=Looking for Job Change)')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts.values, labels=['Not Looking (0)', 'Looking (1)'], autopct='%1.1f%%', \n",
    "            startangle=90, colors=['red', 'green'], explode=(0.05, 0))\n",
    "axes[1].set_title('Target Distribution (Proportion)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(target_counts)\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df['target'].value_counts(normalize=True).round(3))\n",
    "print(f\"\\nClass imbalance ratio: {target_counts.max() / target_counts.min():.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zoyq2hjy27i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features comparison by target class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Exclude enrollee_id from comparison (it's just an identifier)\n",
    "numerical_features_for_comparison = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "\n",
    "for idx, col in enumerate(numerical_features_for_comparison):\n",
    "    looking = df[df['target'] == 1.0][col]\n",
    "    not_looking = df[df['target'] == 0.0][col]\n",
    "    \n",
    "    axes[idx].hist([not_looking, looking], bins=15, label=['Not Looking (0)', 'Looking (1)'], \n",
    "                   edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "    axes[idx].set_title(f'{col} by Target', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ixvk1alzj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of numerical features by target class\n",
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES - MEAN COMPARISON BY TARGET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_features_for_comparison = [col for col in numerical_cols if col not in ['enrollee_id', 'target']]\n",
    "comparison = df.groupby('target')[numerical_features_for_comparison].mean()\n",
    "comparison.index = ['Not Looking (0)', 'Looking (1)']\n",
    "print(\"\\nMean values by target class:\")\n",
    "print(comparison.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIFFERENCE (Looking - Not Looking)\")\n",
    "print(\"=\" * 60)\n",
    "difference = comparison.loc['Looking (1)'] - comparison.loc['Not Looking (0)']\n",
    "print(difference.round(2))\n",
    "\n",
    "# Visualize mean comparison\n",
    "comparison.T.plot(kind='bar', figsize=(10, 5), edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "plt.title('Mean Comparison of Numerical Features by Target', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Target')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w229mmkye5l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs target - job change rates\n",
    "categorical_features_for_analysis = [col for col in categorical_cols]\n",
    "\n",
    "# Limit to top categories for high-cardinality features like 'city'\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features_for_analysis[:10]):\n",
    "    if col == 'city':\n",
    "        # For city, show only top 10 cities\n",
    "        top_cities = df[col].value_counts().head(10).index\n",
    "        df_subset = df[df[col].isin(top_cities)]\n",
    "        ct = pd.crosstab(df_subset[col], df_subset['target'], normalize='index') * 100\n",
    "    else:\n",
    "        ct = pd.crosstab(df[col], df['target'], normalize='index') * 100\n",
    "    \n",
    "    ct.plot(kind='bar', ax=axes[idx], edgecolor='black', alpha=0.7, color=['red', 'green'])\n",
    "    axes[idx].set_title(f'Job Change Rate by {col}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Percentage (%)')\n",
    "    axes[idx].legend(['Not Looking (0)', 'Looking (1)'], fontsize=7)\n",
    "    axes[idx].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(categorical_features_for_analysis[:10]), 12):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0rabbgh7t4ip",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mywig6bawtj",
   "metadata": {},
   "source": [
    "**Target Class Distribution:** The target variable exhibits **severe class imbalance** with **14,381 candidates (75.1%) not looking for job change** versus **4,777 candidates (24.9%) actively seeking change**, yielding a **3.01:1 imbalance ratio**. This significant imbalance will require careful handling during model training through class weighting, SMOTE resampling, or stratified sampling to prevent the model from defaulting to majority class predictions and achieving deceptively high accuracy while failing to identify job-seeking candidates.\n",
    "\n",
    "**Numerical Features vs Target:** The comparative histograms reveal **minimal discriminatory power** for both numerical features. `city_development_index` shows nearly identical distributions for both classes, with both concentrated at high development indices (0.9-0.95 range). The mean comparison confirms this: **Not Looking: 0.83** vs **Looking: 0.81** — a trivial difference of only **-0.02**. Similarly, `training_hours` displays overlapping distributions with both classes peaking at 0-50 hours, though job-seekers show slightly broader spread into higher training hours. The mean difference is also minimal: **Not Looking: 65.70** vs **Looking: 64.35** — a difference of **-1.35 hours**. The bar chart visualization dramatically illustrates these negligible differences, with both features showing nearly identical mean values across classes. This suggests **neither numerical feature alone provides strong predictive signal** for job change intentions, indicating the model will need to rely heavily on categorical features and feature interactions.\n",
    "\n",
    "**Categorical Features vs Target:** The categorical feature analysis reveals **substantial variation in job change propensity** across different segments. **City** shows dramatic variance, with city_21 exhibiting the **highest job change rate (~60%)** while most other top cities hover around 10-20%. **Gender** reveals **females (26%) have higher job-seeking rates than males (22%)**, contradicting common assumptions. **Relevant experience** shows a striking pattern: candidates **without relevant experience have 35% job change rate** versus only **20% for experienced candidates** — suggesting less-established professionals are more mobile. **Enrolled university** demonstrates **full-time students have the highest mobility (38%)**, followed by part-time (25%), while non-enrolled show lowest rates (20%). **Education level** reveals **Graduate and High School** graduates have higher mobility (~28%) compared to Masters, PhD, and Primary School. **Major discipline** shows relatively **balanced rates (20-28%) across all fields**. **Experience levels** display interesting non-linearity: **fresher candidates (<1 year) and very experienced (>20 years) show higher job-seeking rates (~25-30%)** compared to mid-career professionals (15-20%). **Company size** reveals **smaller companies (<10, 10/49) have higher attrition (~23%)** versus larger corporations (~15-18%). **Company type** shows **Early Stage Startups have highest mobility (~24%)** while Public Sector shows lowest (~18%). **Last new job** demonstrates **\"never\" changed jobs candidates have highest rate (~32%)**, suggesting first-time job seekers, while recent changers (1-2 years) show ~22-25%. These categorical patterns will be **crucial for model predictions**, as they reveal distinct behavioral segments with varying job change propensities that numerical features fail to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ido0ujpu1m",
   "metadata": {},
   "source": [
    "### 4.5 Feature Correlation & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cq5lfcgssn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features correlation analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES CORRELATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Only correlate meaningful numerical features (exclude enrollee_id)\n",
    "numerical_features_for_correlation = [col for col in numerical_cols if col not in ['enrollee_id']]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[numerical_features_for_correlation].corr()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, fmt='.3f')\n",
    "plt.title('Correlation Heatmap - Numerical Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cxjzqv8hg3",
   "metadata": {},
   "source": [
    "#### Categorical Feature Correlation (Cramér's V)\n",
    "\n",
    "Cramér's V mesure l'association entre variables catégorielles (0 = aucune, 1 = parfaite).\n",
    "- **V < 0.1**: Négligeable | **0.1-0.3**: Faible | **0.3-0.5**: Modérée | **V ≥ 0.5**: Forte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jz36j1hlnq",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.contingency import association\n",
    "\n",
    "# Calculate Cramér's V matrix for categorical features\n",
    "categorical_features_for_corr = [col for col in categorical_cols]\n",
    "n_features = len(categorical_features_for_corr)\n",
    "cramers_matrix = np.zeros((n_features, n_features))\n",
    "\n",
    "for i, col1 in enumerate(categorical_features_for_corr):\n",
    "    for j, col2 in enumerate(categorical_features_for_corr):\n",
    "        if i == j:\n",
    "            cramers_matrix[i, j] = 1.0\n",
    "        elif i < j:\n",
    "            mask = df[col1].notna() & df[col2].notna()\n",
    "            contingency = pd.crosstab(df.loc[mask, col1], df.loc[mask, col2])\n",
    "            v = association(contingency, method='cramer')\n",
    "            cramers_matrix[i, j] = v\n",
    "            cramers_matrix[j, i] = v\n",
    "\n",
    "# Visualize\n",
    "cramers_df = pd.DataFrame(cramers_matrix, index=categorical_features_for_corr, columns=categorical_features_for_corr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramers_df, annot=True, cmap='YlOrRd', square=True, fmt='.2f', vmin=0, vmax=1)\n",
    "plt.title(\"Cramér's V - Categorical Features Correlation\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccabs6yjhv",
   "metadata": {},
   "source": [
    "#### Chi-Square Tests: Categorical Features vs Target\n",
    "\n",
    "Test d'indépendance pour évaluer l'association entre chaque feature catégorielle et la variable cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zdvvamjs4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Chi-square tests: categorical features vs target\n",
    "chi2_results = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    mask = df[col].notna()\n",
    "    contingency = pd.crosstab(df.loc[mask, col], df.loc[mask, 'target'])\n",
    "    chi2, p_value, dof, _ = chi2_contingency(contingency)\n",
    "    v = association(contingency, method='cramer')\n",
    "    \n",
    "    chi2_results.append({\n",
    "        'Feature': col,\n",
    "        'Chi-square': round(chi2, 2),\n",
    "        'p-value': p_value,\n",
    "        \"Cramér's V\": round(v, 3)\n",
    "    })\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_results).sort_values(\"Cramér's V\", ascending=False)\n",
    "print(\"Chi-square Tests: Categorical Features vs Target\")\n",
    "print(chi2_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(chi2_df['Feature'], chi2_df[\"Cramér's V\"], color='steelblue', edgecolor='black')\n",
    "plt.xlabel(\"Cramér's V\")\n",
    "plt.title(\"Association Strength with Target\", fontsize=12, fontweight='bold')\n",
    "plt.axvline(x=0.1, color='orange', linestyle='--', label='Weak (0.1)')\n",
    "plt.axvline(x=0.3, color='red', linestyle='--', label='Moderate (0.3)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ilbo2lx1xpj",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qqjspyjuzqo",
   "metadata": {},
   "source": [
    "**Corrélation numérique:** Les deux features numériques (`city_development_index` et `training_hours`) sont indépendantes (r = 0.002). Seul `city_development_index` montre une corrélation modérée avec le target (r = -0.34), indiquant que les candidats des villes moins développées cherchent davantage à changer d'emploi.\n",
    "\n",
    "**Corrélation catégorielle (Cramér's V):** La matrice révèle des associations modérées entre certaines features liées à l'expérience professionnelle:\n",
    "- `relevent_experience` ↔ `experience` (V = 0.40)\n",
    "- `relevent_experience` ↔ `enrolled_university` / `last_new_job` (V = 0.39)\n",
    "- `relevent_experience` ↔ `education_level` (V = 0.32)\n",
    "\n",
    "Ces corrélations sont logiques (progression de carrière) mais restent modérées, sans multicolinéarité sévère.\n",
    "\n",
    "**Association avec le target (Chi-square):** Toutes les features catégorielles sont statistiquement significatives (p < 0.05). Classement par force d'association:\n",
    "- **Modérée (V ≥ 0.3):** `city` (0.396) — meilleur prédicteur catégoriel\n",
    "- **Faible (0.1-0.3):** `experience` (0.192), `enrolled_university` (0.156), `relevent_experience` (0.128)\n",
    "- **Négligeable (V < 0.1):** `education_level`, `last_new_job`, `company_size`, `company_type`, `major_discipline`, `gender`\n",
    "\n",
    "**Conclusion:** `city` et `city_development_index` sont les prédicteurs les plus forts. Les features liées à l'expérience (`experience`, `enrolled_university`, `relevent_experience`) forment un second groupe de prédicteurs utiles. Les features démographiques (`gender`, `major_discipline`) ont un pouvoir prédictif faible mais restent significatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1uutp5z6qr",
   "metadata": {},
   "source": [
    "### 4.6 Unsupervised Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8emgkzabdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données pour le clustering\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sélection des features (sans enrollee_id et target)\n",
    "X_cluster = df.drop(['enrollee_id', 'target'], axis=1)\n",
    "\n",
    "# Identifier les colonnes\n",
    "num_cols = X_cluster.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols = X_cluster.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Pipeline de prétraitement\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), num_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "# Transformer les données\n",
    "X_processed = preprocessor.fit_transform(X_cluster)\n",
    "print(f\"Données prétraitées: {X_processed.shape[0]} samples, {X_processed.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "poy4lmwci0p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode du coude (Elbow Method)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_processed)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Nombre de clusters (k)')\n",
    "plt.ylabel('Inertie')\n",
    "plt.title('Méthode du Coude', fontsize=13, fontweight='bold')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jx93sby6isl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application K-Means (choisir k après avoir vu le coude)\n",
    "k_optimal = 4  # À ajuster selon le graphique du coude\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_processed)\n",
    "\n",
    "# Réduction PCA pour visualisation\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_processed)\n",
    "\n",
    "# Visualisation des clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title(f'K-Means Clustering (k={k_optimal}) - Projection PCA', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variance expliquée par PCA: {pca.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zyofvj1ttk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des clusters vs target\n",
    "df_clusters = df.copy()\n",
    "df_clusters['cluster'] = clusters\n",
    "\n",
    "# Distribution du target par cluster\n",
    "cluster_analysis = df_clusters.groupby('cluster')['target'].agg(['count', 'sum', 'mean'])\n",
    "cluster_analysis.columns = ['Total', 'Looking for Change', 'Rate (%)']\n",
    "cluster_analysis['Rate (%)'] = (cluster_analysis['Rate (%)'] * 100).round(1)\n",
    "print(\"Distribution du target par cluster:\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Taille des clusters\n",
    "axes[0].bar(cluster_analysis.index, cluster_analysis['Total'], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Cluster')\n",
    "axes[0].set_ylabel('Nombre de candidats')\n",
    "axes[0].set_title('Taille des clusters', fontweight='bold')\n",
    "\n",
    "# Taux de recherche d'emploi par cluster\n",
    "colors = plt.cm.RdYlGn_r(cluster_analysis['Rate (%)'] / 100)\n",
    "axes[1].bar(cluster_analysis.index, cluster_analysis['Rate (%)'], color=colors, edgecolor='black')\n",
    "axes[1].axhline(y=df['target'].mean()*100, color='red', linestyle='--', label=f'Moyenne globale ({df[\"target\"].mean()*100:.1f}%)')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Taux de recherche (%)')\n",
    "axes[1].set_title('Taux de recherche d\\'emploi par cluster', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcr5cnxcxhj",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Le clustering K-Means (k=4) révèle **4 segments de candidats** avec des comportements distincts vis-à-vis de la recherche d'emploi:\n",
    "\n",
    "- **Cluster 1** (~8,800 candidats, 13%): Profil le plus stable, taux de recherche bien inférieur à la moyenne\n",
    "- **Cluster 3** (~4,800 candidats, 47%): Profil à **haut risque d'attrition**, taux presque 2x supérieur à la moyenne globale (24.9%)\n",
    "- **Clusters 0 et 2**: Comportements intermédiaires (18% et 27%)\n",
    "\n",
    "La variance expliquée par PCA (29.6%) est limitée en raison du grand nombre de features catégorielles encodées, mais les clusters restent visuellement distincts. Le fait que le clustering non-supervisé identifie des groupes corrélés au target **sans l'avoir utilisé** confirme que les features contiennent des patterns prédictifs exploitables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1xqgtjif38",
   "metadata": {},
   "source": [
    "### 4.7 Interpretations & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6jchjowi29",
   "metadata": {},
   "source": [
    "L'analyse exploratoire a révélé les points clés suivants:\n",
    "\n",
    "**Données:**\n",
    "- 19,158 candidats, 12 features prédictives (2 numériques, 10 catégorielles)\n",
    "- Déséquilibre de classes: 75% / 25% (ratio 3:1)\n",
    "- Valeurs manquantes significatives (53% des lignes affectées), notamment `company_size` et `company_type` (candidats sans emploi)\n",
    "\n",
    "**Features les plus prédictives:**\n",
    "| Feature | Type | Association avec target |\n",
    "|---------|------|------------------------|\n",
    "| city | Catégorielle | V = 0.396 (modérée) |\n",
    "| city_development_index | Numérique | r = -0.342 (modérée) |\n",
    "| experience | Catégorielle | V = 0.192 (faible) |\n",
    "| enrolled_university | Catégorielle | V = 0.156 (faible) |\n",
    "| relevent_experience | Catégorielle | V = 0.128 (faible) |\n",
    "\n",
    "**Insights métier:**\n",
    "- Les candidats des **villes moins développées** cherchent davantage à changer d'emploi\n",
    "- Les **étudiants à temps plein** et candidats **sans expérience pertinente** sont plus mobiles\n",
    "- Le clustering identifie un segment à **haut risque (47%)** vs un segment **stable (13%)**\n",
    "\n",
    "**Implications pour la modélisation:**\n",
    "1. Gérer le déséquilibre de classes (class_weight, SMOTE)\n",
    "2. Traiter les valeurs manquantes (imputation + catégorie \"Unknown\")\n",
    "3. Encoder les features à haute cardinalité (`city`: 123 valeurs)\n",
    "4. Privilégier les modèles non-linéaires (Random Forest, XGBoost) pour capturer les interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqc8z5vqw4l",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ML Baseline & Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gt7bbs25u3k",
   "metadata": {},
   "source": [
    "### 5.1 Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xamw7fik04a",
   "metadata": {},
   "source": [
    "Division des données en **3 ensembles distincts** pour une évaluation rigoureuse:\n",
    "\n",
    "| Ensemble | Proportion | Rôle |\n",
    "|----------|------------|------|\n",
    "| **Train** | 70% | Apprentissage des modèles |\n",
    "| **Validation** | 15% | Tuning des hyperparamètres (section 6) |\n",
    "| **Test** | 15% | Évaluation finale (jamais vu pendant l'entraînement) |\n",
    "\n",
    "**Points clés:**\n",
    "- *Stratification* → préserve le ratio de classes (75/25) dans chaque ensemble\n",
    "- Suppression de `enrollee_id` → identifiant sans valeur prédictive\n",
    "- `random_state=42` → reproductibilité des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1y89f4ne4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "X = df.drop(['enrollee_id', 'target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split 70/15/15 stratifié\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Vérification\n",
    "print(f\"Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w0em90lymgs",
   "metadata": {},
   "source": [
    "### 5.2 Pipelines & Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nl95wzkjc9p",
   "metadata": {},
   "source": [
    "Construction de **pipelines** qui enchaînent prétraitement et modèle en une seule étape:\n",
    "\n",
    "**Prétraitement automatique:**\n",
    "- *Features numériques* → Imputation (médiane) + Standardisation\n",
    "- *Features catégorielles* → Imputation (mode) + One-Hot Encoding\n",
    "\n",
    "**Modèles baseline:**\n",
    "| Modèle | Type | Pourquoi |\n",
    "|--------|------|----------|\n",
    "| **Logistic Regression** | Linéaire | Simple, interprétable, baseline rapide |\n",
    "| **Random Forest** | Ensemble | Capture les interactions non-linéaires |\n",
    "\n",
    "**Note:** `class_weight='balanced'` → gère automatiquement le déséquilibre 75/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n3uayft3v9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des types de features\n",
    "num_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Préprocesseur\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_features),\n",
    "    ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), cat_features)\n",
    "])\n",
    "\n",
    "# Pipelines avec modèles\n",
    "lr_pipeline = Pipeline([('preprocessor', preprocessor), ('model', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))])\n",
    "rf_pipeline = Pipeline([('preprocessor', preprocessor), ('model', RandomForestClassifier(class_weight='balanced', random_state=42))])\n",
    "\n",
    "print(f\"Features: {len(num_features)} numériques, {len(cat_features)} catégorielles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7p38rbqmjzj",
   "metadata": {},
   "source": [
    "### 5.3 Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tvesz5yfps",
   "metadata": {},
   "source": [
    "**Cross-validation 5-fold** sur l'ensemble d'entraînement pour évaluer la performance des modèles de manière robuste.\n",
    "\n",
    "*Principe:* Le train set est divisé en 5 parties → on entraîne sur 4, on valide sur 1, et on répète 5 fois.\n",
    "\n",
    "**Métriques évaluées:**\n",
    "- `accuracy` → % de prédictions correctes\n",
    "- `f1_weighted` → équilibre précision/rappel (adapté aux classes déséquilibrées)\n",
    "- `roc_auc` → capacité à discriminer les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8y4eycfir0j",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scoring = ['accuracy', 'f1_weighted', 'roc_auc']\n",
    "\n",
    "# Cross-validation pour les deux modèles\n",
    "results = {}\n",
    "for name, pipeline in [('Logistic Regression', lr_pipeline), ('Random Forest', rf_pipeline)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    results[name] = {}\n",
    "    for metric in scoring:\n",
    "        scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=metric)\n",
    "        results[name][metric] = scores.mean()\n",
    "        print(f\"  {metric}: {scores.mean():.3f} (+/- {scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rcsmc5lyfa",
   "metadata": {},
   "source": [
    "### 5.4 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n3mxevhq1k",
   "metadata": {},
   "source": [
    "**Évaluation finale** sur le test set (données jamais vues pendant l'entraînement).\n",
    "\n",
    "- Entraînement sur `X_train` complet\n",
    "- Prédiction et évaluation sur `X_test`\n",
    "- **Classification report** → précision, rappel, F1 par classe\n",
    "- **Matrice de confusion** → visualisation des erreurs\n",
    "- **Feature importance** → comparaison RF (*Gini importance*) vs LR (*coefficients*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l7y521g4zp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement et évaluation sur test set\n",
    "models = {'Logistic Regression': lr_pipeline, 'Random Forest': rf_pipeline}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\n{name}\\n{'='*50}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# Matrice de confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{name}')\n",
    "    axes[idx].set_xlabel('Prédit')\n",
    "    axes[idx].set_ylabel('Réel')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qw6kpu3stal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance - Comparaison des deux modèles\n",
    "feature_names = num_features + list(rf_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Random Forest - Feature Importances\n",
    "rf_importances = rf_pipeline.named_steps['model'].feature_importances_\n",
    "top_rf = np.argsort(rf_importances)[-15:]\n",
    "axes[0].barh(range(15), rf_importances[top_rf], color='steelblue')\n",
    "axes[0].set_yticks(range(15))\n",
    "axes[0].set_yticklabels([feature_names[i] for i in top_rf])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest (Gini Importance)', fontweight='bold')\n",
    "\n",
    "# Logistic Regression - Coefficients (valeur absolue)\n",
    "lr_coefs = np.abs(lr_pipeline.named_steps['model'].coef_[0])\n",
    "top_lr = np.argsort(lr_coefs)[-15:]\n",
    "axes[1].barh(range(15), lr_coefs[top_lr], color='coral')\n",
    "axes[1].set_yticks(range(15))\n",
    "axes[1].set_yticklabels([feature_names[i] for i in top_lr])\n",
    "axes[1].set_xlabel('|Coefficient|')\n",
    "axes[1].set_title('Logistic Regression (Coefficient Magnitude)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4wn484c6q",
   "metadata": {},
   "source": [
    "### 5.5 Results Interpretation & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sgg1wo5i2bn",
   "metadata": {},
   "source": [
    "**Comparaison des modèles baseline:**\n",
    "\n",
    "| Métrique | Logistic Regression | Random Forest | Meilleur |\n",
    "|----------|---------------------|---------------|----------|\n",
    "| Accuracy | 0.73 | **0.77** | RF |\n",
    "| ROC-AUC | **0.777** | 0.765 | LR |\n",
    "| Recall (classe 1) | **0.70** | 0.39 | LR |\n",
    "| F1 (classe 1) | **0.57** | 0.45 | LR |\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "1. **Logistic Regression surpasse Random Forest** pour l'objectif métier (détecter les candidats cherchant un emploi)\n",
    "   - Détecte 70% des chercheurs d'emploi vs seulement 39% pour RF\n",
    "   - Meilleur ROC-AUC (0.777 vs 0.765)\n",
    "\n",
    "2. **L'accuracy est trompeuse** avec des classes déséquilibrées\n",
    "   - RF a 77% d'accuracy mais rate 61% des candidats à risque\n",
    "\n",
    "3. **Feature importance révèle des stratégies différentes:**\n",
    "   - *LR* → exploite fortement la variable `city` (meilleur prédicteur identifié en EDA)\n",
    "   - *RF* → disperse sur `training_hours` malgré sa faible corrélation linéaire\n",
    "\n",
    "**Prochaine étape (Section 6):** Optimiser les hyperparamètres pour améliorer les performances, notamment le recall de la classe 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cvggwxipyvh",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Enhanced Models & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kg16rtxrd5",
   "metadata": {},
   "source": [
    "### 6.1 Justification of Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "begk7qkxzl",
   "metadata": {},
   "source": [
    "**Choix du modèle: HistGradientBoostingClassifier**\n",
    "\n",
    "*Justification basée sur notre analyse:*\n",
    "- **EDA (Section 4):** Déséquilibre de classes (75/25), `city` est le meilleur prédicteur (V=0.396), features catégorielles à haute cardinalité\n",
    "- **Baselines (Section 5):** Random Forest sous-performe sur le recall classe 1 (0.39), LR meilleure mais limitée aux relations linéaires\n",
    "- **Cours:** \"RandomForest, AdaBoost, GBRT, and HGB are among the first models you should test\"\n",
    "\n",
    "*Pourquoi HistGradientBoosting:*\n",
    "- **Gradient Boosting** → souvent meilleur que Random Forest sur données tabulaires\n",
    "- Implémentation **optimisée** dans scikit-learn (rapide)\n",
    "- Gère bien les **interactions non-linéaires** entre features\n",
    "\n",
    "*Preprocessing:* Même pipeline que les baselines (imputation + one-hot encoding) pour comparaison équitable.\n",
    "\n",
    "**Métriques d'optimisation:** `roc_auc` et `f1_weighted` (adaptées au déséquilibre de classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m27bj9o9qf",
   "metadata": {},
   "source": [
    "### 6.2 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5mm80lfd4l",
   "metadata": {},
   "source": [
    "**GridSearchCV** pour trouver les meilleurs hyperparamètres:\n",
    "- `max_depth` → profondeur des arbres (contrôle overfitting)\n",
    "- `learning_rate` → vitesse d'apprentissage\n",
    "- `max_iter` → nombre d'arbres dans l'ensemble\n",
    "\n",
    "Validation sur `X_val` (ensemble de validation créé en 5.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tl2fadk15o",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Pipeline avec preprocessor (réutilise celui de la section 5.2)\n",
    "hgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', HistGradientBoostingClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Grille d'hyperparamètres (préfixe 'model__' pour le pipeline)\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'model__max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "# GridSearchCV avec les deux métriques\n",
    "results_grid = {}\n",
    "for scoring in ['roc_auc', 'f1_weighted']:\n",
    "    print(f\"\\n{'='*50}\\nOptimisation pour: {scoring}\\n{'='*50}\")\n",
    "    \n",
    "    grid_search = GridSearchCV(hgb_pipeline, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    results_grid[scoring] = grid_search\n",
    "    print(f\"Meilleurs paramètres: {grid_search.best_params_}\")\n",
    "    print(f\"Meilleur score CV: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ftpfpzn8gl8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des deux modèles optimisés (ROC-AUC vs F1)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for idx, (metric, label) in enumerate([('roc_auc', 'ROC-AUC'), ('f1_weighted', 'F1-weighted')]):\n",
    "    best_model = results_grid[metric].best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\nHGB optimisé pour {label}\\n{'='*50}\")\n",
    "    print(f\"Params: {results_grid[metric].best_params_}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(f'HGB optimisé {label}')\n",
    "    axes[idx].set_xlabel('Prédit')\n",
    "    axes[idx].set_ylabel('Réel')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qxdtuqjiu4b",
   "metadata": {},
   "source": [
    "### 6.3 Final Results & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9kggnmk5w1v",
   "metadata": {},
   "source": [
    "**Comparaison finale de tous les modèles (Test Set):**\n",
    "\n",
    "| Modèle | ROC-AUC | Recall (classe 1) | F1 (classe 1) | Candidats détectés |\n",
    "|--------|---------|-------------------|---------------|-------------------|\n",
    "| Logistic Regression | 0.777 | 0.70 | 0.57 | 504/716 (70%) |\n",
    "| Random Forest | 0.765 | 0.39 | 0.45 | 276/716 (39%) |\n",
    "| **HGB (ROC-AUC)** | **0.784** | **0.76** | **0.59** | **542/716 (76%)** |\n",
    "| HGB (F1) | 0.785 | 0.74 | 0.59 | 530/716 (74%) |\n",
    "\n",
    "**Meilleur modèle: HistGradientBoosting optimisé pour ROC-AUC**\n",
    "\n",
    "*Hyperparamètres:* `max_depth=5`, `learning_rate=0.05`, `max_iter=100`\n",
    "\n",
    "**Gains par rapport aux baselines:**\n",
    "- **+0.7%** ROC-AUC vs Logistic Regression\n",
    "- **+6 points** de recall vs LR → détecte 38 candidats supplémentaires\n",
    "- **+37 points** de recall vs Random Forest → détecte 266 candidats supplémentaires\n",
    "\n",
    "**Interprétation métier:** Sur 716 candidats cherchant un emploi, le modèle optimisé en identifie correctement **542 (76%)**, contre 504 pour LR et seulement 276 pour RF. Cette amélioration permet un ciblage plus efficace des candidats à risque d'attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ur24e8fjq9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f01f20",
   "metadata": {},
   "source": [
    "**Objectif:** Prédire les candidats cherchant à changer d'emploi à partir de leurs caractéristiques démographiques, éducatives et professionnelles.\n",
    "\n",
    "**Principales découvertes (EDA):**\n",
    "- Dataset de 19,158 candidats avec déséquilibre de classes (75/25)\n",
    "- `city` et `city_development_index` sont les meilleurs prédicteurs\n",
    "- Le clustering révèle un segment à haut risque (47% de mobilité)\n",
    "\n",
    "**Comparaison des modèles:**\n",
    "\n",
    "| Modèle | ROC-AUC | Recall (classe 1) |\n",
    "|--------|---------|-------------------|\n",
    "| Logistic Regression | 0.777 | 70% |\n",
    "| Random Forest | 0.765 | 39% |\n",
    "| **HGB (Optimisé)** | **0.784** | **76%** |\n",
    "\n",
    "**Meilleur modèle:** HistGradientBoostingClassifier\n",
    "- Hyperparamètres: `max_depth=5`, `learning_rate=0.05`, `max_iter=100`\n",
    "- Détecte **76% des candidats** cherchant un emploi (542/716)\n",
    "\n",
    "**Limites:**\n",
    "- Déséquilibre de classes impacte la précision (49% de faux positifs)\n",
    "- Feature `city` à haute cardinalité (123 valeurs) → one-hot encoding crée 123 features\n",
    "\n",
    "**Pistes d'amélioration:**\n",
    "- Target encoding pour `city` (réduire la dimensionnalité)\n",
    "- Tester d'autres modèles (XGBoost, LightGBM)\n",
    "- Ajuster le seuil de décision pour équilibrer précision/rappel"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
